[
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "This new version of NA³ is currently available via the Python Package Index or via GitHub. To be able to install and run NA³ on your system you currently need to run a few commands in the terminal, as the fully GUI-based installation will be available only soon. But don´t worry, we´ll take you through the process step by step!",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#install-anaconda",
    "href": "installation.html#install-anaconda",
    "title": "Installation",
    "section": "1. Install Anaconda",
    "text": "1. Install Anaconda\nThough not strictly required, we highly recommend installing NA³ in a virtual Python environment that you setup via Anaconda - especially for users with limited (or even no) programming expertise. Please download the corresponding distribution for your operating system here and follow the respective installation wizard to install Anaconda on your system.",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#create-and-activate-a-new-virtual-environment",
    "href": "installation.html#create-and-activate-a-new-virtual-environment",
    "title": "Installation",
    "section": "2. Create and activate a new virtual environment",
    "text": "2. Create and activate a new virtual environment\nNext, we´ll need a terminal to install NA³ with just a few commands. For this, we suggest using the tool Anaconda Prompt that came with the installation of Anaconda (but if you prefer to use a different terminal, please feel free to use it - the only requirement is that conda is accessible, which comes per default in Anaconda Prompt). In the terminal (e.g. in Anaconda Prompt), please type the following command and execute it by hitting the Return key on your keyboard. It will create a new virtual environment that we can use to install NA³, using conda:\n\nconda create --name na3 -y python=3.11\n\nOnce the setup of the environment has concluded, you should see a message like the following, informing you how to activate the environment:\n\nWe´ll do exactly as suggested to activate the newly created virtual environment to continue the installation of NA³. Thus, please run the following command in the Anaconda Prompt terminal:\n\nconda activate na3\n\nYou should now see that the input line in the Anaconda Prompt terminal starts with (na3) - indicating that you successfully switched to this virtual environment.",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#install-the-neuralactivitycubic-python-package",
    "href": "installation.html#install-the-neuralactivitycubic-python-package",
    "title": "Installation",
    "section": "3. Install the neuralactivitycubic Python package:",
    "text": "3. Install the neuralactivitycubic Python package:\nWith the activated na3 environment, please complete the installation of NA³ by running on final command in the Anaconda Prompt terminal:\n\npip install neuralactivitycubic\n\nUpon executing this command, you will see a lot of output and activity in the terminal while it downloads the source code of NA³ and it´s dependencies, and installs them all in the virtual environment on your system. You know that the installation has finished, when the input line starting with (na3) appears again. The final few messages that are shown in Anaconda Prompt will then likely look similar to this:\n\n\n\nOutput after successful installation of neuralactivitycubic.",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#done",
    "href": "installation.html#done",
    "title": "Installation",
    "section": "4. Done!",
    "text": "4. Done!\nCongratulations! That´s already everything you need to do to install NA³ on your system. You can now use NA³ to analyze your data and hopefully start a journey to the next scientific breakthrough!\nIf you´d like to get some more information on how to use NA³, please check out the following chapters in this documentation:\n\nUsing NA³ via the GUI\nUsing NA³ via the API",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#wed-love-to-hear-your-feedback",
    "href": "installation.html#wed-love-to-hear-your-feedback",
    "title": "Installation",
    "section": "We´d love to hear your Feedback:",
    "text": "We´d love to hear your Feedback:\nYou are using NA³ for your research - or would like to do so, but there´s that one cool feature missing for you? That´s great! We´d love to hear your feedback, feature requests, or bug reports to keep improving NA³ - please use this form on GitHub to submit it, we appreciate it a lot!\nPlease feel free to also explore all other Research Software that our not-for-profit organization has developed, or learn more about us and what we do on our website. If you are interested to partner with us for a similar collaboration on your Research Software, or to develop it with or for you from scratch, don´t hesitate and drop us an email - we´re always looking for new opportunities to catalyze science be delivering cutting-edge technology tailored to the specific research challenges of our collaborators!\n\n\n\nIndoc Research Europe gGmbH",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "07_datamodels.html",
    "href": "07_datamodels.html",
    "title": "neuralactivitycubic",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "07_datamodels.html#tests",
    "href": "07_datamodels.html#tests",
    "title": "neuralactivitycubic",
    "section": "Tests:",
    "text": "Tests:\nSetup for testing:\n\nfrom fastcore.test import test_fail\n\nfilepath = '../test_data/00/spiking_neuron.avi'\n\ncorrect_general_config = Config().to_dict()\ncorrect_general_config['data_source_path'] = filepath\n\nexample_general_config = dict(\n    batch_mode=False,\n    baseline_estimation_method=BaselineEstimationMethod.ASLS,\n    customize_octave_filtering=False,\n    data_source_path=filepath,\n    end_frame_idx=500,\n    export_to_nwb=False,\n    focus_area_enabled=False,\n    focus_area_filepath=None,\n    grid_size=10,\n    include_variance=False,\n    mean_signal_threshold=10.0,\n    min_octave_span=1.0,\n    min_peak_count=2,\n    noise_window_size=200,\n    recording_filepath=None,\n    results_filepath=None,\n    roi_filepath=None,\n    roi_mode=ROIMode.GRID,\n    save_overview_png=True,\n    save_single_trace_results=False,\n    save_summary_results=True,\n    signal_to_noise_ratio=3.0,\n    start_frame_idx=0,\n    use_frame_range=False,\n    variance_window_size=15,\n)\n\ncorrect_peak_config = {\n    'frame_idx': 10,\n    'intensity': 10.0,\n    'amplitude': 10.0,\n    'delta_f_over_f': 10.0,\n    'has_neighboring_intersections': True,\n    'frame_idxs_of_neighboring_intersections': (1,2),\n    'area_under_curve': 10.0,\n    'peak_type': 'normal',\n}\nminimal_peak_config = {\n    'frame_idx': 10,\n    'intensity': 10.0,\n}\n\ndef test_correct_config():\n    return Config.from_dict(**correct_general_config)\n\ndef test_minimal_config():\n    return Config.from_dict(data_source_path=filepath)\n\ndef test_config_with_specific_filepaths():\n    specific_config = correct_general_config.copy()\n    specific_config['data_source_path'] = None\n    specific_config['recording_filepath'] = filepath\n    specific_config['roi_filepath'] = filepath\n    specific_config['focus_area_filepath'] = filepath\n    return Config.from_dict(**specific_config)\n\ndef test_config_with_specific_filepaths_list():\n    specific_config = correct_general_config.copy()\n    specific_config['data_source_path'] = None\n    specific_config['recording_filepath'] = filepath\n    specific_config['roi_filepath'] = [filepath, filepath]\n    specific_config['focus_area_filepath'] = filepath\n    return Config.from_dict(**specific_config)\n\ndef test_config_to_json():\n    config = Config.from_dict(**example_general_config)\n    return config.to_json()\n\n# enumerators are special cases, so we need to check them separately\ndef test_config_enum_from_str():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['baseline_estimation_method'] = 'asls'  # valid value for baseline_estimation_method\n    return Config.from_dict(**incorrect_config)\n\ndef test_incorrect_config_enum_from_str():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['baseline_estimation_method'] = 'not_valid'  # invalid value for baseline_estimation_method\n    return Config.from_dict(**incorrect_config)\n\n# general invalid types checks\ndef test_incorrect_config_batch_mode():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['batch_mode'] = 'invalid_value'  # invalid type for batch_mode\n    return Config.from_dict(**incorrect_config)\n\ndef test_incorrect_config_end_frame_idx():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['end_frame_idx'] = 'invalid_value'  # invalid type for end_frame_idx\n    return Config.from_dict(**incorrect_config)\n\n# enumerators are special cases, so we need to check them separately\ndef test_incorrect_config_baseline_estimation_method():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['baseline_estimation_method'] = 1337  # invalid type for baseline_estimation_method\n    return Config.from_dict(**incorrect_config)\n\n# boolean fields are also special cases, so we need to check them separately\ndef test_incorrect_config_customize_octave_filtering():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['customize_octave_filtering'] = 'invalid_value'  # invalid type for customize_octave_filtering\n    return Config.from_dict(**incorrect_config)\n\n# roi_filepath is a special case, so we need to check it separately\ndef test_incorrect_config_roi_filepath():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['roi_filepath'] = (1, 2)  # invalid type for roi_filepath\n    return Config.from_dict(**incorrect_config)\n\n# other filepaths should not be set up with data_source_path\ndef test_incorrect_config_singular_path():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['recording_filepath'] = filepath\n    incorrect_config['roi_filepath'] = filepath\n    incorrect_config['focus_area_filepath'] = filepath\n    return Config.from_dict(**incorrect_config)\n\ndef test_correct_peak_config():\n    return Peak.from_dict(**correct_peak_config)\n\ndef test_minimal_peak_config():\n    return Peak.from_dict(**minimal_peak_config)\n\ndef test_incomplete_peak_config():\n    incomplete_peak_config = correct_peak_config.copy()\n    incomplete_peak_config.pop('frame_idx')\n    return Peak.from_dict(**incomplete_peak_config)\n\ndef test_wrong_peak_config():\n    wrong_peak_config = correct_peak_config.copy()\n    wrong_peak_config['frame_idx'] = 'invalid_value'\n    return Peak.from_dict(**wrong_peak_config)\n\nRun tests:\n\n# correct inputs tests\nassert isinstance(test_correct_config(), Config)\nassert isinstance(test_minimal_config(), Config)\nassert isinstance(test_config_enum_from_str(), Config)\nassert isinstance(test_config_with_specific_filepaths(), Config)\nassert isinstance(test_config_with_specific_filepaths_list(), Config)\nassert isinstance(test_correct_peak_config(), Peak)\nassert isinstance(test_minimal_peak_config(), Peak)\nassert test_config_to_json() == dumps(example_general_config)\n\n# incomplete inputs tests\ntest_fail(test_incomplete_peak_config)\n\n# wrong inputs tests\ntest_fail(test_incorrect_config_batch_mode)\ntest_fail(test_incorrect_config_baseline_estimation_method)\ntest_fail(test_incorrect_config_customize_octave_filtering)\ntest_fail(test_incorrect_config_end_frame_idx)\ntest_fail(test_incorrect_config_roi_filepath)\ntest_fail(test_incorrect_config_singular_path)\ntest_fail(test_incorrect_config_enum_from_str)\ntest_fail(test_wrong_peak_config)"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "model",
    "section": "",
    "text": "New to literate programming?\n\n\n\n\n\nWelcome to the inner workings of neuralactivitycubic!\nThis message is here only for you, as we´d like to share some basic information about this piece of research software with you to facilitate your interactions with it. We´re using a literate programming framework called nbdev for the development, testing, documentation, and dissemination of neuralactivitycubic. We believe the concept of rich annotations and usage examples directly intermixed with the source code (read more about the concept of literate programming for instance here) holds great value and potential, especially in the context of research software, as it makes it easier for others to understand, (re-)use, and ideally even to adapt or contribute to your code. That being said, some things in here might look a bit confusing to you - conversely especially if you are an experienced developer used to “regular” source code.\nFor instance, you´ll regularly find the implementation of a class, like the Model here, interrupted by markdown text and maybe even some addtional code cells that serve as usage examples or even represent the implementation of tests, only for the subsequent class methods to continue in code cells below, literally patched to the class using the @patch decorator. Just be aware that this may not meet your expectations of what conventional source code “should” look like and maybe you can discover some valuable takeaway for you along the way, too!\nOne final note before you head on, though: we are ourselves still experimenting a lot with this concept and how it can be used to best effect and are more than happy to engage in discussion or to hear your feedback on this topic. Feel free to drop us a message via GitHub!\nEverybody needs some help - and we´re standing on the shoulders of some giants here. Let´s start by importing all dependencies we need:\nExported source\n# Actual functional dependencies:\n# external:\nfrom pathlib import Path\nfrom datetime import datetime, timezone\nfrom matplotlib.pyplot import show\nimport ipywidgets as w\nimport multiprocessing\nfrom fastcore.basics import patch\nimport gc\n\n# and internal:\nfrom neuralactivitycubic.datamodels import Config\nfrom neuralactivitycubic.processing import AnalysisJob\nfrom neuralactivitycubic.input import RecordingLoaderFactory, ROILoaderFactory, RecordingLoader, ROILoader, get_filepaths_with_supported_extension_in_dirpath, FocusAreaPathRestrictions\n\n# Finally, some dependencies regarding type hints:\nfrom typing import Callable\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes._axes import Axes\nNow let´s get started:\nsource",
    "crumbs": [
      "API",
      "model"
    ]
  },
  {
    "objectID": "model.html#testing",
    "href": "model.html#testing",
    "title": "model",
    "section": "Testing",
    "text": "Testing\n\nSetup\n\nfrom shutil import rmtree\nimport os\nfrom re import compile\n\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\n\nfrom neuralactivitycubic.view import WidgetsInterface\n\n\ntest_filepath = Path('../test_data/00')\n\ntest00_filepath = Path('../test_data/00')\ntest01_filepath = Path('../test_data/01')\nparent_test_filepath = Path('../test_data')\nexample_results_dir = Path('../test_data/00/example_test_results_for_spiking_neuron')\nresults_filepath = Path('../test_data/00/results_directory')\n# we have to split results by own directories due to concurrency issues\nresults_case01_filepath = Path('../test_data/results/case_01/')\nresults_case02_filepath = Path('../test_data/results/case_02/')\nresults_case03_filepath = Path('../test_data/results/case_03/')\nresults_case04_filepath = Path('../test_data/results/case_04/')\nresults_case05_filepath = Path('../test_data/results/case_05/')\nresults_case06_filepath = Path('../test_data/results/case_06/')\n\ndef test_correct_model_run():\n    correct_config = WidgetsInterface().export_user_settings()\n    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n    correct_config.save_single_trace_results = True\n    model = Model(correct_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n    return model.result_directories\n\ndef test_correct_model_run_with_custom_results_dir():\n    correct_config = WidgetsInterface().export_user_settings()\n    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n    correct_config.results_filepath = results_filepath\n    model = Model(correct_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n    return correct_config.results_filepath\n\n# tests with different modes of running analysis\n\ndef test_run_grid_focus_mode():\n    \"\"\"\n    Test run_analysis function with focus area enabled.\n    \"\"\"\n    grid_focus_mode_config = WidgetsInterface().export_user_settings()\n    grid_focus_mode_config.data_source_path = test01_filepath\n    grid_focus_mode_config.focus_area_enabled = True\n    grid_focus_mode_config.results_filepath = results_case01_filepath\n    model = Model(grid_focus_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return grid_focus_mode_config.results_filepath\n\ndef test_run_file_focus_mode():\n    \"\"\"\n    Test run_analysis function with focus area enabled.\n    \"\"\"\n    file_focus_mode_config = WidgetsInterface().export_user_settings()\n    file_focus_mode_config.data_source_path = test01_filepath\n    file_focus_mode_config.focus_area_enabled = True\n    file_focus_mode_config.roi_mode = 'file'\n    file_focus_mode_config.results_filepath = results_case01_filepath\n    model = Model(file_focus_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return file_focus_mode_config.results_filepath\n\ndef test_run_grid_batch_mode():\n    \"\"\"\n    Test run_analysis function with batch mode enabled.\n    \"\"\"\n    grid_batch_mode_config = WidgetsInterface().export_user_settings()\n    grid_batch_mode_config.data_source_path = parent_test_filepath\n    grid_batch_mode_config.batch_mode = True\n    grid_batch_mode_config.results_filepath = results_case02_filepath\n    model = Model(grid_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return grid_batch_mode_config.results_filepath\n\ndef test_run_file_batch_mode():\n    \"\"\"\n    Test run_analysis function with batch mode enabled.\n    \"\"\"\n    file_batch_mode_config = WidgetsInterface().export_user_settings()\n    file_batch_mode_config.data_source_path = parent_test_filepath\n    file_batch_mode_config.batch_mode = True\n    file_batch_mode_config.roi_mode = 'file'\n    file_batch_mode_config.results_filepath = results_case02_filepath\n    model = Model(file_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return file_batch_mode_config.results_filepath\n\ndef test_run_grid_focus_batch_mode():\n    \"\"\"\n    Test run_analysis function with focus and batch mode enabled.\n    \"\"\"\n    grid_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n    grid_focus_batch_mode_config.data_source_path = parent_test_filepath\n    grid_focus_batch_mode_config.batch_mode = True\n    grid_focus_batch_mode_config.focus_area_enabled = True\n    grid_focus_batch_mode_config.results_filepath = results_case03_filepath\n    model = Model(grid_focus_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return grid_focus_batch_mode_config.results_filepath\n\ndef test_run_file_focus_batch_mode():\n    \"\"\"\n    Test run_analysis function with focus and batch mode enabled.\n    \"\"\"\n    file_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n    file_focus_batch_mode_config.data_source_path = parent_test_filepath\n    file_focus_batch_mode_config.batch_mode = True\n    file_focus_batch_mode_config.focus_area_enabled = True\n    file_focus_batch_mode_config.roi_mode = 'file'\n    file_focus_batch_mode_config.results_filepath = results_case03_filepath\n    model = Model(file_focus_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return file_focus_batch_mode_config.results_filepath\n\n\ndef _test_csv_files(relative_filepath_to_csv: str, results_dir: Path) -&gt; bool:\n    filepath = results_dir / relative_filepath_to_csv\n    # confirm results have been created:\n    if not filepath.is_file():\n        return False\n    # confirm computational consistency of results, while allowing minor numerical tolerance\n    df_test = pd.read_csv(filepath)\n    df_validation = pd.read_csv(example_results_dir / relative_filepath_to_csv)\n    if assert_frame_equal(df_test, df_validation) is not None:\n        return False\n    else:\n        return True\n\ndef test_all_peak_results(results_dir):\n    return _test_csv_files('all_peak_results.csv', results_dir)\n\ndef test_amplitude_and_df_over_f_results(results_dir):\n    return _test_csv_files('Amplitude_and_dF_over_F_results.csv', results_dir)\n\ndef test_auc_results(results_dir):\n    return _test_csv_files('AUC_results.csv', results_dir)\n\ndef test_variance_area_results(results_dir):\n    return _test_csv_files('Variance_area_results.csv', results_dir)\n\ndef test_representative_single_trace_results(results_dir):\n    return _test_csv_files('single_traces/data_of_ROI_7-10.csv', results_dir)\n\ndef test_activity_overview_png(results_dir):\n    filepath = results_dir / 'activity_overview.png'\n    return filepath.is_file()\n\ndef test_roi_label_ids_overview_png(results_dir):\n    filepath = results_dir / 'ROI_label_IDs_overview.png'\n    return filepath.is_file()\n\ndef test_individual_traces_with_identified_events_pdf(results_dir):\n    filepath = results_dir / 'Individual_traces_with_identified_events.pdf'\n    return filepath.is_file()\n\ndef test_logs_txt(results_dir):\n    filepath = results_dir / 'logs.txt'\n    return filepath.is_file()\n\ndef test_user_settings_json(results_dir):\n    filepath = results_dir / 'user_settings.json'\n    return filepath.is_file()\n\ndef test_nwb_export(results_dir):\n    filepath = results_dir / 'autogenerated_nwb_file.nwb'\n\ndef test_all_correct_files_created(results_dir: Path, test_csv: bool = True, single_trace: bool = False) -&gt; bool:\n    \"\"\"\n    Check if all expected files have been created in the results directory.\n    \"\"\"\n    # confirm all csv files have been created and are correct:\n    if test_csv:\n        assert test_all_peak_results(results_dir), 'There is an issue with the \"all_peak_results.csv\" file!'\n        assert test_amplitude_and_df_over_f_results(results_dir)\n        assert test_auc_results(results_dir)\n        assert test_variance_area_results(results_dir)\n        if single_trace:\n            assert test_representative_single_trace_results(results_dir)\n\n    # confirm all other result files have been created:\n    assert test_activity_overview_png(results_dir)\n    assert test_roi_label_ids_overview_png(results_dir)\n    assert test_logs_txt(results_dir)\n    assert test_user_settings_json(results_dir)\n    if single_trace:\n        assert test_individual_traces_with_identified_events_pdf(results_dir)\n\n    return True\n\n\ndef find_directories_after_test(base_path):\n    \"\"\"\n    Find directories after test with timestamps.\n    \"\"\"\n    pattern = compile(r'\\d{4}_\\d{2}_\\d{2}_\\d{2}-\\d{2}-\\d{2}_.+')\n\n    matching_dirs = [\n        str(base_path) + d for d in os.listdir(base_path)\n        if os.path.isdir(os.path.join(base_path, d)) and pattern.fullmatch(d)\n    ]\n\n    return matching_dirs\n\ndef delete_directories_after_test(paths_list):\n    \"\"\"\n    Delete directories after test.\n    \"\"\"\n    for res_dir in find_directories_after_test(paths_list):\n        rmtree(res_dir)\n\nRun tests:\n\n# confirm that model can be executed:\nresult_directories = test_correct_model_run()\n\nfor directory in result_directories:\n    assert directory.exists()\n    assert test_all_correct_files_created(directory)\n    # cleanup\n    rmtree(directory)\n\n\n# confirm that model can be executed with custom result directory:\nresults_directory = test_correct_model_run_with_custom_results_dir()\n\nassert results_directory.exists()\n# only one directory with analysis files should be created, as there are only one focus area:\nassert len(list(results_directory.iterdir())) == 1\n\nfor directory in results_directory.iterdir():\n    assert test_all_correct_files_created(directory)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with focus mode:\nresults_directory = test_run_grid_focus_mode()\n\nassert results_directory.exists()\n\n# only one directory should be created, as there are only one focus area:\nassert len(list(results_directory.iterdir())) == 2\nfor directory in results_directory.iterdir():\n    assert test_all_correct_files_created(directory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch mode:\nresults_directory = test_run_grid_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch and focus mode:\nresults_directory = test_run_grid_focus_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    # test case with multiple focus areas\n    if '01' in directory.name:\n        assert len(list(directory.iterdir())) == 2\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with focus mode:\nresults_directory = test_run_file_focus_mode()\n\nassert results_directory.exists()\n\n# only one directory should be created, as there are only one focus area:\nassert len(list(results_directory.iterdir())) == 2\nfor directory in results_directory.iterdir():\n    assert test_all_correct_files_created(directory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch mode:\nresults_directory = test_run_file_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch and focus mode:\nresults_directory = test_run_file_focus_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    # test case with multiple focus areas\n    if '01' in directory.name:\n        assert len(list(directory.iterdir())) == 2\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)",
    "crumbs": [
      "API",
      "model"
    ]
  },
  {
    "objectID": "using_the_gui.html",
    "href": "using_the_gui.html",
    "title": "Using NA³ via the GUI",
    "section": "",
    "text": "Note\n\n\n\nWe´re still putting a few finishing touches onto the new implementation of NA³, so please be aware that this version remains under active development and should not yet be considered as a stable release.",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#launching-the-gui",
    "href": "using_the_gui.html#launching-the-gui",
    "title": "Using NA³ via the GUI",
    "section": "Launching the GUI",
    "text": "Launching the GUI\nAfter you successfully completed the installation of NA³ (e.g. by following our Installation Guide), please open a JupyterNotebook to launch the GUI of NA³.\nTo do this, open the Anaconda Prompt terminal (or the terminal of your choice) and make sure the virtual environment in which you installed NA³ is active. If you followed our installation guide, you should see that the input line starts with (na3). If this is not the case, you can always activate it by running the following command:\n\nconda activate na3\n\nWith the virtual environment in which you installed NA³ activated, please execute the following command in the terminal:\n\njupyter-lab\n\nNext, open a new JupyterNotebook by clicking on the “Python 3 (ipykernel)” button in the Notebook section. In this JupyterNotebook, paste the following code in a cell:\n\nimport neuralactivitycubic as na3\n\nna3.open_gui()\n\nThen, as a final step, execute the cell, for instance by clicking on the little play icon on top while the cell is selected and the GUI of NA³ will open in the Notebook:\n\n\n\n\n\n\n\nTip\n\n\n\nIf you´re working on a smaller screen, e.g. if you´re using a notebook, you can use the keyboard shortcut Ctrl + b to collapse the file explorer panel of JupyterLab to the left of the JupyterNotebook to give you some extra horizontal space. You can always use the same keyboard shortcut again to bring it back again if needed.",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#quick-walkthrough",
    "href": "using_the_gui.html#quick-walkthrough",
    "title": "Using NA³ via the GUI",
    "section": "Quick Walkthrough",
    "text": "Quick Walkthrough\nWe´re all busy people and documentation can be quite exhaustive to read. Thus, here´s a quick walkthrough of NA³´s core functionalities that should allow you to get going with your own first tests:\n\nFor more detailed descriptions of the individual features and settings, please see the sections below.",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#general-settings",
    "href": "using_the_gui.html#general-settings",
    "title": "Using NA³ via the GUI",
    "section": "General Settings",
    "text": "General Settings\nThe first section of NA³’s GUI prompts you to specify the general settings of your analysis before being able to continue. Most importantly, the selections will have an impact on the structure and organization of your source data that NA³ expects. Thus, please make sure you consult the Source Data Structure section below to check what structure is requested for your specific combination of settings.\n\nROI modes\nNA³´s core functionality is to compute the signal intensity over time for defined regions of interest (ROIs) within the whole image. You either have the chance to provide source data that defines these ROIs (“Predefined ROIs”), or you can use NA³´s “Grid” mode to automatically create congruent square ROIs over your recording with adjustable sizes (Grid size - see Analysis Settings) that will be analyzed.\n\nGrid (congruent squares)\nThe main advantage of NA³´s Grid mode is that it does not require any additional input other than your recording file to start the analysis with NA³. This can be especially useful in high-throughput settings, as it eliminates additional (potentially manual) preprocessing steps to generate ROIs.\n\n\nPredefined ROIs\nSometimes you want to investigate specific features instead of the whole field of view, though. For this, NA³ provides the option for you to provide predefined ROIs along with your source data. Currently, only ROIs created with the ImageJ tools are supported.\n\n\n\n\n\n\nTip\n\n\n\nThe predefined ROI filetype that your are using is currently not supported? That needs to be changed! Please drop us your feature request via GitHub and we´ll implement it for you asap.\n\n\n\n\n\nBatch mode\nThe period of trouble shooting and of adjusting individual experimental parameters is finally over and you are ready to process all your data at once? That´s awesome, congrats! As an additional reward, NA³ offers you a batch mode feature: You select your entire dataset as source data and then only have to specify the settings once, hit “Run Analysis” and then leave for a well deserved end-of-day, while NA³ and your computer do the heavy lifting for you and process your entire dataset, recording after recording. Batch mode is compatible with all ROI-modes (Grid & predefined ROIs), and can even be combined with Focus Areas.\n\n\nFocus areas\nThis feature is for those of you who want - or may need - to squeeze out every single percent of processing speed. It allows you to provide another ROI (or even set of ROIs), to which NA³ will restrict it´s analysis, while ignoring any ROIs that reside outside of these Focus Areas. Since processing of each ROI takes ~ the same amount of time (it can be considered a linear operation), not processing 50 % of the ROIs will cut your processing time almost in half! We tried to illustrate this with the following example, in which we used NA³´s Grid mode with the Focus Area being disabled or enabled, saving ~2/3´s of processing time:\n\n\n\nFocus Area mode to max out performance.\n\n\n\n\nSource data structure\nNA³ expects your source data to be organized in a very specific manner for it to function correctly, and this structure looks slightly different, depending on which processing modes you want to use. We actually spent quite some time thinking about it, with the intention of creating a structure that does not result in conflicts when you start switching between modes when exploring what method fits your data best and requires you to constantly re-adjust your source data. But this will only become evident over time when people like you test and use NA³ - so we´d highly appreciate your feedback, especially if you should run into some issues with the required source data structures! Please open an issue on GitHub and we´re eager to fix it.\nTo understand what source data structure is expected for your specific combination of processing modes, we provide you with representative examples for each. Please expand the following sections to browse through them. In all cases, the “Source Data Path” that you have to select when using NA³ is always the file/folder that is shown at the very top and is highlighted in magenta (e.g. the parent directory your_dataset/ or single_recording/, or the recording file itself your_recording_file.avi):\n\n\n\n\n\n\nROI-mode: Grid\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nThis is as straightforward as it gets! All you need to do is select the recording file you´d like to analyze:\n\n\n\nBatch-mode: OFF | ROI-mode: Grid (congruent squares) | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nThis requires you to organize your data within a directory (called single_recording in this example) which contains your recording file and another subdirectory in which the Focus Area ROI(s) are located. This subdirectory must be called focus_areas for NA³ to be able to recognize it:\n\n\n\nBatch-mode: OFF | ROI-mode: Grid (congruent squares) | Focus Area: ON\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROI-mode: predefined ROIs\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nUsing predefined ROIs for analysis rather than the automatically generated Grid requires you to organize your source data within a directory (called single_recording in this example), which contains a single recording file (e.g. your_recording.avi in this example) and as many ROI files as you need. In this example, there is only a single predefined_ROIs.zip file, which was generated with ImageJ2 and which contains multiple ROIs. You could, however, also provide each ROI individually, e.g. as ROI_a.roi, ROI_b.roi, and so on.\n\n\n\nBatch-mode: OFF | ROI-mode: predefined ROIs | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nUsing predefined ROIs for analysis rather than the automatically generated Grid requires you to organize your source data within a directory (called single_recording in this example), which contains a single recording file (e.g. your_recording.avi in this example) and as many ROI files as you need. In this example, there is only a single predefined_ROIs.zip file, which was generated with ImageJ2 and which contains multiple ROIs. You could, however, also provide each ROI individually, e.g. as ROI_a.roi, ROI_b.roi, and so on.\nFor NA³ to differentiate between predefined ROIs you´d like to analyze, and ROIs that are intended as Focus Areas, you have to provide the Focus Area ROIs within a subdirectory that must be named focus_areas. In this directory, you can provide as many ROI files as needed, just like for the predefined ROIs.\n\n\n\nBatch-mode: OFF | ROI-mode: predefined ROIs | Focus Area: ON\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch mode: ON\n\n\n\n\n\nWith activate batch mode, you essentially have to provide the same data structure as described above for each individual recording, and then organize all recordings as nested directory. You can find some representative examples again for all possible mode combinations with enabled batch mode below:\n\n\n\n\n\n\nROI-mode: Grid\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nIn this case, you only need to separate the respective recording files in individual subdirectories:\n\n\n\nBatch-mode: ON | ROI-mode: Grid (congruent squares) | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nWith enabled Focus Area, you can specify focus area ROIs for each recording individually. For this, you have to add a subdirectory called focus_areas within the respective recording subdirectory. Please note, that you can add different focus area ROIs for each recording, and you can also opt to not provide any focus area ROIs for individual recordings - here, the entire field of view will then be analyzed.\n\n\n\nBatch-mode: ON | ROI-mode: Grid (congruent squares) | Focus Area: ON\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROI-mode: predefined ROIs\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nIf you´d like to use batch mode processing in combination of using predefined ROIs for analyses, please organize your individual recordings together with the corresponding ROI files into separate subdirectories. Please note, that you can (obviously) provide different ROI files for each recording, and that they also don´t have to be of the same filetype (e.g. .zip and .roi files in this example):\n\n\n\nBatch-mode: ON | ROI-mode: predefined ROIs | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nTo use predefined ROIs with enabled Focus Area and batch mode, please provide your data organized as depicted below. Again, please note that any combination of ROI files - for both predefined ROIs and Focus Areas - is possible. For the Focus Areas, it´s also supported to not provide ROIs for individual recordings. You always have to provide some predefined ROIs, though, if you select this mode.\n\n\n\nBatch-mode: ON | ROI-mode: predefined ROIs | Focus Area: ON",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#analysis-settings",
    "href": "using_the_gui.html#analysis-settings",
    "title": "Using NA³ via the GUI",
    "section": "Analysis Settings",
    "text": "Analysis Settings\nNA³ provides you with several option to configure the analysis exactly as you need it for your specific dataset. There are a few settings, however, that are strictly required (grouped here in the “Analysis Settings”) and some that are optional (correpsondingly in the “Optional Settings”). For all required settings, there are default values specified that NA³ will use if you don´t change them. They certainly provide a generic good-fit, but you might want to tweak some of them as needed to accomodate for some special characteristics of your individual dataset!\n\nGrid Size\nAllows you to specify the size in pixel of the congruent squares that will be generated and used as ROIs for analysis. For instance, selecting “10” means that NA³ will create 10 x 10 pixel squares as ROIs for analysis. You can use the Preview button next to the slider widget to create a preview of how the grid would look like on your specific data. Moreover, NA³ will highlight the area within the field of view within which the indicated squares will be analyzed by a cyan box. This can be of interest, as NA³ might crop your image - depending on the dimensions of your field of view and the selected grid size - to ensure equal sizes of the square ROIs across the entire field of view. Again, as an example, if your field of view is 432 x 567 px, and you specify a grid size of 10 x 10 px, NA³ will crop your field of view by 2 and 7 pixels, respectively - i.e. to 430 x 560 px - to be able to fill the entire (remaining) field of view with congruent squares.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this setting is only available in the ROI mode “Grid (congruent squares)” is selected.\n\n\n\n\nSignal-to-Noise Ratio (SNR)\nTo identify peaks within the signal intensity trace (i.e. a timeseries of bit-values) for each analysis ROI, NA³ leverages the find_peaks_cwt function implemented by SciPy. The SNR value you specify in NA³ will be used as min_snr in when calling the find_peaks_cwt function.\n\n\nNoise Window Size (NWS)\nThe SNR value you specify in NA³ will be used as window_size in when calling the find_peaks_cwt function.\n\n\nSignal Average Threshold (SAT)\nThis value allows you to correct for global background noise. For each analysis ROI, NA³ computes the mean intensity value over the entire timeseries for the specified analysis interval. Only if this “average signal” matches or exceeds the defined SAT value, peak detection computations will be performed for this analysis ROI.\n\n\nMinimal Activity Counts (MAC)\nAllows to filter out analysis ROIs with too few peaks. If there were less peaks detected for a given analysis ROI than defined as MAC, this ROI will be excluded from the final results.\n\n\nBaseline Estimation Method\nNA³ computes area-under-curve for the identified peaks. In order to do so, a baseline for the signal is required. For this, NA³ leverages a selection of the baseline estimation methods offered by pybaselines:\n\nAsymmetric Least Squares: Uses the asls method with it´s default values. See here for additional information and representative baseline computations using this method.\nFully Automatic Baseline Correction: Uses the fabc method with it´s default values. See here for additional information and representative baseline computations using this method.\nPeaked Signal’s Asymmetric Least Squares Algorithm: Uses the asls method with it´s default values. See here for additional information and representative baseline computations using this method.\nStandard Deviation Distribution: Uses the asls method with it´s default values. See here for additional information and representative baseline computations using this method.\n\n\n\n\n\n\n\nNot the Baseline Estimation method you´re looking for?!\n\n\n\nWith the highly modular design of NA³, it is very straightforward for us to implement any of the baseline estimation methods provided by the pybaselines Python package. So if there is another one that sparks your interest or is absolutely crucial for your specific analysis - don´t hesitate for even a second! Please drop us your request directly via GitHub and we´ll be happy to implement it for you.",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#optional-settings",
    "href": "using_the_gui.html#optional-settings",
    "title": "Using NA³ via the GUI",
    "section": "Optional settings",
    "text": "Optional settings\nNA³ also provides you with some optional settings. These are - as their category indicates - not required for processing of your Calcium Imaging data, but enable you to make some very specifc adjustments. To enable them, click the check box and the corresponding settings widget will appear - uncheck the checkbox again, and the settings widgets dissapear (and the settings you made will be ignored).\n\nInclude Variance\nAs described in the original paper by Prada et al. (2018), the signal´s variance can be used as a proxy for neuronal excitability1. To enable variance computation of the signal for your analysis, click the checkbox next to “include variance”. A new widget will appear and allow you to specify the Variance value (default: 15). This is the window size that will be used to compute the variance of the signal intensity for each analysis ROI with a sliding window approach.\n\n\nAnalyze Interval\nIf you are interested in analyzing only a specific interval of your recording, this feature is for you. Simply enable it by clicking the checkbox and another widget will appear, allowing you to use two sliders that specify start & end frame of the interval you´d like to analyze (inclusive ends, e.g. 12 - 55 means that you will analyze the interval starting at frame 12 and ending at frame 55, with both frames 12 & 55 included).\n\n\n\n\n\n\nImportant\n\n\n\nIf you´re using this feature in combination with enabled batch mode processing, please be aware that you are not able to configure the interval for each individual recording, but that the same frame interval will be used for all recordings in this batch!\n\n\n\n\nConfigure Octaves\nYou can enable this feature again by clicking the checkbox next to it, which will cause another widget to appear, allowing you to specify the Min. Octaves. As described above, NA³ uses the find_peaks_cwt function implemented in SciPy. You can use this optional Min. Octaves setting to change the value that NA³ will pass to this function as min_length. However, this is not a direct mapping, but min_length will be computed from the Min. Octaves value you specified, in dependence of the length (i.e. number of frames) of the respective recording. The exact computation is as follows, where:\n\nself.mean_intensity_over_time.shape[0] = Number of frames in the recording (potentially limited by analysis interval)\nmin_octave_span = the value you set via Min. Octaves\n\n\nwidths = np.logspace(np.log10(1), np.log10(self.mean_intensity_over_time.shape[0]), 100)\nmin_length = octaves_ridge_needs_to_spann / np.log2(widths[1] / widths[0])\n\n\n\n\n\n\n\nWarning\n\n\n\nIn general, we don´t recommend changing this value unless you are highly familiar with NA³ and have utilized all other configuration options already.",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#result-settings",
    "href": "using_the_gui.html#result-settings",
    "title": "Using NA³ via the GUI",
    "section": "Result settings",
    "text": "Result settings\nThese settings allow you to specify whether you want NA³ to create result files for your current analysis (e.g. to avoid creation of result files while you´re still testing different setting configurations or are testing different processing modes). Creation of all result files is enabled by default.\n\nSave Overview Plot\nIf you uncheck this box, the overview.png fill will not be created.\n\n\nSave Detailed Results\nIf you uncheck this box, none of the other result files will be created, i.e.: - all_peak_results.csv - Amplitude_and_dF_over_F_results.csv - AUC_results.csv - Individual_traces_with_identified_events.pdf - logs.txt - ROI_label_IDs_overview.png - user_settings.json - Variance_area_results.csv",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#generated-results",
    "href": "using_the_gui.html#generated-results",
    "title": "Using NA³ via the GUI",
    "section": "Generated Results",
    "text": "Generated Results\nNA³ generates several comprehensive result files for you (see list above). You will always find the results in the same directory in which the corresponding recording was located. That means, in case you´ve been using the batch processing mode, NA³ will create separate result folders in each recording subdirectory. The result folders will have the following naming conventions:\n\ndatetime_results_for_recording_filename, e.g.: 2024_12_17_13-48-54_results_for_spiking_neuron\n\nor\n\ndatetime_results_for_recording_filename_with_focus_area_roi_filename, e.g.: 2024_12_17_13-48-54_results_for_spiking_neuron_with_area_a\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are using more than one Focus Area ROI, NA³ will create a new results directory for each of them, to ensure you can easily differentiate between them.\n\n\nLet´s take a look at them in more details:\n\nLogs & User Settings\nUpfront we have two files that NA³ creates that are intended to ensure reproducibility, namely user_settings.json and logs.txt. While the human and machine readable JSON file makes sure you never forget to note down what the specific settings for this very analyses were, the Logs file saves some additional information regarding the processing and might be of value in case there is some troubleshooting or debugging that needs to happen.\n\n\nActivity Overview\nNA³ offers an elegant way to capture high-level activity patterns across all ROIs of your field of view at a glance. For this, the “Activity Overview” plot is generated, that can be saved both as a separate PNG image (activity_overview.png) and as part of the wholistic PDF (Individual_traces_with_identified_events.pdf). It will look something like this:\n\n\n\nRepresentative Activity Overview Plot\n\n\nIt will indicate the respective ROIs with the total detected peaks per ROI, after applying all thresholding and filtering criteria (e.g. Minimum Activity Count). The plots title also informs you about the total activity detected, i.e. the sum of all individually registered peaks across all ROIs within the field of view. In this example, the Grid method was used to automatically generate congruent squares covering most of the field of view (cropping boundaries are indicated by cyan border). The x- and y-axis labels provide a unique XY ID for each of the squares, which allows you to match the information presented in other plots or result files to a uniquely identifiable ROI.\n\n\n\n\n\n\nWhat are the unique IDs of my predefined ROIs?\n\n\n\nThere is also a ROI_label_IDs_overview.png file that NA³ creates, which shows the ID of each ROI within it´s respective outline - which is specifically in case of predefined ROIs an important reference, since they cannot be defined using XY coordinates in a Grid. For reasons of better visibility & readability, only the IDs of ROIs that remain after applying all fitering & thresholding critera are shown in these overviews:\n\n\n\nUnique label IDs for predefined ROIs\n\n\n\n\n\n\nActivity Traces with Identified Events\nFor visual inspection, the PDF Individual_traces_with_identified_events.pdf gives you an even more detailed overview of your analysis results - and likely also allows you to assess it´s quality with the current settings. The first three pages of the PDF contain the following plots:\n\nThe Activity Overview plot\nA brightness and contrast enhanced image of your field of view with only the ROIs superimposed (no activity counts)\nThe ROI label ID Overview plot\n\nOn all subsequent pages of this PDF will be one plot for each ROI with sufficient activity according to your settings. Each plot shows the extracted signal intensity trace (bit values on y-axis and frame number on x-axis) in gray and the estimated baseline in cyan. Identified peaks are marked with dots on the signal intensity trace, where a magenta coloration indicates that an area-under-curve (AUC) value was calculated for the respective event, while black coloration indicates that this was not possible (this is usually the case if the signal trace did not cross the estimated baseline on both sides of the peak, i.e. usually at the beginning or at the very end of the recording). Moreover, a yellow fill indicates the identified AUC, respectively.\n\n\n\nRepresentative signal trace plot with identified activity events\n\n\n\n\nCSV-encoded Results\n\nAll Peak Results\nThis table comprises all computed characteristics for each identified activity peak across all ROIs. This information can be found in each column:\n\nROI label ID: The unique label ID that allows you to identify the ROI, e.g. “3/15” in Grid mode or “7” when using predefined ROIs, as described above.\npeak frame index: The frame index at which the respective peak was identified.\npeak bit value: The raw bit value of the signal trace at the indicated peak frame index. Remember: the computed signal trace represents the average intensity across all pixels within the given ROI.\npeak amplitude: The absolute difference between the peak bit value and the value of the estimated baseline at the indicated peak frame index, calculated as: peak bit value - bit value estimated baseline\npeak dF/F: The relative difference between peak bit value and the value of the estimated baseline at the indicated peak frame index, calculated as: peak amplitude / bit value estimated baseline\npeak AUC: The value of the area-under-curve associated with this peak. See the warning below, however.\npeak classification: A classification of whether the peak is singular, clustered, or isolated:\n\nisolated: A peak for which no area-under-curve could be calculated. This is usually the case, if the signal trace did not cross the estimated baseline on both sides of the peak, i.e. usually at the beginning or at the very end of the recording.\nsingular: A peak for which an area-under-curve could be calculated that is solely associated with this singular peak, i.e. the signal trace crosses the baseline between this and any preceeding or following peaks.\nclustered: A peak for which an area-under-curve could be calculated, but it is associated with multiple peaks, i.e. the signlar trace did not cross the baseline between this and a preceeding or following peak.\n\n\n\n\n\n\n\n\nAUCs of clustered peaks\n\n\n\nIn case of clustered peaks, NA³ cannot clearly associate a given AUC value with a single peak, because the signal trace does not cross the estimated baseline between two (or more) peaks. Therefore, both peaks will be listed with the identical AUC value - which you may have to take into account when you perform statistical analysis of your data. We decided to include them, though, because these clustered peaks could also represent event trains, which might be interesting features to look at for some of you.\n\n\n\n\nArea-under-Curve (AUC) Results\nEssentially the same information about AUC values per peak, organized in a different format, though: wide instead of long.\n\n\nAmplitude and dF/F Results\nEssentially the same information about amplitude and dF/F values per peak, organized in a different format, though: wide instead of long.\n\n\nVariance Results (optional)\nVariance results will only be generated if you enabled the optional setting “Include Variance”. It comprises the computed variance area per ROI.",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#wed-love-to-hear-your-feedback",
    "href": "using_the_gui.html#wed-love-to-hear-your-feedback",
    "title": "Using NA³ via the GUI",
    "section": "We´d love to hear your Feedback:",
    "text": "We´d love to hear your Feedback:\nYou are using NA³ for your research - or would like to do so, but there´s that one cool feature missing for you? That´s great! We´d love to hear your feedback, feature requests, or bug reports to keep improving NA³ - please use this form on GitHub to submit it, we appreciate it a lot!\nPlease feel free to also explore all other Research Software that our not-for-profit organization has developed, or learn more about us and what we do on our website. If you are interested to partner with us for a similar collaboration on your Research Software, or to develop it with or for you from scratch, don´t hesitate and drop us an email - we´re always looking for new opportunities to catalyze science be delivering cutting-edge technology tailored to the specific research challenges of our collaborators!\n\n\n\nIndoc Research Europe gGmbH",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#footnotes",
    "href": "using_the_gui.html#footnotes",
    "title": "Using NA³ via the GUI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrada J, Sasi M, Martin C, Jablonka S, Dandekar T, Blum R (2018) An open source tool for automatic spatiotemporal assessment of calcium transients and local ‘signal-close-to-noise’ activity in calcium imaging data. PLoS computational biology 14(3): e1006054. https://doi.org/10.1371/journal.pcbi.1006054↩︎",
    "crumbs": [
      "User Guides",
      "Using NA³ via the GUI"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "results",
    "section": "",
    "text": "source\n\nplot_roi_boundaries\n\n plot_roi_boundaries (roi:Union[neuralactivitycubic.input.ROI,neuralactivi\n                      tycubic.analysis.AnalysisROI], line_color:str,\n                      line_style:str, line_width:Union[int,float])\n\n\nsource\n\n\nplot_window_size_preview\n\n plot_window_size_preview (preview_image:numpy.ndarray,\n                           grid_configs:Dict[str,Any], focus_area_roi:Opti\n                           onal[neuralactivitycubic.input.ROI]=None)\n\n\nsource\n\n\nplot_activity_overview\n\n plot_activity_overview (analysis_rois_with_sufficient_activity:Union[List\n                         [neuralactivitycubic.analysis.AnalysisROI],List],\n                         preview_image:numpy.ndarray,\n                         indicate_activity:bool=False, focus_area:Optional\n                         [neuralactivitycubic.input.ROI]=None,\n                         grid_configs:Optional[Dict[str,Any]]=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nanalysis_rois_with_sufficient_activity\nUnion\n\nWith python 3.11 - change back to: List[Never]\n\n\npreview_image\nndarray\n\n\n\n\nindicate_activity\nbool\nFalse\n\n\n\nfocus_area\nOptional\nNone\n\n\n\ngrid_configs\nOptional\nNone\n\n\n\nReturns\nTuple\n\n\n\n\n\n\nsource\n\n\nplot_rois_with_label_id_overview\n\n plot_rois_with_label_id_overview (analysis_rois_with_sufficient_activity:\n                                   Union[List[neuralactivitycubic.analysis\n                                   .AnalysisROI],List],\n                                   preview_image:numpy.ndarray, focus_area\n                                   :Optional[neuralactivitycubic.input.ROI\n                                   ]=None, grid_configs:Optional[Dict[str,\n                                   Any]]=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nanalysis_rois_with_sufficient_activity\nUnion\n\nWith python 3.11 - change back to: List[Never]\n\n\npreview_image\nndarray\n\n\n\n\nfocus_area\nOptional\nNone\n\n\n\ngrid_configs\nOptional\nNone\n\n\n\nReturns\nTuple\n\n\n\n\n\n\nsource\n\n\nplot_intensity_trace_with_identified_peaks_for_individual_roi\n\n plot_intensity_trace_with_identified_peaks_for_individual_roi\n                                                                (analysis_\n                                                                roi:neural\n                                                                activitycu\n                                                                bic.analys\n                                                                is.Analysi\n                                                                sROI)\n\n\nsource\n\n\nexport_peak_results_df_from_analysis_roi\n\n export_peak_results_df_from_analysis_roi\n                                           (analysis_roi:neuralactivitycub\n                                           ic.analysis.AnalysisROI)\n\n\nsource\n\n\ncreate_single_roi_amplitude_and_delta_f_over_f_results\n\n create_single_roi_amplitude_and_delta_f_over_f_results\n                                                         (df_all_results_s\n                                                         ingle_roi:pandas.\n                                                         core.frame.DataFr\n                                                         ame,\n                                                         zfill_factor:int)\n\n\nsource\n\n\ncreate_single_roi_auc_results\n\n create_single_roi_auc_results\n                                (df_all_results_single_roi:pandas.core.fra\n                                me.DataFrame, zfill_factor:int)",
    "crumbs": [
      "API",
      "results"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to NeuralActivityCubic",
    "section": "",
    "text": "NeuralActivityCubic (NA³) is an open-source calcium image analysis tool published in 2018 by J. Prada and colleagues1, who describe it as following in their Author Summary:\nSince its publication in 2018, updates to several software packages on which the original implementation of NA3 depends have rendered this version of NA³ virtually un-installable and, thus, effectively inaccessible for its target user audience - the Neuroscientific Community. Given the continued interest in NA³, however, this was not acceptable. Thus, we formed a collaboration between the original developers of NA³ and research software engineering experts from the not-for-profit organization Indoc Research Europe to revamp NA³, with the goal of making it easily accessible to the Neuroscientific Community once again. While on it, we also enhanced NA³´s performance, it´s scope of features, and it´s maintainability to ensure NA³ remains accessible moving forward. Today, we´re happy to present to you this revamped version of NA³ - we hope you´ll like it!\nNote: We´re still putting a few finishing touches on this new implementation of NA³, so please be aware that this version remains under active development and should not yet be considered as a stable release. We´re currently also working on a paper describing our work in more details, so make sure you stay tuned and regularly check these docs for updates!",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Welcome to NeuralActivityCubic",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nIf you´re comfortable working with virtual Python environments and installing packages via command line interfaces, please follow one of the subsequent options to install NA³. If you´d prefer a full step-by-step guide instead, we also got you covered: please find our detailed installation guide here.\nInstall latest from GitHub:\n$ pip install git+https://github.com/Indoc-Research/neuralactivitycubic.git\nor from pypi\n$ pip install neuralactivitycubic\n\n\nDocumentation\nDocumentation for NA³ can be found here.",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#how-to-use---quick-start",
    "href": "index.html#how-to-use---quick-start",
    "title": "Welcome to NeuralActivityCubic",
    "section": "How to use - quick start:",
    "text": "How to use - quick start:\nAfter installing neuralactivitycubic, open a Jupyter Notebook and execute the following code to launch the GUI of NA³:\n\nimport neuralactivitycubic as na3\n\nna3.open_gui()\n\n\n\n\nGUI of NA³.",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "Welcome to NeuralActivityCubic",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall NeuralActivityCubic in Development mode\n# make sure NeuralActivityCubic package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to NeuralActivityCubic\n$ nbdev_prepare",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Welcome to NeuralActivityCubic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrada J, Sasi M, Martin C, Jablonka S, Dandekar T, Blum R (2018) An open source tool for automatic spatiotemporal assessment of calcium transients and local ‘signal-close-to-noise’ activity in calcium imaging data. PLoS computational biology 14(3): e1006054. https://doi.org/10.1371/journal.pcbi.1006054↩︎",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "input.html",
    "href": "input.html",
    "title": "input",
    "section": "",
    "text": "source",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "input.html#source-data-handler",
    "href": "input.html#source-data-handler",
    "title": "input",
    "section": "Source Data Handler:",
    "text": "Source Data Handler:\n\nsource\n\nData\n\n Data (filepath:pathlib.Path, loaded_data:Any)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nRecording\n\n Recording (filepath:pathlib.Path, loaded_data:Any)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nROI\n\n ROI (filepath:pathlib.Path, loaded_data:Any)\n\nHelper class that provides a standard way to create an ABC using inheritance.",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "input.html#source-data-loader",
    "href": "input.html#source-data-loader",
    "title": "input",
    "section": "Source Data Loader:",
    "text": "Source Data Loader:\n\nsource\n\nDataLoader\n\n DataLoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nGridWrapperROILoader\n\n GridWrapperROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nRecordingLoader\n\n RecordingLoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nAVILoader\n\n AVILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nNWBRecordingLoader\n\n NWBRecordingLoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nROILoader\n\n ROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nImageJROILoader\n\n ImageJROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nNWBROILoader\n\n NWBROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "input.html#loader-factories",
    "href": "input.html#loader-factories",
    "title": "input",
    "section": "Loader Factories",
    "text": "Loader Factories\n\nsource\n\nDataLoaderFactory\n\n DataLoaderFactory ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nRecordingLoaderFactory\n\n RecordingLoaderFactory ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nROILoaderFactory\n\n ROILoaderFactory ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nget_filepaths_with_supported_extension_in_dirpath\n\n get_filepaths_with_supported_extension_in_dirpath (dirpath:pathlib.Path,\n                                                    all_supported_extensio\n                                                    ns:List[str], max_resu\n                                                    lts:Optional[int]=None\n                                                    )\n\n\nsource\n\n\nRecLoaderROILoaderCombinator\n\n RecLoaderROILoaderCombinator (dir_path:pathlib.Path)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n# Specification of filepaths that can be used for testing:\n\n### ImageJ ROI filepaths:\nvalid_imagej_roi_filepath_1 = Path('../test_data/00/RoiSet_spiking.zip')\nvalid_imagej_roi_filepath_2 = Path('../test_data/00/focus_area/focus_spiking.roi')\n### NWB ROI filepath:\nvalid_nwb_roi_filepath = Path('../test_data/02/spiking_neuron.nwb')\n\n### AVI Recording filepath:\nvalid_avi_recording_filepath = Path('../test_data/00/spiking_neuron.avi')\n### NWB Recording filepath:\nvalid_nwb_recording_filepath = Path('../test_data/02/spiking_neuron.nwb')\n\n### A filepath of an unsupported file type:\nunsupported_filepath = Path('../test_data/00/example_test_results_for_spiking_neuron/activity_overview.png')\n\n\nroi_loader_factory = ROILoaderFactory()\n\n# tests that the ROILoaderFactory returns the correct Loader subclass for the respective file types:\nassert isinstance(roi_loader_factory.get_loader(valid_imagej_roi_filepath_1), ImageJROILoader)\nassert isinstance(roi_loader_factory.get_loader(valid_imagej_roi_filepath_2), ImageJROILoader)\nassert isinstance(roi_loader_factory.get_loader(valid_nwb_roi_filepath), NWBROILoader)\n\n# tests that the ROILoaderFactory raises a NotImplementedError for an unsupported file type:\nassert test_unsupported_file_extension(roi_loader_factory, unsupported_filepath)\n\n# tests that the individual Loader subclass correctly loads and parses the file content into ROI instances:\nassert test_successful_roi_loading(valid_imagej_roi_filepath_1)\nassert test_successful_roi_loading(valid_imagej_roi_filepath_2)\nassert test_successful_roi_loading(valid_nwb_roi_filepath)\n\n\nrecording_loader_factory = RecordingLoaderFactory()\n\n# tests that the RecordingLoaderFactory returns the correct Loader subclass for the respective file types:\nassert isinstance(recording_loader_factory.get_loader(valid_avi_recording_filepath), AVILoader)\nassert isinstance(recording_loader_factory.get_loader(valid_nwb_recording_filepath), NWBRecordingLoader)\n\n# tests that the RecordingLoaderFactory raises a NotImplementedError for an unsupported file type:\nassert test_unsupported_file_extension(recording_loader_factory, unsupported_filepath)\n\n# tests that the individual Loader subclass correctly loads and parses the file content into Recording instances:\nassert test_successful_recording_loading(valid_avi_recording_filepath)\nassert test_successful_recording_loading(valid_nwb_recording_filepath)",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "controller.html",
    "href": "controller.html",
    "title": "controller",
    "section": "",
    "text": "source\n\nApp\n\n App ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nopen_gui\n\n open_gui ()\n\nStart the interactive widgets interface for NeuralActivityCubic\n\n\nGUI Tests Dependencies\n\nimport os\nimport uuid\nimport sys\nimport subprocess\nimport ipykernel\nimport requests\n\nfrom time import sleep\nfrom pathlib import Path\nfrom playwright.async_api import expect\nfrom playwright.async_api import ViewportSize\nfrom playwright.async_api import async_playwright\nfrom jupyter_server import serverapp\nfrom jupyter_server.services.contents.filemanager import FileContentsManager\nfrom contextlib import contextmanager\nfrom contextlib import asynccontextmanager\nfrom nbformat import v4 as nbformat\n\n\ndef get_kernel_id() -&gt; str | None:\n    try:\n        connection_file = ipykernel.get_connection_file()\n        return Path(connection_file).stem.split('-', 1).pop(-1)\n    except RuntimeError:\n        return None\n\n\ndef find_server_info_by_kernel_id(kernel_id: str):\n    servers = list(serverapp.list_running_servers())\n    for server in servers:\n        try:\n            response = requests.get(f\"{server['url']}api/sessions\", headers={'Authorization': f\"token {server['token']}\"})\n            for session in response.json():\n                if session['kernel']['id'] == kernel_id:\n                    return server\n        except Exception as e:\n            continue\n\n    return None\n\n\ndef find_server_info_by_process_id(process_id: int):\n    servers = list(serverapp.list_running_servers())\n    for server in servers:\n        if server['pid'] == process_id:\n            return server\n\n    return None\n\n\n@contextmanager\ndef run_jupyter_server_app(project_root: str):\n    jupyter_process = subprocess.Popen([sys.executable, '-m', 'jupyter', 'server', f'--ServerApp.root_dir={project_root}', '--ServerApp.open_browser=false'])\n\n    sleep(10)  # Wait for the server to start\n\n    yield find_server_info_by_process_id(jupyter_process.pid)\n\n    jupyter_process.terminate()\n\n\n@contextmanager\ndef get_jupyter_server():\n    kernel_id = get_kernel_id()\n\n    if kernel_id:\n        yield find_server_info_by_kernel_id(kernel_id)\n    else:\n        with run_jupyter_server_app(project_root=Path.cwd().parent) as server_info:\n            yield server_info\n\n\n@contextmanager\ndef create_temporary_notebook(cells: list[str], kernel_name: str = 'python3'):\n    fcm = FileContentsManager()\n    name = f\"test-{uuid.uuid4()}.ipynb\"\n    try:\n        notebook = nbformat.new_notebook()\n        notebook['metadata'] = {'kernelspec': {'name': kernel_name, 'display_name': kernel_name}}\n        for cell in cells:\n            notebook['cells'].append(nbformat.new_code_cell(source=cell))\n        fcm.save({'type': 'notebook', 'content': notebook}, name)\n        yield name\n    finally:\n        fcm.delete(name)\n\n\n@asynccontextmanager\nasync def provide_playwright_page(headless: bool):\n    browser = 'chromium'\n    if headless:\n        browser = 'chromium-headless-shell'\n    subprocess.run([sys.executable, '-m', 'playwright', 'install', '--with-deps', browser])\n\n    async with async_playwright() as playwright:\n        browser_type = playwright.chromium\n        browser = await browser_type.launch(headless=headless)\n        page = await browser.new_page(viewport=ViewportSize(width=1280, height=1024))\n\n        yield page\n\n        await browser.close()\n\n\n@asynccontextmanager\nasync def provide_na3_gui(visible: bool = True):\n    with get_jupyter_server() as server_info, create_temporary_notebook(cells=['import neuralactivitycubic as na3; na3.open_gui()']) as notebook_name:\n        token = server_info['token']\n        lab_url = server_info['url'] + 'lab/tree/nbs'\n\n        async with provide_playwright_page(headless=not visible) as page:\n            await page.goto(f'{lab_url}/{notebook_name}?token={token}')\n\n            await expect(page.get_by_role('main')).to_contain_text(notebook_name)\n\n            selected_tab_id = await page.get_by_role('main').get_by_role('tab', selected=True).first.get_attribute('data-id')\n            selected_tab = page.locator(f'#{selected_tab_id}').first\n            await expect(selected_tab).to_be_visible()\n\n            try:\n                await expect(page.locator('#filebrowser')).to_be_visible(timeout=1)\n                await page.locator('li[data-id=\"filebrowser\"]').click()\n            except AssertionError:\n                pass\n\n            kernel_status = selected_tab.locator('.jp-Notebook-ExecutionIndicator')\n            await expect(kernel_status).to_have_attribute('data-status', 'idle')\n            await page.wait_for_timeout(timeout=1000)\n\n            run_menu = page.get_by_role('menuitem').filter(has=page.get_by_text('Run')).first\n            await run_menu.click()\n\n            run_all_cells_option = page.get_by_role('menuitem').filter(has=page.get_by_text('Run All Cells')).first\n            await expect(run_all_cells_option).to_be_visible()\n            await run_all_cells_option.click()\n\n            na3_gui = selected_tab.locator('.box-na3-gui')\n            await expect(na3_gui).to_be_visible()\n\n            yield na3_gui\n\n\nvisible = os.environ.get('CI') != 'true'\n\nasync with provide_na3_gui(visible=visible) as gui:\n    general_settings = gui.locator('.box-general-settings')\n    assert not await general_settings.get_by_role(\"button\", name=\"Select\").is_disabled()\n    assert await general_settings.get_by_role(\"button\", name=\"Load Data\").is_disabled()\n\n    analysis_settings = gui.locator('.box-analysis-settings')\n\n    buttons = await analysis_settings.locator(\"button\").all()\n    assert buttons\n    for button in buttons:\n        assert await button.is_disabled()\n\n    inputs = await analysis_settings.locator(\"input\").all()\n    assert inputs\n    for input in inputs:\n        assert await input.is_disabled()",
    "crumbs": [
      "API",
      "controller"
    ]
  },
  {
    "objectID": "08_api.html",
    "href": "08_api.html",
    "title": "neuralactivitycubic",
    "section": "",
    "text": "source\n\nrun_analysis\n\n run_analysis (config:neuralactivitycubic.datamodels.Config|str)\n\n*Run analysis.\nArgs: config (Config | str): Configuration for analysis or data source path to run analysis with default settings.*"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "analysis",
    "section": "",
    "text": "source\n\nBaselineEstimatorFactory\n\n BaselineEstimatorFactory ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPeak\n\n Peak (frame_idx:int, intensity:float, amplitude:Optional[float]=None,\n       delta_f_over_f:Optional[float]=None,\n       has_neighboring_intersections:Optional[bool]=None, frame_idxs_of_ne\n       ighboring_intersections:Optional[Tuple[int,int]]=None,\n       area_under_curve:Optional[float]=None,\n       peak_type:Optional[str]=None)\n\n\nsource\n\n\nAnalysisROI\n\n AnalysisROI (roi:neuralactivitycubic.input.ROI,\n              row_col_offset:Tuple[int,int], zstack:numpy.ndarray)\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API",
      "analysis"
    ]
  },
  {
    "objectID": "processing.html",
    "href": "processing.html",
    "title": "processing",
    "section": "",
    "text": "source\n\nprocess_analysis_rois\n\n process_analysis_rois\n                        (analysis_roi:neuralactivitycubic.analysis.Analysi\n                        sROI, configs:Dict[str,Any])\n\n\nsource\n\n\nAnalysisJob\n\n AnalysisJob (number_of_parallel_processes:int, data_loaders:Dict[str,Unio\n              n[neuralactivitycubic.input.DataLoader,List[neuralactivitycu\n              bic.input.DataLoader]]])\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API",
      "processing"
    ]
  },
  {
    "objectID": "view.html",
    "href": "view.html",
    "title": "view",
    "section": "",
    "text": "source\n\nchange_widget_state\n\n change_widget_state (widget:ipywidgets.widgets.widget.Widget,\n                      value:Optional[Any]=None,\n                      description:Optional[str]=None,\n                      disabled:Optional[bool]=None,\n                      visibility:Optional[str]=None,\n                      tooltip:Optional[str]=None,\n                      button_style:Optional[str]=None)\n\n\nsource\n\n\nUserInfoPanel\n\n UserInfoPanel ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSourceDataPanel\n\n SourceDataPanel (user_info_panel:__main__.UserInfoPanel)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nAnalysisSettingsPanel\n\n AnalysisSettingsPanel ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nMainScreen\n\n MainScreen ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nWidgetsInterface\n\n WidgetsInterface ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSourceDataStructureWidget\n\n SourceDataStructureWidget ()\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API",
      "view"
    ]
  }
]