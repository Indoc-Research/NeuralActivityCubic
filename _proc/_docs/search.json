[
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "This new version of NAÂ³ is currently available via the Python Package Index or via GitHub. To be able to install and run NAÂ³ on your system you currently need to run a few commands in the terminal, as the fully GUI-based installation will be available only soon. But donÂ´t worry, weÂ´ll take you through the process step by step!",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#install-anaconda",
    "href": "installation.html#install-anaconda",
    "title": "Installation",
    "section": "1. Install Anaconda",
    "text": "1. Install Anaconda\nThough not strictly required, we highly recommend installing NAÂ³ in a virtual Python environment that you setup via Anaconda - especially for users with limited (or even no) programming expertise. Please download the corresponding distribution for your operating system here and follow the respective installation wizard to install Anaconda on your system.",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#create-and-activate-a-new-virtual-environment",
    "href": "installation.html#create-and-activate-a-new-virtual-environment",
    "title": "Installation",
    "section": "2. Create and activate a new virtual environment",
    "text": "2. Create and activate a new virtual environment\nNext, weÂ´ll need a terminal to install NAÂ³ with just a few commands. For this, we suggest using the tool Anaconda Prompt that came with the installation of Anaconda (but if you prefer to use a different terminal, please feel free to use it - the only requirement is that conda is accessible, which comes per default in Anaconda Prompt). In the terminal (e.g.Â in Anaconda Prompt), please type the following command and execute it by hitting the Return key on your keyboard. It will create a new virtual environment that we can use to install NAÂ³, using conda:\n\nconda create --name na3 -y python=3.11\n\nOnce the setup of the environment has concluded, you should see a message like the following, informing you how to activate the environment:\n\nWeÂ´ll do exactly as suggested to activate the newly created virtual environment to continue the installation of NAÂ³. Thus, please run the following command in the Anaconda Prompt terminal:\n\nconda activate na3\n\nYou should now see that the input line in the Anaconda Prompt terminal starts with (na3) - indicating that you successfully switched to this virtual environment.",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#install-the-neuralactivitycubic-python-package",
    "href": "installation.html#install-the-neuralactivitycubic-python-package",
    "title": "Installation",
    "section": "3. Install the neuralactivitycubic Python package:",
    "text": "3. Install the neuralactivitycubic Python package:\nWith the activated na3 environment, please complete the installation of NAÂ³ by running on final command in the Anaconda Prompt terminal:\n\npip install neuralactivitycubic\n\nUpon executing this command, you will see a lot of output and activity in the terminal while it downloads the source code of NAÂ³ and itÂ´s dependencies, and installs them all in the virtual environment on your system. You know that the installation has finished, when the input line starting with (na3) appears again. The final few messages that are shown in Anaconda Prompt will then likely look similar to this:\n\n\n\nOutput after successful installation of neuralactivitycubic.",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#done",
    "href": "installation.html#done",
    "title": "Installation",
    "section": "4. Done!",
    "text": "4. Done!\nCongratulations! ThatÂ´s already everything you need to do to install NAÂ³ on your system. You can now use NAÂ³ to analyze your data and hopefully start a journey to the next scientific breakthrough!\nIf youÂ´d like to get some more information on how to use NAÂ³, please check out the following chapters in this documentation:\n\nUsing NAÂ³ via the GUI\nUsing NAÂ³ via the API",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "installation.html#wed-love-to-hear-your-feedback",
    "href": "installation.html#wed-love-to-hear-your-feedback",
    "title": "Installation",
    "section": "WeÂ´d love to hear your Feedback:",
    "text": "WeÂ´d love to hear your Feedback:\nYou are using NAÂ³ for your research - or would like to do so, but thereÂ´s that one cool feature missing for you? ThatÂ´s great! WeÂ´d love to hear your feedback, feature requests, or bug reports to keep improving NAÂ³ - please use this form on GitHub to submit it, we appreciate it a lot!\nPlease feel free to also explore all other Research Software that our not-for-profit organization has developed, or learn more about us and what we do on our website. If you are interested to partner with us for a similar collaboration on your Research Software, or to develop it with or for you from scratch, donÂ´t hesitate and drop us an email - weÂ´re always looking for new opportunities to catalyze science be delivering cutting-edge technology tailored to the specific research challenges of our collaborators!\n\n\n\nIndoc Research Europe gGmbH",
    "crumbs": [
      "User Guides",
      "Installation"
    ]
  },
  {
    "objectID": "07_datamodels.html",
    "href": "07_datamodels.html",
    "title": "neuralactivitycubic",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "07_datamodels.html#tests",
    "href": "07_datamodels.html#tests",
    "title": "neuralactivitycubic",
    "section": "Tests:",
    "text": "Tests:\nSetup for testing:\n\nfrom fastcore.test import test_fail\n\nfilepath = '../test_data/00/spiking_neuron.avi'\n\ncorrect_general_config = Config().to_dict()\ncorrect_general_config['data_source_path'] = filepath\n\nexample_general_config = dict(\n    batch_mode=False,\n    baseline_estimation_method=BaselineEstimationMethod.ASLS,\n    customize_octave_filtering=False,\n    data_source_path=filepath,\n    end_frame_idx=500,\n    export_to_nwb=False,\n    focus_area_enabled=False,\n    focus_area_filepath=None,\n    grid_size=10,\n    include_variance=False,\n    mean_signal_threshold=10.0,\n    min_octave_span=1.0,\n    min_peak_count=2,\n    noise_window_size=200,\n    recording_filepath=None,\n    results_filepath=None,\n    roi_filepath=None,\n    roi_mode=ROIMode.GRID,\n    save_overview_png=True,\n    save_single_trace_results=False,\n    save_summary_results=True,\n    signal_to_noise_ratio=3.0,\n    start_frame_idx=0,\n    use_frame_range=False,\n    variance_window_size=15,\n)\n\ncorrect_peak_config = {\n    'frame_idx': 10,\n    'intensity': 10.0,\n    'amplitude': 10.0,\n    'delta_f_over_f': 10.0,\n    'has_neighboring_intersections': True,\n    'frame_idxs_of_neighboring_intersections': (1,2),\n    'area_under_curve': 10.0,\n    'peak_type': 'normal',\n}\nminimal_peak_config = {\n    'frame_idx': 10,\n    'intensity': 10.0,\n}\n\ndef test_correct_config():\n    return Config.from_dict(**correct_general_config)\n\ndef test_minimal_config():\n    return Config.from_dict(data_source_path=filepath)\n\ndef test_config_with_specific_filepaths():\n    specific_config = correct_general_config.copy()\n    specific_config['data_source_path'] = None\n    specific_config['recording_filepath'] = filepath\n    specific_config['roi_filepath'] = filepath\n    specific_config['focus_area_filepath'] = filepath\n    return Config.from_dict(**specific_config)\n\ndef test_config_with_specific_filepaths_list():\n    specific_config = correct_general_config.copy()\n    specific_config['data_source_path'] = None\n    specific_config['recording_filepath'] = filepath\n    specific_config['roi_filepath'] = [filepath, filepath]\n    specific_config['focus_area_filepath'] = filepath\n    return Config.from_dict(**specific_config)\n\ndef test_config_to_json():\n    config = Config.from_dict(**example_general_config)\n    return config.to_json()\n\n# enumerators are special cases, so we need to check them separately\ndef test_config_enum_from_str():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['baseline_estimation_method'] = 'asls'  # valid value for baseline_estimation_method\n    return Config.from_dict(**incorrect_config)\n\ndef test_incorrect_config_enum_from_str():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['baseline_estimation_method'] = 'not_valid'  # invalid value for baseline_estimation_method\n    return Config.from_dict(**incorrect_config)\n\n# general invalid types checks\ndef test_incorrect_config_batch_mode():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['batch_mode'] = 'invalid_value'  # invalid type for batch_mode\n    return Config.from_dict(**incorrect_config)\n\ndef test_incorrect_config_end_frame_idx():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['end_frame_idx'] = 'invalid_value'  # invalid type for end_frame_idx\n    return Config.from_dict(**incorrect_config)\n\n# enumerators are special cases, so we need to check them separately\ndef test_incorrect_config_baseline_estimation_method():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['baseline_estimation_method'] = 1337  # invalid type for baseline_estimation_method\n    return Config.from_dict(**incorrect_config)\n\n# boolean fields are also special cases, so we need to check them separately\ndef test_incorrect_config_customize_octave_filtering():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['customize_octave_filtering'] = 'invalid_value'  # invalid type for customize_octave_filtering\n    return Config.from_dict(**incorrect_config)\n\n# roi_filepath is a special case, so we need to check it separately\ndef test_incorrect_config_roi_filepath():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['roi_filepath'] = (1, 2)  # invalid type for roi_filepath\n    return Config.from_dict(**incorrect_config)\n\n# other filepaths should not be set up with data_source_path\ndef test_incorrect_config_singular_path():\n    incorrect_config = correct_general_config.copy()\n    incorrect_config['recording_filepath'] = filepath\n    incorrect_config['roi_filepath'] = filepath\n    incorrect_config['focus_area_filepath'] = filepath\n    return Config.from_dict(**incorrect_config)\n\ndef test_correct_peak_config():\n    return Peak.from_dict(**correct_peak_config)\n\ndef test_minimal_peak_config():\n    return Peak.from_dict(**minimal_peak_config)\n\ndef test_incomplete_peak_config():\n    incomplete_peak_config = correct_peak_config.copy()\n    incomplete_peak_config.pop('frame_idx')\n    return Peak.from_dict(**incomplete_peak_config)\n\ndef test_wrong_peak_config():\n    wrong_peak_config = correct_peak_config.copy()\n    wrong_peak_config['frame_idx'] = 'invalid_value'\n    return Peak.from_dict(**wrong_peak_config)\n\nRun tests:\n\n# correct inputs tests\nassert isinstance(test_correct_config(), Config)\nassert isinstance(test_minimal_config(), Config)\nassert isinstance(test_config_enum_from_str(), Config)\nassert isinstance(test_config_with_specific_filepaths(), Config)\nassert isinstance(test_config_with_specific_filepaths_list(), Config)\nassert isinstance(test_correct_peak_config(), Peak)\nassert isinstance(test_minimal_peak_config(), Peak)\nassert test_config_to_json() == dumps(example_general_config)\n\n# incomplete inputs tests\ntest_fail(test_incomplete_peak_config)\n\n# wrong inputs tests\ntest_fail(test_incorrect_config_batch_mode)\ntest_fail(test_incorrect_config_baseline_estimation_method)\ntest_fail(test_incorrect_config_customize_octave_filtering)\ntest_fail(test_incorrect_config_end_frame_idx)\ntest_fail(test_incorrect_config_roi_filepath)\ntest_fail(test_incorrect_config_singular_path)\ntest_fail(test_incorrect_config_enum_from_str)\ntest_fail(test_wrong_peak_config)"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "model",
    "section": "",
<<<<<<< HEAD
    "text": "New to literate programming?\n\n\n\n\n\nWelcome to the inner workings of neuralactivitycubic!\nThis message is here only for you, as weÂ´d like to share some basic information about this piece of research software with you to facilitate your interactions with it. WeÂ´re using a literate programming framework called nbdev for the development, testing, documentation, and dissemination of neuralactivitycubic. We believe the concept of rich annotations and usage examples directly intermixed with the source code (read more about the concept of literate programming for instance here) holds great value and potential, especially in the context of research software, as it makes it easier for others to understand, (re-)use, and ideally even to adapt or contribute to your code. That being said, some things in here might look a bit confusing to you - conversely especially if you are an experienced developer used to â€œregularâ€ source code.\nFor instance, youÂ´ll regularly find the implementation of a class, like the Model here, interrupted by markdown text and maybe even some addtional code cells that serve as usage examples or even represent the implementation of tests, only for the subsequent class methods to continue in code cells below, literally patched to the class using the @patch decorator. Just be aware that this may not meet your expectations of what conventional source code â€œshouldâ€ look like and maybe you can discover some valuable takeaway for you along the way, too!\nOne final note before you head on, though: we are ourselves still experimenting a lot with this concept and how it can be used to best effect and are more than happy to engage in discussion or to hear your feedback on this topic. Feel free to drop us a message via GitHub!\nEverybody needs some help - and weÂ´re standing on the shoulders of some giants here. LetÂ´s start by importing all dependencies we need:\nExported source\n# Actual functional dependencies:\n# external:\nfrom pathlib import Path\nfrom datetime import datetime, timezone\nfrom matplotlib.pyplot import show\nimport ipywidgets as w\nimport multiprocessing\nfrom fastcore.basics import patch\nimport gc\n\n# and internal:\nfrom neuralactivitycubic.datamodels import Config\nfrom neuralactivitycubic.processing import AnalysisJob\nfrom neuralactivitycubic.input import RecordingLoaderFactory, ROILoaderFactory, RecordingLoader, ROILoader, get_filepaths_with_supported_extension_in_dirpath, FocusAreaPathRestrictions\n\n# Finally, some dependencies regarding type hints:\nfrom typing import Callable\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes._axes import Axes\nNow letÂ´s get started:\nsource",
=======
    "text": "If youâ€™re here, chances are youâ€™re interested in understanding or extending the Model class that sits at the heart of neuralactivitycubic. Whether youâ€™re a researcher, a contributor, or just curious, this notebook is for you!\nBefore we dive into the code, letâ€™s take a moment to talk about how this piece fits into the big picture.\n\n\nThink of the Model as the brain of neuralactivitycubic. Itâ€™s the hub that:\n\nManages and validates configuration settings\nCreates analysis jobs based on user input or directory structures\nExecutes the core analysis pipeline\nCommunicates with the graphical user interface (GUI), if used\nSaves logs, plots, and results for each analysis\n\nIt can run standalone or be accessed via the GUI. It handles batch jobs, logs everything, and orchestrates the creation of structured outputs, including NWB files. Crucially, though, as the brain of neuralactivitycubic, it essentially sends the commands to the other, downstream modules, which in turn implement the logic that will be executed. Like, for instance, the Model leverages the RecordingLoader that is implemented in the input module for loading of your recording data when creating the analysis jobs, and so on. Thus, the Model also represents the highest level of abstraction in neuralactivitycubic and enables you to run a wholistic analysis with only a few commands.\nHereâ€™s what a minimal programmatic workflow might look like:\n\nfrom neuralactivitycubic.model import Model\n\nmodel = Model('path/to/my/recording.avi')\nmodel.create_analysis_jobs()\nmodel.run_analysis()\n\nBoom, thatÂ´s it! Your data is analyzed, and results are saved. We made this even more accessible for you, by the way, in the api module. If youÂ´re planning to use neuralactivitycubic programmatically, we highly recommend you checkout it out.\n\n\n\nAlso, before we start with the actual source code of this module, we also added some quick notes for those of you that are new to the concept of Literate Programming, which we are using throughout neuralactivitycubic. Why? Our goal is to make the logic transparent and easy to understand - because research software deserves to be as readable as the papers that use it! Therefore, all notebooks of neuralactivitycubic interleave documentation, explanations, usage examples, and tests with the actual source code. We hope you like it!\n\n\n\n\n\n\nNew to literate programming?\n\n\n\n\n\nWeÂ´re using a literate programming framework called nbdev for the development, testing, documentation, and dissemination of neuralactivitycubic. We believe the concept of rich annotations and usage examples directly intermixed with the source code (read more about the concept of literate programming for instance here) holds great value and potential, especially in the context of research software, as it makes it easier for others to understand, (re-)use, and ideally even to adapt or contribute to the code. That being said, some things in here might look a bit confusing to you - conversely especially if you are an experienced developer used to â€œregularâ€ source code.\nFor instance, youÂ´ll regularly find the implementation of a class, like the Model here, interrupted by markdown text and maybe even some addtional code cells that serve as usage examples or even represent the implementation of tests, only for the subsequent class methods to continue in code cells below, literally patched to the class using the @patch decorator. Just be aware that this may not meet your expectations of what conventional source code typically looks like and maybe you can discover some valuable takeaway for you along the way, too!\nOne final note before you head on, though: we are ourselves still experimenting a lot with this concept and how it can be used to best effect and are more than happy to engage in discussion or to hear your feedback on this topic. Feel free to drop us a message via GitHub!\n\n\n\n\n\n\nEverybody needs some help - and weÂ´re standing on the shoulders of some giants here. LetÂ´s start by importing all dependencies we need to make neuralactivitycubic work:\n\n\nExported source\n# Actual functional dependencies:\n# external:\nfrom pathlib import Path\nfrom datetime import datetime, timezone\nfrom matplotlib.pyplot import show\nimport ipywidgets as w\nimport multiprocessing\nfrom fastcore.basics import patch\n\n# and internal:\nfrom neuralactivitycubic.datamodels import Config\nfrom neuralactivitycubic.processing import AnalysisJob\nfrom neuralactivitycubic.input import RecordingLoaderFactory, ROILoaderFactory, RecordingLoader, ROILoader, get_filepaths_with_supported_extension_in_dirpath, FocusAreaPathRestrictions\n\n# Finally, some dependencies regarding type hints:\nfrom typing import Callable\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes._axes import Axes",
>>>>>>> fb1f145 (literate docs for model completed)
    "crumbs": [
      "API",
      "model"
    ]
  },
  {
    "objectID": "model.html#welcome-to-the-model-module-of-neuralactivitycubic",
    "href": "model.html#welcome-to-the-model-module-of-neuralactivitycubic",
    "title": "model",
<<<<<<< HEAD
    "section": "Testing",
    "text": "Testing\n\nSetup\n\nfrom shutil import rmtree\nimport os\nfrom re import compile\n\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\n\nfrom neuralactivitycubic.view import WidgetsInterface\n\n\ntest_filepath = Path('../test_data/00')\n\ntest00_filepath = Path('../test_data/00')\ntest01_filepath = Path('../test_data/01')\nparent_test_filepath = Path('../test_data')\nexample_results_dir = Path('../test_data/00/example_test_results_for_spiking_neuron')\nresults_filepath = Path('../test_data/00/results_directory')\n# we have to split results by own directories due to concurrency issues\nresults_case01_filepath = Path('../test_data/results/case_01/')\nresults_case02_filepath = Path('../test_data/results/case_02/')\nresults_case03_filepath = Path('../test_data/results/case_03/')\nresults_case04_filepath = Path('../test_data/results/case_04/')\nresults_case05_filepath = Path('../test_data/results/case_05/')\nresults_case06_filepath = Path('../test_data/results/case_06/')\n\ndef test_correct_model_run():\n    correct_config = WidgetsInterface().export_user_settings()\n    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n    correct_config.save_single_trace_results = True\n    model = Model(correct_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n    return model.result_directories\n\ndef test_correct_model_run_with_custom_results_dir():\n    correct_config = WidgetsInterface().export_user_settings()\n    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n    correct_config.results_filepath = results_filepath\n    model = Model(correct_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n    return correct_config.results_filepath\n\n# tests with different modes of running analysis\n\ndef test_run_grid_focus_mode():\n    \"\"\"\n    Test run_analysis function with focus area enabled.\n    \"\"\"\n    grid_focus_mode_config = WidgetsInterface().export_user_settings()\n    grid_focus_mode_config.data_source_path = test01_filepath\n    grid_focus_mode_config.focus_area_enabled = True\n    grid_focus_mode_config.results_filepath = results_case01_filepath\n    model = Model(grid_focus_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return grid_focus_mode_config.results_filepath\n\ndef test_run_file_focus_mode():\n    \"\"\"\n    Test run_analysis function with focus area enabled.\n    \"\"\"\n    file_focus_mode_config = WidgetsInterface().export_user_settings()\n    file_focus_mode_config.data_source_path = test01_filepath\n    file_focus_mode_config.focus_area_enabled = True\n    file_focus_mode_config.roi_mode = 'file'\n    file_focus_mode_config.results_filepath = results_case01_filepath\n    model = Model(file_focus_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return file_focus_mode_config.results_filepath\n\ndef test_run_grid_batch_mode():\n    \"\"\"\n    Test run_analysis function with batch mode enabled.\n    \"\"\"\n    grid_batch_mode_config = WidgetsInterface().export_user_settings()\n    grid_batch_mode_config.data_source_path = parent_test_filepath\n    grid_batch_mode_config.batch_mode = True\n    grid_batch_mode_config.results_filepath = results_case02_filepath\n    model = Model(grid_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return grid_batch_mode_config.results_filepath\n\ndef test_run_file_batch_mode():\n    \"\"\"\n    Test run_analysis function with batch mode enabled.\n    \"\"\"\n    file_batch_mode_config = WidgetsInterface().export_user_settings()\n    file_batch_mode_config.data_source_path = parent_test_filepath\n    file_batch_mode_config.batch_mode = True\n    file_batch_mode_config.roi_mode = 'file'\n    file_batch_mode_config.results_filepath = results_case02_filepath\n    model = Model(file_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return file_batch_mode_config.results_filepath\n\ndef test_run_grid_focus_batch_mode():\n    \"\"\"\n    Test run_analysis function with focus and batch mode enabled.\n    \"\"\"\n    grid_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n    grid_focus_batch_mode_config.data_source_path = parent_test_filepath\n    grid_focus_batch_mode_config.batch_mode = True\n    grid_focus_batch_mode_config.focus_area_enabled = True\n    grid_focus_batch_mode_config.results_filepath = results_case03_filepath\n    model = Model(grid_focus_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return grid_focus_batch_mode_config.results_filepath\n\ndef test_run_file_focus_batch_mode():\n    \"\"\"\n    Test run_analysis function with focus and batch mode enabled.\n    \"\"\"\n    file_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n    file_focus_batch_mode_config.data_source_path = parent_test_filepath\n    file_focus_batch_mode_config.batch_mode = True\n    file_focus_batch_mode_config.focus_area_enabled = True\n    file_focus_batch_mode_config.roi_mode = 'file'\n    file_focus_batch_mode_config.results_filepath = results_case03_filepath\n    model = Model(file_focus_batch_mode_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n\n    return file_focus_batch_mode_config.results_filepath\n\n\ndef _test_csv_files(relative_filepath_to_csv: str, results_dir: Path) -&gt; bool:\n    filepath = results_dir / relative_filepath_to_csv\n    # confirm results have been created:\n    if not filepath.is_file():\n        return False\n    # confirm computational consistency of results, while allowing minor numerical tolerance\n    df_test = pd.read_csv(filepath)\n    df_validation = pd.read_csv(example_results_dir / relative_filepath_to_csv)\n    if assert_frame_equal(df_test, df_validation) is not None:\n        return False\n    else:\n        return True\n\ndef test_all_peak_results(results_dir):\n    return _test_csv_files('all_peak_results.csv', results_dir)\n\ndef test_amplitude_and_df_over_f_results(results_dir):\n    return _test_csv_files('Amplitude_and_dF_over_F_results.csv', results_dir)\n\ndef test_auc_results(results_dir):\n    return _test_csv_files('AUC_results.csv', results_dir)\n\ndef test_variance_area_results(results_dir):\n    return _test_csv_files('Variance_area_results.csv', results_dir)\n\ndef test_representative_single_trace_results(results_dir):\n    return _test_csv_files('single_traces/data_of_ROI_7-10.csv', results_dir)\n\ndef test_activity_overview_png(results_dir):\n    filepath = results_dir / 'activity_overview.png'\n    return filepath.is_file()\n\ndef test_roi_label_ids_overview_png(results_dir):\n    filepath = results_dir / 'ROI_label_IDs_overview.png'\n    return filepath.is_file()\n\ndef test_individual_traces_with_identified_events_pdf(results_dir):\n    filepath = results_dir / 'Individual_traces_with_identified_events.pdf'\n    return filepath.is_file()\n\ndef test_logs_txt(results_dir):\n    filepath = results_dir / 'logs.txt'\n    return filepath.is_file()\n\ndef test_user_settings_json(results_dir):\n    filepath = results_dir / 'user_settings.json'\n    return filepath.is_file()\n\ndef test_nwb_export(results_dir):\n    filepath = results_dir / 'autogenerated_nwb_file.nwb'\n\ndef test_all_correct_files_created(results_dir: Path, test_csv: bool = True, single_trace: bool = False) -&gt; bool:\n    \"\"\"\n    Check if all expected files have been created in the results directory.\n    \"\"\"\n    # confirm all csv files have been created and are correct:\n    if test_csv:\n        assert test_all_peak_results(results_dir), 'There is an issue with the \"all_peak_results.csv\" file!'\n        assert test_amplitude_and_df_over_f_results(results_dir)\n        assert test_auc_results(results_dir)\n        assert test_variance_area_results(results_dir)\n        if single_trace:\n            assert test_representative_single_trace_results(results_dir)\n\n    # confirm all other result files have been created:\n    assert test_activity_overview_png(results_dir)\n    assert test_roi_label_ids_overview_png(results_dir)\n    assert test_logs_txt(results_dir)\n    assert test_user_settings_json(results_dir)\n    if single_trace:\n        assert test_individual_traces_with_identified_events_pdf(results_dir)\n\n    return True\n\n\ndef find_directories_after_test(base_path):\n    \"\"\"\n    Find directories after test with timestamps.\n    \"\"\"\n    pattern = compile(r'\\d{4}_\\d{2}_\\d{2}_\\d{2}-\\d{2}-\\d{2}_.+')\n\n    matching_dirs = [\n        str(base_path) + d for d in os.listdir(base_path)\n        if os.path.isdir(os.path.join(base_path, d)) and pattern.fullmatch(d)\n    ]\n\n    return matching_dirs\n\ndef delete_directories_after_test(paths_list):\n    \"\"\"\n    Delete directories after test.\n    \"\"\"\n    for res_dir in find_directories_after_test(paths_list):\n        rmtree(res_dir)\n\nRun tests:\n\n# confirm that model can be executed:\nresult_directories = test_correct_model_run()\n\nfor directory in result_directories:\n    assert directory.exists()\n    assert test_all_correct_files_created(directory)\n    # cleanup\n    rmtree(directory)\n\n\n# confirm that model can be executed with custom result directory:\nresults_directory = test_correct_model_run_with_custom_results_dir()\n\nassert results_directory.exists()\n# only one directory with analysis files should be created, as there are only one focus area:\nassert len(list(results_directory.iterdir())) == 1\n\nfor directory in results_directory.iterdir():\n    assert test_all_correct_files_created(directory)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with focus mode:\nresults_directory = test_run_grid_focus_mode()\n\nassert results_directory.exists()\n\n# only one directory should be created, as there are only one focus area:\nassert len(list(results_directory.iterdir())) == 2\nfor directory in results_directory.iterdir():\n    assert test_all_correct_files_created(directory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch mode:\nresults_directory = test_run_grid_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch and focus mode:\nresults_directory = test_run_grid_focus_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    # test case with multiple focus areas\n    if '01' in directory.name:\n        assert len(list(directory.iterdir())) == 2\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with focus mode:\nresults_directory = test_run_file_focus_mode()\n\nassert results_directory.exists()\n\n# only one directory should be created, as there are only one focus area:\nassert len(list(results_directory.iterdir())) == 2\nfor directory in results_directory.iterdir():\n    assert test_all_correct_files_created(directory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch mode:\nresults_directory = test_run_file_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with batch and focus mode:\nresults_directory = test_run_file_focus_batch_mode()\n\nassert results_directory.exists()\n\nassert len(list(results_directory.iterdir())) == 3\nfor directory in results_directory.iterdir():\n    # test case with multiple focus areas\n    if '01' in directory.name:\n        assert len(list(directory.iterdir())) == 2\n    for subdirectory in directory.iterdir():\n        assert test_all_correct_files_created(subdirectory, test_csv=False)\n\n# cleanup\nrmtree(results_directory)",
=======
    "section": "",
    "text": "If youâ€™re here, chances are youâ€™re interested in understanding or extending the Model class that sits at the heart of neuralactivitycubic. Whether youâ€™re a researcher, a contributor, or just curious, this notebook is for you!\nBefore we dive into the code, letâ€™s take a moment to talk about how this piece fits into the big picture.\n\n\nThink of the Model as the brain of neuralactivitycubic. Itâ€™s the hub that:\n\nManages and validates configuration settings\nCreates analysis jobs based on user input or directory structures\nExecutes the core analysis pipeline\nCommunicates with the graphical user interface (GUI), if used\nSaves logs, plots, and results for each analysis\n\nIt can run standalone or be accessed via the GUI. It handles batch jobs, logs everything, and orchestrates the creation of structured outputs, including NWB files. Crucially, though, as the brain of neuralactivitycubic, it essentially sends the commands to the other, downstream modules, which in turn implement the logic that will be executed. Like, for instance, the Model leverages the RecordingLoader that is implemented in the input module for loading of your recording data when creating the analysis jobs, and so on. Thus, the Model also represents the highest level of abstraction in neuralactivitycubic and enables you to run a wholistic analysis with only a few commands.\nHereâ€™s what a minimal programmatic workflow might look like:\n\nfrom neuralactivitycubic.model import Model\n\nmodel = Model('path/to/my/recording.avi')\nmodel.create_analysis_jobs()\nmodel.run_analysis()\n\nBoom, thatÂ´s it! Your data is analyzed, and results are saved. We made this even more accessible for you, by the way, in the api module. If youÂ´re planning to use neuralactivitycubic programmatically, we highly recommend you checkout it out.\n\n\n\nAlso, before we start with the actual source code of this module, we also added some quick notes for those of you that are new to the concept of Literate Programming, which we are using throughout neuralactivitycubic. Why? Our goal is to make the logic transparent and easy to understand - because research software deserves to be as readable as the papers that use it! Therefore, all notebooks of neuralactivitycubic interleave documentation, explanations, usage examples, and tests with the actual source code. We hope you like it!\n\n\n\n\n\n\nNew to literate programming?\n\n\n\n\n\nWeÂ´re using a literate programming framework called nbdev for the development, testing, documentation, and dissemination of neuralactivitycubic. We believe the concept of rich annotations and usage examples directly intermixed with the source code (read more about the concept of literate programming for instance here) holds great value and potential, especially in the context of research software, as it makes it easier for others to understand, (re-)use, and ideally even to adapt or contribute to the code. That being said, some things in here might look a bit confusing to you - conversely especially if you are an experienced developer used to â€œregularâ€ source code.\nFor instance, youÂ´ll regularly find the implementation of a class, like the Model here, interrupted by markdown text and maybe even some addtional code cells that serve as usage examples or even represent the implementation of tests, only for the subsequent class methods to continue in code cells below, literally patched to the class using the @patch decorator. Just be aware that this may not meet your expectations of what conventional source code typically looks like and maybe you can discover some valuable takeaway for you along the way, too!\nOne final note before you head on, though: we are ourselves still experimenting a lot with this concept and how it can be used to best effect and are more than happy to engage in discussion or to hear your feedback on this topic. Feel free to drop us a message via GitHub!\n\n\n\n\n\n\nEverybody needs some help - and weÂ´re standing on the shoulders of some giants here. LetÂ´s start by importing all dependencies we need to make neuralactivitycubic work:\n\n\nExported source\n# Actual functional dependencies:\n# external:\nfrom pathlib import Path\nfrom datetime import datetime, timezone\nfrom matplotlib.pyplot import show\nimport ipywidgets as w\nimport multiprocessing\nfrom fastcore.basics import patch\n\n# and internal:\nfrom neuralactivitycubic.datamodels import Config\nfrom neuralactivitycubic.processing import AnalysisJob\nfrom neuralactivitycubic.input import RecordingLoaderFactory, ROILoaderFactory, RecordingLoader, ROILoader, get_filepaths_with_supported_extension_in_dirpath, FocusAreaPathRestrictions\n\n# Finally, some dependencies regarding type hints:\nfrom typing import Callable\nfrom matplotlib.figure import Figure\nfrom matplotlib.axes._axes import Axes",
    "crumbs": [
      "API",
      "model"
    ]
  },
  {
    "objectID": "model.html#start-of-the-source-code",
    "href": "model.html#start-of-the-source-code",
    "title": "model",
    "section": "ðŸ“˜ Start of the source code",
    "text": "ðŸ“˜ Start of the source code\nReady? Grab a coffee and enjoy the tour through the module! Letâ€™s jump into the first class, the Logger:\nBefore any jobs are created or analysis is run, we need a reliable way to record what happens and when - and thatÂ´s what the Logger does. It logs messages with precise UTC timestamps and can:\n\nStore messages in memory for programmatic access\nPrint logs to the console (or GUI)\nSave logs to disk in a simple text file\nClear logs when switching contexts or rerunning jobs\n\n\nsource\n\nLogger\n\n Logger ()\n\nA simple logging utility class that captures log messages with UTC timestamps, allows retrieval and clearing of logs, and supports saving them to a file.\n\n\nExported source\nclass Logger:\n\n    \"\"\"\n    A simple logging utility class that captures log messages with UTC timestamps,\n    allows retrieval and clearing of logs, and supports saving them to a file.\n    \"\"\"\n    \n    def __init__(self):\n        self.logs = []\n\n    def add_new_log(self, message: str) -&gt; None:\n        \"\"\"\n        Add a new log message with a UTC timestamp prefix. \n        The timestamp is formatted as 'dd-mm-yy HH:MM:SS.ffffff (UTC)'.\n        \"\"\"\n        time_prefix_in_utc = datetime.now(timezone.utc).strftime('%d-%m-%y %H:%M:%S.%f')\n        self.logs.append(f'{time_prefix_in_utc} (UTC): {message}')\n        print(f'{time_prefix_in_utc} (UTC): {message}')\n\n    def get_logs(self) -&gt; list[str]:\n        return self.logs\n\n    def clear_logs(self) -&gt; None:\n        self.logs = []\n\n    def save_current_logs(self, save_dir: Path) -&gt; None:\n        filepath = save_dir.joinpath('logs.txt')\n        with open(filepath , 'w+') as logs_file:\n            for log_message in self.logs:\n                logs_file.write(f'{log_message}\\n')\n        print(f'Logs saved to {filepath}')\n\n\nIts usage is simple, but invaluable for debugging and traceability. HereÂ´s a short example of how to interact with the Logger:\n\n# LetÂ´s create a logger:\nlogger = Logger()\n\n# Add some random messages to log:\nlogger.add_new_log('Pretending to start the analysis ...')\nlogger.add_new_log('.. ba bi bu bup ..')\nlogger.add_new_log('.. I guess you got the hang of it now?')\n\n13-07-25 00:46:11.142592 (UTC): Pretending to start the analysis ...\n13-07-25 00:46:11.142743 (UTC): .. ba bi bu bup ..\n13-07-25 00:46:11.142808 (UTC): .. I guess you got the hang of it now?\n\n\n\n# Access what has been logged so far:\nlogger.get_logs()\n\n['13-07-25 00:46:11.142592 (UTC): Pretending to start the analysis ...',\n '13-07-25 00:46:11.142743 (UTC): .. ba bi bu bup ..',\n '13-07-25 00:46:11.142808 (UTC): .. I guess you got the hang of it now?']\n\n\nAnd if youÂ´d like to save the logs, you can do so pretty easily as well - just be aware that it expects a pathlib.Path object that defines the directory in which the logs.txt file should be saved! Once all logs are saved, you might want to clean it up before starting your next analysis:\n\n# Create a Path:\ndestination_directory = Path('path/to/my/directory/of/choice')\n\n# Pass the Path to the Logger to save the logs.txt\nlogger.save_current_logs(destination_directory)\n\n\n# Clean up all previously logged messages and your off to a fresh start:\nlogger.clear_logs()\n\nYou have some trust issues? You can actually check that the list of logged messages is actually empty by calling the get_logs() method again:\n\n# Confirm the logs were actually cleaned:\nlogger.get_logs()\n\n[]\n\n\nAwesome â€” thatâ€™s exactly the behavior we expected! ðŸŽ¯\nSure, this might be a simple example, but itâ€™s a perfect introduction to the idea of testing your code. If you know what your code should do â€” and maybe just as importantly, what it shouldnâ€™t do â€” you can write a test to check for that.\nEven better: those tests are just regular Python code. That means they can be executed automatically, for example, every time you make changes â€” helping you catch bugs early and ensure nothing breaks by accident.\nAnd since weâ€™re using the literate programming framework nbdev, we can write these tests right here, directly next to the source code they validate. Thatâ€™s not just tidy â€” itâ€™s powerful.\nSo letâ€™s put that into practice and write a test for the Logger class that automates the check we just did manually: confirming that all log messages are properly cleared when calling clear_logs().\n\n\n\n\n\n\nHow does testing in nbdev work?\n\n\n\n\n\nThe literate programming framework we are using here, called nbdev, essentially uses any assert statements you define in the notebook for testing (see for instance here in their official documentation on how they envision literate test implementation.\nTo add some more background, an assert statement only passes if itÂ´s True, otherwise it raises an AssertionError. For instance:\n\nassert 1 == 1\n\nwould pass, while:\n\nassert 1 == 2\n\nwould raise an AssertionError and nbdevÂ´s automated testing pipelines would catch and raise that error, notifying you that something is wrong with your code.\nFor this reason, if weÂ´re defining custom functions for the testing, we always need to make sure they return True when the check passes, such that the corresponding assert that calls the function also passes.\n\n\n\n\ndef test_log_clean_up():\n    logger = Logger()\n    logger.add_new_log('test')\n    logger.clear_logs()\n    logs_count = len(logger.get_logs())\n    return logs_count == 0\n\n\nassert test_log_clean_up()\n\nAnd just like that â€” weâ€™ve got automated tests running! ðŸŽ‰\nEvery time we push a new version of the code to GitHub, this test (along with all the others weâ€™ve written) runs automatically. If something breaks or behaves unexpectedly, we find out right away and can fix it before it becomes a problem.\n\n\n\n\n\n\nImportant\n\n\n\nThink about how powerful that is: we donâ€™t have to wait for a sharp-eyed user to notice somethingâ€™s off, track us down, and explain the issue. We catch bugs early â€” long before they can silently affect someoneâ€™s work.\nIn the context of research software, thatâ€™s not just convenient â€” itâ€™s critical. A bug that slips through unnoticed could end up influencing someoneâ€™s results, and possibly even get baked into a publication. Thatâ€™s a scenario none of us want.\nTesting is always important â€” but in science and research, the stakes are higher. Thatâ€™s why we take it seriously.\n\n\nNext up is the Model, which sits right at the core of neuralactivitycubic:\n\nsource\n\n\nModel\n\n Model (config:neuralactivitycubic.datamodels.Config|str)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nExported source\nclass Model:\n\n    def __init__(self, \n                 config: Config | str\n                ) -&gt; None:\n        self.num_processes = multiprocessing.cpu_count()\n        self.analysis_job_queue = []\n        self.logs = Logger()\n        if isinstance(config, str):\n            config = Config(data_source_path=config)\n        self.config = config\n        self.gui_enabled = False\n        self.callback_view_update_infos = None\n        self.callback_view_show_output_screen = None\n        self.view_output = None\n        self.pixel_conversion = None\n\n\nThis constructor sets up everything needed to use na3 in either programmatic, batch, or GUI mode.\n\nThe config parameter can either be a Config object or just a path string to the data source. If itâ€™s a string, the constructor wraps it in a Config.\nA Logger is created to track whatâ€™s happening and report status.\nGUI-related hooks are initialized but left inactive until explicitly connected.\n\nThis object can now create and run jobsâ€”but wait, whatâ€™s a job in this context? Thatâ€™s what weâ€™ll get into next ðŸ‘‡\nOnce the Model is initialized, the first thing you do is call create_analysis_jobs(). This method determines what data should be analyzed and sets up jobs accordingly.\nIt supports both single-file and batch-directory analysis and uses a set of private helpers to figure out how to match recordings, ROI masks, and focus area files.\nLetâ€™s take a look:\n\nsource\n\n\nModel.create_analysis_jobs\n\n Model.create_analysis_jobs ()\n\n\n\nExported source\n@patch\ndef create_analysis_jobs(self: Model\n                        ) -&gt; None:\n    self._ensure_data_from_previous_jobs_was_removed()\n    self.add_info_to_logs('Basic configurations for data import validated. Starting creation of analysis job(s)...', True)\n    if self.config.batch_mode:\n        all_subdir_paths_with_rec_file = self._get_all_subdir_paths_with_rec_file(self.config.data_source_path)\n        all_subdir_paths_with_rec_file.sort()\n        for idx, subdir_path in enumerate(all_subdir_paths_with_rec_file):\n            self._create_analysis_jobs_for_single_rec(subdir_path)\n    else:\n        self._create_analysis_jobs_for_single_rec()\n    self.add_info_to_logs('All job creation(s) completed.', True, 100.0)\n\n\nThis method acts like a scout: it ventures into the provided folder(s), searches for files that NAÂ³ knows how to handle, and sets up one or more analysis jobs based on what it finds.\nWhether it creates one job or many depends on the configuration youâ€™ve selected. The only setup that guarantees a single job is the following:\nbatch mode: OFF\nfocus area: OFF\nROI mode: Grid\nFor any other combination, the number of analysis jobs depends entirely on how your source data is structuredâ€”e.g.Â how many recordings are detected, how many ROI masks are found, or how many focus area regions exist.\nThe actual logic for locating those files and assembling jobs doesnâ€™t live directly in create_analysis_jobs() â€” instead, itâ€™s delegated to a series of helper methods. These methods do the real legwork and are marked as private by convention, meaning theyâ€™re not intended to be called from outside the Model class.\nFor your convenience, weâ€™ve bundled all of them together in the next code block so you can collapse them as a group, skim through them if you like, and then jump back into the higher-level functionality.\n\n\nExported source\n@patch\ndef _ensure_data_from_previous_jobs_was_removed(self: Model\n                                               ) -&gt; None:\n    # if we are deleting all data anyway, why bother checking?\n    if len(self.analysis_job_queue) &gt; 0:\n        self.add_info_to_logs('Loading of new source data. All previously created jobs & logs will be deleted.', True)\n        self.analysis_job_queue = []\n        self.logs.clear_logs()\n        # self._check_if_gui_setup_is_completed()\n\n\n@patch\n#@staticmethod\ndef _get_all_subdir_paths_with_rec_file(self: Model, \n                                        top_level_dir_path: Path\n                                       ) -&gt; list[Path]:\n    rec_loader_factory = RecordingLoaderFactory()\n    supported_extensions_for_recordings = rec_loader_factory.all_supported_extensions\n    all_subdir_paths_that_contain_a_supported_recording_file = []\n    for elem in top_level_dir_path.iterdir():\n        if not elem.name.startswith('.'):\n            if elem.is_dir():\n                supported_recording_filepaths = [elem_2 for elem_2 in elem.iterdir() if elem_2.suffix in supported_extensions_for_recordings]\n                if len(supported_recording_filepaths) &gt; 0:\n                    all_subdir_paths_that_contain_a_supported_recording_file.append(elem)\n    return all_subdir_paths_that_contain_a_supported_recording_file\n\n\n\n@patch\ndef _create_analysis_jobs_for_single_rec(self: Model, \n                                         recording_path: Path = None\n                                        ) -&gt; None:\n    if not recording_path:\n        if self.config.recording_filepath:\n            recording_path = self.config.recording_filepath\n        else:\n            recording_path = self.config.data_source_path\n    self.add_info_to_logs(f'Starting with Job creation(s) for {str(recording_path)}', True)\n    recording_loader = self._get_recording_loader(recording_path)\n\n\n    if self.config.roi_filepath:\n        roi_filepath = self.config.roi_filepath\n    else:\n        roi_filepath = self.config.data_source_path\n    if self.config.roi_mode == 'file':\n        roi_loaders = self._get_all_roi_loaders(roi_filepath)\n    else:\n        roi_loaders = None\n\n    if self.config.focus_area_filepath:\n        focus_area_filepath = self.config.focus_area_filepath\n    else:\n        focus_area_filepath = recording_path\n    if self.config.focus_area_enabled:\n        focus_area_dir_path = self._get_focus_area_dir_path(focus_area_filepath)\n        if focus_area_dir_path is None:\n            analysis_job = self._create_single_analysis_job(recording_loader, roi_loaders)\n            self.analysis_job_queue.append(analysis_job)\n            self.add_info_to_logs(f'Successfully created a single job for {focus_area_filepath} at queue position: #{len(self.analysis_job_queue)}.', True)\n        else:\n            all_focus_area_loaders = self._get_all_roi_loaders(focus_area_dir_path)\n            assert len(all_focus_area_loaders) &gt; 0, f'Focus Area analysis enabled, but no focus area ROIs could be found. Please revisit your source data and retry!'\n            for idx, focus_area_loader in enumerate(all_focus_area_loaders):\n                analysis_job_with_focus_area = self._create_single_analysis_job(recording_loader, roi_loaders, focus_area_loader)\n                self.analysis_job_queue.append(analysis_job_with_focus_area)\n                job_creation_message = (f'Successfully created {idx + 1} out of {len(all_focus_area_loaders)} job(s) for {recording_path} '\n                                        f'at queue position: #{len(self.analysis_job_queue)}.')\n                self.add_info_to_logs(job_creation_message, True)\n    else:\n        analysis_job = self._create_single_analysis_job(recording_loader, roi_loaders)\n        self.analysis_job_queue.append(analysis_job)\n        self.add_info_to_logs(f'Successfully created a single job for {recording_path} at queue position: #{len(self.analysis_job_queue)}.', True)\n    self.add_info_to_logs(f'Finished Job creation(s) for {recording_path}!', True)\n\n\n@patch\ndef _get_recording_loader(self: Model, \n                          source_path: Path\n                         ) -&gt; RecordingLoader:\n    rec_loader_factory = RecordingLoaderFactory()\n    if source_path.is_dir():\n        self.add_info_to_logs(f'Looking for a valid recording file in {source_path}...', True)\n        valid_filepaths = get_filepaths_with_supported_extension_in_dirpath(source_path, rec_loader_factory.all_supported_extensions, 1)\n        if len(valid_filepaths) == 0:\n            self.add_info_to_logs(f'Could not find any recording files of supported type at {source_path}!', True)\n        elif len(valid_filepaths) &gt;  1:\n            filepath = valid_filepaths[0]\n            too_many_files_message = (f'Found more than a single recording file of supported type at {source_path}, i.e.: {valid_filepaths}. '\n                                      f'However, only a single file was expected. NA3 continues with {filepath} and will ignore the other files.')\n            self.add_info_to_logs(too_many_files_message, True)\n        else:\n            filepath = valid_filepaths[0]\n            self.add_info_to_logs(f'Found recording file of supported type at: {filepath}.', True)\n    else:\n        filepath = source_path\n        self.add_info_to_logs(f'Found recording file of supported type at: {filepath}.', True)\n    recording_loader = rec_loader_factory.get_loader(filepath)\n    return recording_loader\n\n\n@patch\ndef _get_all_roi_loaders(self: Model, \n                         data_source_path: Path\n                        ) -&gt; list[ROILoader]:\n    assert data_source_path.is_dir(), f'You must provide a directory as source data when using ROI mode or enabling Focus Areas. Please revisit your input data and retry.'\n    roi_loader_factory = ROILoaderFactory()\n    all_filepaths_with_supported_filetype_extensions = get_filepaths_with_supported_extension_in_dirpath(data_source_path, roi_loader_factory.all_supported_extensions)\n    all_roi_loaders = [roi_loader_factory.get_loader(roi_filepath) for roi_filepath in all_filepaths_with_supported_filetype_extensions]\n    return all_roi_loaders\n\n\n@patch\ndef _get_focus_area_dir_path(self: Model, \n                             source_path: Path\n                            ) -&gt; Path:\n    focus_area_path_restrictions = FocusAreaPathRestrictions()\n    supported_dir_names = focus_area_path_restrictions.supported_dir_names\n    if source_path.is_dir():\n        source_dir_path = source_path\n    else:\n        source_dir_path = source_path.parent\n    dirs_with_valid_name = [elem for elem in source_dir_path.iterdir() if (elem.name in supported_dir_names) & (elem.is_dir() == True)]\n    if len(dirs_with_valid_name) == 0:\n        no_dir_found_message = (f'You enabled Focus Area but a correspondingly named directory could not be found in {source_dir_path}. '\n                                f'Please use one of the following for the name of the directory that contains the Focus Area ROIs: {supported_dir_names}. '\n                                'In absence of such a directory, analysis is continued without using the Focus Area mode for this data.')\n        self.add_info_to_logs(no_dir_found_message, True)\n        focus_area_dir_path = None\n    elif len(dirs_with_valid_name) &gt; 1:\n        too_many_dirs = (f'More than a single Focus Area directory was found in the following parent directory: {source_dir_path}, i.e.: '\n                         f'{dirs_with_valid_name}. However, only the use of a single one that contains all your Focus Area ROIS is '\n                         f'currently supported. {dirs_with_valid_name[0]} will be used for this analysis, while the other(s): {dirs_with_valid_name[1:]} '\n                         'will be ignored to continue processing.')\n        self.add_info_to_logs(too_many_dirs, True)\n        focus_area_dir_path = dirs_with_valid_name[0]\n    else:\n        focus_area_dir_path = dirs_with_valid_name[0]\n    return focus_area_dir_path\n\n\n@patch\ndef _create_single_analysis_job(self: Model, \n                                recording_loader: RecordingLoader, \n                                roi_loaders: list[ROILoader] | None, \n                                focus_area_loader: ROILoader | None = None\n                               ) -&gt; AnalysisJob:\n    data_loaders = {'recording': recording_loader}\n    if roi_loaders is not None:\n        data_loaders['rois'] = roi_loaders\n    if focus_area_loader is not None:\n        data_loaders['focus_area'] = focus_area_loader\n    return AnalysisJob(self.num_processes, data_loaders, self.config.results_filepath)\n\n\nOnce the analysis jobs have been created, itâ€™s time to actually process them! âœ¨\nTo kick off the entire analysis pipeline, you simply call the other main method of the Model:\n\nrun_analysis()\n\nThis method takes care of everything:\n\nexecuting each analysis job in turn,\nlogging the progress and any issues along the way,\ngenerating results and saving them to disk,\nand even updating the GUI if youâ€™re running in interactive mode.\n\n\nsource\n\n\nModel.run_analysis\n\n Model.run_analysis ()\n\n\n\nExported source\n@patch\ndef run_analysis(self: Model\n                ) -&gt; None:\n    self._display_configs()\n    self.add_info_to_logs('Starting analysis...', True)\n    for job_idx, analysis_job in enumerate(self.analysis_job_queue):\n        self.add_info_to_logs(f'Starting to process analysis job with index #{job_idx}.')\n        analysis_job.run_analysis(self.config)\n        self.add_info_to_logs(f'Analysis successfully completed. Continue with creation of results.. ')\n        analysis_job.create_results(self.config)\n        self.add_info_to_logs(f'Results successfully created at: {analysis_job.results_dir_path}')\n        if self.gui_enabled:\n            self.callback_view_show_output_screen()\n            with self.view_output:\n                activity_overview_fig = analysis_job.activity_overview_plot[0]\n                activity_overview_fig.set_figheight(400 * self.pixel_conversion)\n                activity_overview_fig.tight_layout()\n                show(activity_overview_fig)\n        self._save_user_settings_as_json(analysis_job)\n        self.logs.save_current_logs(analysis_job.results_dir_path)\n    self.add_info_to_logs('Updating all log files to contain all logs as final step. All valid logs files will end with this message.')\n    for job_idx, analysis_job in enumerate(self.analysis_job_queue):\n        self.logs.save_current_logs(analysis_job.results_dir_path)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnalysis jobs are currently processed sequentiallyâ€”one after another. However, each job itself is internally parallelized: the ROI traces are processed using all available CPU cores.\nThat means thereâ€™s definitely room for future optimization at the job level. So if youâ€™re feeling adventurous and want to dive into parallel job executionâ€¦ weâ€™d love to see your pull request! ðŸ”§ðŸš€\n\n\nThe private helper methods that run_analysis() uses are significantly less complex. As before, theyâ€™re grouped below for convenience to enable optional collapsing. ðŸ‘‡\n\n\nExported source\n@patch\ndef _display_configs(self: Model\n                    ) -&gt; None:\n    self.add_info_to_logs('Configurations for Analysis Settings and Result Creation validated successfully.', True)\n    self.add_info_to_logs(f'Analysis Settings are:')\n    for line in self.config.display_all_attributes():\n        self.add_info_to_logs(line)\n\n\n@patch\ndef _save_user_settings_as_json(self: Model, \n                                analysis_job: AnalysisJob\n                               ) -&gt; None:\n    filepath = analysis_job.results_dir_path.joinpath('user_settings.json')\n    self.config.recording_filepath = analysis_job.recording.filepath\n    if analysis_job.focus_area_enabled:\n        self.config.focus_area_filepath = analysis_job.focus_area.filepath\n    else:\n        self.config.focus_area_filepath = None\n    if analysis_job.rois_source == 'file':\n        self.config.roi_filepath = [roi.filepath for roi in analysis_job.all_rois]\n    with open(filepath, 'w+') as user_settings_json:\n        user_settings_json.write(self.config.to_json())\n\n\nThe Model is also designed to cooperate closely with na3â€™s graphical user interface (GUI) â€” without ever depending on it directly. This is thanks to a clear separation of concerns, following the Model-View-Controller (MVC) design pattern.\nThe idea is simple:\n\nThe Model knows how to run analyses\nThe View handles the display and user interaction\nThe controller (in this case, the App class) wires the two together\n\nTo support this structure, the Model exposes two public methods that let the controller hook up GUI elements like progress messages and result previews. Letâ€™s look at them:\n\nsource\n\n\nModel.setup_connection_to_update_infos_in_view\n\n Model.setup_connection_to_update_infos_in_view (update_infos:Callable)\n\nAllows to configure the widget in the GUI that is used to display the logs.\n\n\nExported source\n@patch\ndef setup_connection_to_update_infos_in_view(self: Model, \n                                             update_infos: Callable\n                                            ) -&gt; None:\n    \"\"\" Allows to configure the widget in the GUI that is used to display the logs. \"\"\"\n    self.callback_view_update_infos = update_infos\n    self.gui_enabled = True\n\n\nThis method registers a callback that will be used to send updates (logs or progress percentages) from the model to the GUIâ€™s log display widget. Once this is connected, log messages generated during analysis can be sent live to the GUIâ€™s log panel.\n\nsource\n\n\nModel.setup_connection_to_display_results\n\n Model.setup_connection_to_display_results (show_output_screen:Callable,\n                                            output:ipywidgets.widgets.widg\n                                            et_output.Output,\n                                            pixel_conversion:float)\n\nAllows to configure the widget in the GUI that is used to display images, plots, and figures.\n\n\nExported source\n@patch\ndef setup_connection_to_display_results(self: Model, \n                                        show_output_screen: Callable, \n                                        output: w.Output, \n                                        pixel_conversion: float\n                                       ) -&gt; None:\n    \"\"\" Allows to configure the widget in the GUI that is used to display images, plots, and figures. \"\"\"\n    self.callback_view_show_output_screen = show_output_screen\n    self.view_output = output\n    self.pixel_conversion = pixel_conversion\n    self.gui_enabled = True\n\n\nThis second method connects the model to GUI elements responsible for showing figures and images â€” specifically the results of an analysis job. Once wired up, na3 can render the activity overview plots or other visualizations directly into the notebook interface. This makes for a much smoother and more intuitive analysis experience â€” especially for exploratory workflows.\nThe next method was also designed to smoothen your experience with na3Â´s GUI while trying to figure out which grid size you should use:\n\nsource\n\n\nModel.preview_window_size\n\n Model.preview_window_size (grid_size)\n\n\n\nExported source\n@patch\ndef preview_window_size(self: Model, \n                        grid_size\n                       ) -&gt; tuple[Figure, Axes]:\n    job_for_preview = self.analysis_job_queue[0]\n    preview_fig, preview_ax = job_for_preview.preview_window_size(grid_size)\n    return preview_fig, preview_ax\n\n\nThis method shows you how the selected grid_size will divide your image into analysis windows. It uses the first job in the queue (since you havenâ€™t run any yet) and overlays the grid onto the corresponding image. It integrates directly with the GUI output, so users can click a button and immediately see how their current settings affect the analysis layout.\nAt the very beginning of this notebook, we introduced the humble Logger â€” a simple but powerful tool for capturing status updates throughout the analysis pipeline.\nWell, guess what: weâ€™re coming full circle now. The Model uses its own method, add_info_to_logs(), to centralize how messages are handled â€” whether youâ€™re running headless, in a notebook, or in the GUI.\n\nsource\n\n\nModel.add_info_to_logs\n\n Model.add_info_to_logs (message:str, display_in_gui:bool=False,\n                         progress_in_percent:float|None=None)\n\n\n\nExported source\n@patch\ndef add_info_to_logs(self: Model, \n                     message: str, \n                     display_in_gui: bool = False, \n                     progress_in_percent: float | None = None\n                    ) -&gt; None:\n    self.logs.add_new_log(message)\n    if (display_in_gui == True) and (self.gui_enabled == True): \n        self.callback_view_update_infos(message, progress_in_percent)\n\n\nThis method serves as a bridge between the raw logging system and the user interface. It logs the message internally, prints it to the console, and (if GUI mode is active) pushes the update to the GUI via the connected callback.\nItâ€™s small, but it plays a central role in making sure everything you do with NAÂ³ is traceable, debuggable, and visible â€” no matter how you interact with the tool.\nSo if youâ€™re seeing beautifully time-stamped updates in the GUI while a job runs, now you know whoâ€™s responsible! ðŸ™Œ\nAnd that concludes our tour through the source code of the Model â€” the central brain of na3!",
    "crumbs": [
      "API",
      "model"
    ]
  },
  {
    "objectID": "model.html#testing-testing-testing-..",
    "href": "model.html#testing-testing-testing-..",
    "title": "model",
    "section": "ðŸ§ª Testing, testing, testing ..",
    "text": "ðŸ§ª Testing, testing, testing ..\nWe emphasized above already how important testing of your research software code is. Therefore, we also have some more tests implemented in here that test some of the Models behavior\n\nfrom shutil import rmtree\nimport pandas as pd\nfrom pandas.testing import assert_frame_equal\n\nfrom neuralactivitycubic.view import WidgetsInterface\n\n\ntest_filepath = Path('../test_data/00')\nexample_results_dir = Path('../test_data/00/example_test_results_for_spiking_neuron')\nresults_filepath = Path('../test_data/00/results_directory')\n\n\ndef test_correct_model_run():\n    correct_config = WidgetsInterface().export_user_settings()\n    correct_config.data_source_path = test_filepath / 'spiking_neuron.avi'\n    correct_config.save_single_trace_results = True\n    model = Model(correct_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n    # evil hacks\n    return model.analysis_job_queue[0].results_dir_path\n\ndef test_correct_model_run_with_custom_results_dir():\n    correct_config = WidgetsInterface().export_user_settings()\n    correct_config.data_source_path = test_filepath / 'spiking_neuron.avi'\n    correct_config.results_filepath = results_filepath\n    model = Model(correct_config)\n    model.create_analysis_jobs()\n    model.run_analysis()\n    return correct_config.results_filepath\n\n\ndef _test_csv_files(relative_filepath_to_csv: str, results_dir: Path) -&gt; bool:\n    filepath = results_dir / relative_filepath_to_csv\n    # confirm results have been created:\n    if not filepath.is_file():\n        return False\n    # confirm computational consistency of results, while allowing minor numerical tolerance\n    df_test = pd.read_csv(filepath)\n    df_validation = pd.read_csv(example_results_dir / relative_filepath_to_csv)\n    if assert_frame_equal(df_test, df_validation) is not None:\n        return False\n    else:\n        return True\n\ndef test_all_peak_results(results_dir):\n    return _test_csv_files('all_peak_results.csv', results_dir)\n\ndef test_amplitude_and_df_over_f_results(results_dir):\n    return _test_csv_files('Amplitude_and_dF_over_F_results.csv', results_dir)\n\ndef test_auc_results(results_dir):\n    return _test_csv_files('AUC_results.csv', results_dir)\n\ndef test_variance_area_results(results_dir):\n    return _test_csv_files('Variance_area_results.csv', results_dir)\n\ndef test_representative_single_trace_results(results_dir):\n    return _test_csv_files('single_traces/data_of_ROI_7-10.csv', results_dir)\n\n\ndef test_activity_overview_png(results_dir):\n    filepath = results_dir / 'activity_overview.png'\n    return filepath.is_file()\n\ndef test_roi_label_ids_overview_png(results_dir):\n    filepath = results_dir / 'ROI_label_IDs_overview.png'\n    return filepath.is_file()\n\ndef test_individual_traces_with_identified_events_pdf(results_dir):\n    filepath = results_dir / 'Individual_traces_with_identified_events.pdf'\n    return filepath.is_file()\n\ndef test_logs_txt(results_dir):\n    filepath = results_dir / 'logs.txt'\n    return filepath.is_file()\n\ndef test_user_settings_json(results_dir):\n    filepath = results_dir / 'user_settings.json'\n    return filepath.is_file()\n\ndef test_nwb_export(results_dir):\n    filepath = results_dir / 'autogenerated_nwb_file.nwb'\n\nRun tests:\n\n# confirm that model can be executed:\nresults_directory = test_correct_model_run()\n\nassert results_directory.exists()\n\n# confirm all csv files have been created and are correct:\nassert test_all_peak_results(results_directory), 'There is an issue with the \"all_peak_results.csv\" file!'\nassert test_amplitude_and_df_over_f_results(results_directory)\nassert test_auc_results(results_directory)\nassert test_variance_area_results(results_directory)\nassert test_representative_single_trace_results(results_directory)\n\n# confirm all other result files have been created:\nassert test_activity_overview_png(results_directory)\nassert test_roi_label_ids_overview_png(results_directory)\nassert test_individual_traces_with_identified_events_pdf(results_directory)\nassert test_logs_txt(results_directory)\nassert test_user_settings_json(results_directory)\n\n# cleanup\nrmtree(results_directory)\n\n\n# confirm that model can be executed with custom results directory:\nresults_directory = test_correct_model_run_with_custom_results_dir()\n\nassert results_directory.exists()\n\n# confirm all csv files have been created and are correct:\nassert test_all_peak_results(results_directory), 'There is an issue with the \"all_peak_results.csv\" file!'\nassert test_amplitude_and_df_over_f_results(results_directory)\nassert test_auc_results(results_directory)\nassert test_variance_area_results(results_directory)\n\n# confirm all other result files have been created:\nassert test_activity_overview_png(results_directory)\nassert test_roi_label_ids_overview_png(results_directory)\nassert test_logs_txt(results_directory)\nassert test_user_settings_json(results_directory)\n\n# cleanup\nrmtree(results_directory)",
>>>>>>> fb1f145 (literate docs for model completed)
    "crumbs": [
      "API",
      "model"
    ]
  },
  {
    "objectID": "using_the_gui.html",
    "href": "using_the_gui.html",
    "title": "Using NAÂ³ via the GUI",
    "section": "",
    "text": "Note\n\n\n\nWeÂ´re still putting a few finishing touches onto the new implementation of NAÂ³, so please be aware that this version remains under active development and should not yet be considered as a stable release.",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#launching-the-gui",
    "href": "using_the_gui.html#launching-the-gui",
    "title": "Using NAÂ³ via the GUI",
    "section": "Launching the GUI",
    "text": "Launching the GUI\nAfter you successfully completed the installation of NAÂ³ (e.g.Â by following our Installation Guide), please open a JupyterNotebook to launch the GUI of NAÂ³.\nTo do this, open the Anaconda Prompt terminal (or the terminal of your choice) and make sure the virtual environment in which you installed NAÂ³ is active. If you followed our installation guide, you should see that the input line starts with (na3). If this is not the case, you can always activate it by running the following command:\n\nconda activate na3\n\nWith the virtual environment in which you installed NAÂ³ activated, please execute the following command in the terminal:\n\njupyter-lab\n\nNext, open a new JupyterNotebook by clicking on the â€œPython 3 (ipykernel)â€ button in the Notebook section. In this JupyterNotebook, paste the following code in a cell:\n\nimport neuralactivitycubic as na3\n\nna3.open_gui()\n\nThen, as a final step, execute the cell, for instance by clicking on the little play icon on top while the cell is selected and the GUI of NAÂ³ will open in the Notebook:\n\n\n\n\n\n\n\nTip\n\n\n\nIf youÂ´re working on a smaller screen, e.g.Â if youÂ´re using a notebook, you can use the keyboard shortcut Ctrl + b to collapse the file explorer panel of JupyterLab to the left of the JupyterNotebook to give you some extra horizontal space. You can always use the same keyboard shortcut again to bring it back again if needed.",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#quick-walkthrough",
    "href": "using_the_gui.html#quick-walkthrough",
    "title": "Using NAÂ³ via the GUI",
    "section": "Quick Walkthrough",
    "text": "Quick Walkthrough\nWeÂ´re all busy people and documentation can be quite exhaustive to read. Thus, hereÂ´s a quick walkthrough of NAÂ³Â´s core functionalities that should allow you to get going with your own first tests:\n\nFor more detailed descriptions of the individual features and settings, please see the sections below.",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#general-settings",
    "href": "using_the_gui.html#general-settings",
    "title": "Using NAÂ³ via the GUI",
    "section": "General Settings",
    "text": "General Settings\nThe first section of NAÂ³â€™s GUI prompts you to specify the general settings of your analysis before being able to continue. Most importantly, the selections will have an impact on the structure and organization of your source data that NAÂ³ expects. Thus, please make sure you consult the Source Data Structure section below to check what structure is requested for your specific combination of settings.\n\nROI modes\nNAÂ³Â´s core functionality is to compute the signal intensity over time for defined regions of interest (ROIs) within the whole image. You either have the chance to provide source data that defines these ROIs (â€œPredefined ROIsâ€), or you can use NAÂ³Â´s â€œGridâ€ mode to automatically create congruent square ROIs over your recording with adjustable sizes (Grid size - see Analysis Settings) that will be analyzed.\n\nGrid (congruent squares)\nThe main advantage of NAÂ³Â´s Grid mode is that it does not require any additional input other than your recording file to start the analysis with NAÂ³. This can be especially useful in high-throughput settings, as it eliminates additional (potentially manual) preprocessing steps to generate ROIs.\n\n\nPredefined ROIs\nSometimes you want to investigate specific features instead of the whole field of view, though. For this, NAÂ³ provides the option for you to provide predefined ROIs along with your source data. Currently, only ROIs created with the ImageJ tools are supported.\n\n\n\n\n\n\nTip\n\n\n\nThe predefined ROI filetype that your are using is currently not supported? That needs to be changed! Please drop us your feature request via GitHub and weÂ´ll implement it for you asap.\n\n\n\n\n\nBatch mode\nThe period of trouble shooting and of adjusting individual experimental parameters is finally over and you are ready to process all your data at once? ThatÂ´s awesome, congrats! As an additional reward, NAÂ³ offers you a batch mode feature: You select your entire dataset as source data and then only have to specify the settings once, hit â€œRun Analysisâ€ and then leave for a well deserved end-of-day, while NAÂ³ and your computer do the heavy lifting for you and process your entire dataset, recording after recording. Batch mode is compatible with all ROI-modes (Grid & predefined ROIs), and can even be combined with Focus Areas.\n\n\nFocus areas\nThis feature is for those of you who want - or may need - to squeeze out every single percent of processing speed. It allows you to provide another ROI (or even set of ROIs), to which NAÂ³ will restrict itÂ´s analysis, while ignoring any ROIs that reside outside of these Focus Areas. Since processing of each ROI takes ~ the same amount of time (it can be considered a linear operation), not processing 50 % of the ROIs will cut your processing time almost in half! We tried to illustrate this with the following example, in which we used NAÂ³Â´s Grid mode with the Focus Area being disabled or enabled, saving ~2/3Â´s of processing time:\n\n\n\nFocus Area mode to max out performance.\n\n\n\n\nSource data structure\nNAÂ³ expects your source data to be organized in a very specific manner for it to function correctly, and this structure looks slightly different, depending on which processing modes you want to use. We actually spent quite some time thinking about it, with the intention of creating a structure that does not result in conflicts when you start switching between modes when exploring what method fits your data best and requires you to constantly re-adjust your source data. But this will only become evident over time when people like you test and use NAÂ³ - so weÂ´d highly appreciate your feedback, especially if you should run into some issues with the required source data structures! Please open an issue on GitHub and weÂ´re eager to fix it.\nTo understand what source data structure is expected for your specific combination of processing modes, we provide you with representative examples for each. Please expand the following sections to browse through them. In all cases, the â€œSource Data Pathâ€ that you have to select when using NAÂ³ is always the file/folder that is shown at the very top and is highlighted in magenta (e.g.Â the parent directory your_dataset/ or single_recording/, or the recording file itself your_recording_file.avi):\n\n\n\n\n\n\nROI-mode: Grid\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nThis is as straightforward as it gets! All you need to do is select the recording file youÂ´d like to analyze:\n\n\n\nBatch-mode: OFF | ROI-mode: Grid (congruent squares) | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nThis requires you to organize your data within a directory (called single_recording in this example) which contains your recording file and another subdirectory in which the Focus Area ROI(s) are located. This subdirectory must be called focus_areas for NAÂ³ to be able to recognize it:\n\n\n\nBatch-mode: OFF | ROI-mode: Grid (congruent squares) | Focus Area: ON\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROI-mode: predefined ROIs\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nUsing predefined ROIs for analysis rather than the automatically generated Grid requires you to organize your source data within a directory (called single_recording in this example), which contains a single recording file (e.g.Â your_recording.avi in this example) and as many ROI files as you need. In this example, there is only a single predefined_ROIs.zip file, which was generated with ImageJ2 and which contains multiple ROIs. You could, however, also provide each ROI individually, e.g.Â as ROI_a.roi, ROI_b.roi, and so on.\n\n\n\nBatch-mode: OFF | ROI-mode: predefined ROIs | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nUsing predefined ROIs for analysis rather than the automatically generated Grid requires you to organize your source data within a directory (called single_recording in this example), which contains a single recording file (e.g.Â your_recording.avi in this example) and as many ROI files as you need. In this example, there is only a single predefined_ROIs.zip file, which was generated with ImageJ2 and which contains multiple ROIs. You could, however, also provide each ROI individually, e.g.Â as ROI_a.roi, ROI_b.roi, and so on.\nFor NAÂ³ to differentiate between predefined ROIs youÂ´d like to analyze, and ROIs that are intended as Focus Areas, you have to provide the Focus Area ROIs within a subdirectory that must be named focus_areas. In this directory, you can provide as many ROI files as needed, just like for the predefined ROIs.\n\n\n\nBatch-mode: OFF | ROI-mode: predefined ROIs | Focus Area: ON\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch mode: ON\n\n\n\n\n\nWith activate batch mode, you essentially have to provide the same data structure as described above for each individual recording, and then organize all recordings as nested directory. You can find some representative examples again for all possible mode combinations with enabled batch mode below:\n\n\n\n\n\n\nROI-mode: Grid\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nIn this case, you only need to separate the respective recording files in individual subdirectories:\n\n\n\nBatch-mode: ON | ROI-mode: Grid (congruent squares) | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nWith enabled Focus Area, you can specify focus area ROIs for each recording individually. For this, you have to add a subdirectory called focus_areas within the respective recording subdirectory. Please note, that you can add different focus area ROIs for each recording, and you can also opt to not provide any focus area ROIs for individual recordings - here, the entire field of view will then be analyzed.\n\n\n\nBatch-mode: ON | ROI-mode: Grid (congruent squares) | Focus Area: ON\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nROI-mode: predefined ROIs\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: OFF\n\n\n\n\n\nIf youÂ´d like to use batch mode processing in combination of using predefined ROIs for analyses, please organize your individual recordings together with the corresponding ROI files into separate subdirectories. Please note, that you can (obviously) provide different ROI files for each recording, and that they also donÂ´t have to be of the same filetype (e.g.Â .zip and .roi files in this example):\n\n\n\nBatch-mode: ON | ROI-mode: predefined ROIs | Focus Area: OFF\n\n\n\n\n\n\n\n\n\n\n\nFocus Area: ON\n\n\n\n\n\nTo use predefined ROIs with enabled Focus Area and batch mode, please provide your data organized as depicted below. Again, please note that any combination of ROI files - for both predefined ROIs and Focus Areas - is possible. For the Focus Areas, itÂ´s also supported to not provide ROIs for individual recordings. You always have to provide some predefined ROIs, though, if you select this mode.\n\n\n\nBatch-mode: ON | ROI-mode: predefined ROIs | Focus Area: ON",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#analysis-settings",
    "href": "using_the_gui.html#analysis-settings",
    "title": "Using NAÂ³ via the GUI",
    "section": "Analysis Settings",
    "text": "Analysis Settings\nNAÂ³ provides you with several option to configure the analysis exactly as you need it for your specific dataset. There are a few settings, however, that are strictly required (grouped here in the â€œAnalysis Settingsâ€) and some that are optional (correpsondingly in the â€œOptional Settingsâ€). For all required settings, there are default values specified that NAÂ³ will use if you donÂ´t change them. They certainly provide a generic good-fit, but you might want to tweak some of them as needed to accomodate for some special characteristics of your individual dataset!\n\nGrid Size\nAllows you to specify the size in pixel of the congruent squares that will be generated and used as ROIs for analysis. For instance, selecting â€œ10â€ means that NAÂ³ will create 10 x 10 pixel squares as ROIs for analysis. You can use the Preview button next to the slider widget to create a preview of how the grid would look like on your specific data. Moreover, NAÂ³ will highlight the area within the field of view within which the indicated squares will be analyzed by a cyan box. This can be of interest, as NAÂ³ might crop your image - depending on the dimensions of your field of view and the selected grid size - to ensure equal sizes of the square ROIs across the entire field of view. Again, as an example, if your field of view is 432 x 567 px, and you specify a grid size of 10 x 10 px, NAÂ³ will crop your field of view by 2 and 7 pixels, respectively - i.e.Â to 430 x 560 px - to be able to fill the entire (remaining) field of view with congruent squares.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this setting is only available in the ROI mode â€œGrid (congruent squares)â€ is selected.\n\n\n\n\nSignal-to-Noise Ratio (SNR)\nTo identify peaks within the signal intensity trace (i.e.Â a timeseries of bit-values) for each analysis ROI, NAÂ³ leverages the find_peaks_cwt function implemented by SciPy. The SNR value you specify in NAÂ³ will be used as min_snr in when calling the find_peaks_cwt function.\n\n\nNoise Window Size (NWS)\nThe SNR value you specify in NAÂ³ will be used as window_size in when calling the find_peaks_cwt function.\n\n\nSignal Average Threshold (SAT)\nThis value allows you to correct for global background noise. For each analysis ROI, NAÂ³ computes the mean intensity value over the entire timeseries for the specified analysis interval. Only if this â€œaverage signalâ€ matches or exceeds the defined SAT value, peak detection computations will be performed for this analysis ROI.\n\n\nMinimal Activity Counts (MAC)\nAllows to filter out analysis ROIs with too few peaks. If there were less peaks detected for a given analysis ROI than defined as MAC, this ROI will be excluded from the final results.\n\n\nBaseline Estimation Method\nNAÂ³ computes area-under-curve for the identified peaks. In order to do so, a baseline for the signal is required. For this, NAÂ³ leverages a selection of the baseline estimation methods offered by pybaselines:\n\nAsymmetric Least Squares: Uses the asls method with itÂ´s default values. See here for additional information and representative baseline computations using this method.\nFully Automatic Baseline Correction: Uses the fabc method with itÂ´s default values. See here for additional information and representative baseline computations using this method.\nPeaked Signalâ€™s Asymmetric Least Squares Algorithm: Uses the asls method with itÂ´s default values. See here for additional information and representative baseline computations using this method.\nStandard Deviation Distribution: Uses the asls method with itÂ´s default values. See here for additional information and representative baseline computations using this method.\n\n\n\n\n\n\n\nNot the Baseline Estimation method youÂ´re looking for?!\n\n\n\nWith the highly modular design of NAÂ³, it is very straightforward for us to implement any of the baseline estimation methods provided by the pybaselines Python package. So if there is another one that sparks your interest or is absolutely crucial for your specific analysis - donÂ´t hesitate for even a second! Please drop us your request directly via GitHub and weÂ´ll be happy to implement it for you.",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#optional-settings",
    "href": "using_the_gui.html#optional-settings",
    "title": "Using NAÂ³ via the GUI",
    "section": "Optional settings",
    "text": "Optional settings\nNAÂ³ also provides you with some optional settings. These are - as their category indicates - not required for processing of your Calcium Imaging data, but enable you to make some very specifc adjustments. To enable them, click the check box and the corresponding settings widget will appear - uncheck the checkbox again, and the settings widgets dissapear (and the settings you made will be ignored).\n\nInclude Variance\nAs described in the original paper by Prada et al.Â (2018), the signalÂ´s variance can be used as a proxy for neuronal excitability1. To enable variance computation of the signal for your analysis, click the checkbox next to â€œinclude varianceâ€. A new widget will appear and allow you to specify the Variance value (default: 15). This is the window size that will be used to compute the variance of the signal intensity for each analysis ROI with a sliding window approach.\n\n\nAnalyze Interval\nIf you are interested in analyzing only a specific interval of your recording, this feature is for you. Simply enable it by clicking the checkbox and another widget will appear, allowing you to use two sliders that specify start & end frame of the interval youÂ´d like to analyze (inclusive ends, e.g.Â 12 - 55 means that you will analyze the interval starting at frame 12 and ending at frame 55, with both frames 12 & 55 included).\n\n\n\n\n\n\nImportant\n\n\n\nIf youÂ´re using this feature in combination with enabled batch mode processing, please be aware that you are not able to configure the interval for each individual recording, but that the same frame interval will be used for all recordings in this batch!\n\n\n\n\nConfigure Octaves\nYou can enable this feature again by clicking the checkbox next to it, which will cause another widget to appear, allowing you to specify the Min. Octaves. As described above, NAÂ³ uses the find_peaks_cwt function implemented in SciPy. You can use this optional Min. Octaves setting to change the value that NAÂ³ will pass to this function as min_length. However, this is not a direct mapping, but min_length will be computed from the Min. Octaves value you specified, in dependence of the length (i.e.Â number of frames) of the respective recording. The exact computation is as follows, where:\n\nself.mean_intensity_over_time.shape[0] = Number of frames in the recording (potentially limited by analysis interval)\nmin_octave_span = the value you set via Min. Octaves\n\n\nwidths = np.logspace(np.log10(1), np.log10(self.mean_intensity_over_time.shape[0]), 100)\nmin_length = octaves_ridge_needs_to_spann / np.log2(widths[1] / widths[0])\n\n\n\n\n\n\n\nWarning\n\n\n\nIn general, we donÂ´t recommend changing this value unless you are highly familiar with NAÂ³ and have utilized all other configuration options already.",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#result-settings",
    "href": "using_the_gui.html#result-settings",
    "title": "Using NAÂ³ via the GUI",
    "section": "Result settings",
    "text": "Result settings\nThese settings allow you to specify whether you want NAÂ³ to create result files for your current analysis (e.g.Â to avoid creation of result files while youÂ´re still testing different setting configurations or are testing different processing modes). Creation of all result files is enabled by default.\n\nSave Overview Plot\nIf you uncheck this box, the overview.png fill will not be created.\n\n\nSave Detailed Results\nIf you uncheck this box, none of the other result files will be created, i.e.: - all_peak_results.csv - Amplitude_and_dF_over_F_results.csv - AUC_results.csv - Individual_traces_with_identified_events.pdf - logs.txt - ROI_label_IDs_overview.png - user_settings.json - Variance_area_results.csv",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#generated-results",
    "href": "using_the_gui.html#generated-results",
    "title": "Using NAÂ³ via the GUI",
    "section": "Generated Results",
    "text": "Generated Results\nNAÂ³ generates several comprehensive result files for you (see list above). You will always find the results in the same directory in which the corresponding recording was located. That means, in case youÂ´ve been using the batch processing mode, NAÂ³ will create separate result folders in each recording subdirectory. The result folders will have the following naming conventions:\n\ndatetime_results_for_recording_filename, e.g.: 2024_12_17_13-48-54_results_for_spiking_neuron\n\nor\n\ndatetime_results_for_recording_filename_with_focus_area_roi_filename, e.g.: 2024_12_17_13-48-54_results_for_spiking_neuron_with_area_a\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are using more than one Focus Area ROI, NAÂ³ will create a new results directory for each of them, to ensure you can easily differentiate between them.\n\n\nLetÂ´s take a look at them in more details:\n\nLogs & User Settings\nUpfront we have two files that NAÂ³ creates that are intended to ensure reproducibility, namely user_settings.json and logs.txt. While the human and machine readable JSON file makes sure you never forget to note down what the specific settings for this very analyses were, the Logs file saves some additional information regarding the processing and might be of value in case there is some troubleshooting or debugging that needs to happen.\n\n\nActivity Overview\nNAÂ³ offers an elegant way to capture high-level activity patterns across all ROIs of your field of view at a glance. For this, the â€œActivity Overviewâ€ plot is generated, that can be saved both as a separate PNG image (activity_overview.png) and as part of the wholistic PDF (Individual_traces_with_identified_events.pdf). It will look something like this:\n\n\n\nRepresentative Activity Overview Plot\n\n\nIt will indicate the respective ROIs with the total detected peaks per ROI, after applying all thresholding and filtering criteria (e.g.Â Minimum Activity Count). The plots title also informs you about the total activity detected, i.e.Â the sum of all individually registered peaks across all ROIs within the field of view. In this example, the Grid method was used to automatically generate congruent squares covering most of the field of view (cropping boundaries are indicated by cyan border). The x- and y-axis labels provide a unique XY ID for each of the squares, which allows you to match the information presented in other plots or result files to a uniquely identifiable ROI.\n\n\n\n\n\n\nWhat are the unique IDs of my predefined ROIs?\n\n\n\nThere is also a ROI_label_IDs_overview.png file that NAÂ³ creates, which shows the ID of each ROI within itÂ´s respective outline - which is specifically in case of predefined ROIs an important reference, since they cannot be defined using XY coordinates in a Grid. For reasons of better visibility & readability, only the IDs of ROIs that remain after applying all fitering & thresholding critera are shown in these overviews:\n\n\n\nUnique label IDs for predefined ROIs\n\n\n\n\n\n\nActivity Traces with Identified Events\nFor visual inspection, the PDF Individual_traces_with_identified_events.pdf gives you an even more detailed overview of your analysis results - and likely also allows you to assess itÂ´s quality with the current settings. The first three pages of the PDF contain the following plots:\n\nThe Activity Overview plot\nA brightness and contrast enhanced image of your field of view with only the ROIs superimposed (no activity counts)\nThe ROI label ID Overview plot\n\nOn all subsequent pages of this PDF will be one plot for each ROI with sufficient activity according to your settings. Each plot shows the extracted signal intensity trace (bit values on y-axis and frame number on x-axis) in gray and the estimated baseline in cyan. Identified peaks are marked with dots on the signal intensity trace, where a magenta coloration indicates that an area-under-curve (AUC) value was calculated for the respective event, while black coloration indicates that this was not possible (this is usually the case if the signal trace did not cross the estimated baseline on both sides of the peak, i.e.Â usually at the beginning or at the very end of the recording). Moreover, a yellow fill indicates the identified AUC, respectively.\n\n\n\nRepresentative signal trace plot with identified activity events\n\n\n\n\nCSV-encoded Results\n\nAll Peak Results\nThis table comprises all computed characteristics for each identified activity peak across all ROIs. This information can be found in each column:\n\nROI label ID: The unique label ID that allows you to identify the ROI, e.g.Â â€œ3/15â€ in Grid mode or â€œ7â€ when using predefined ROIs, as described above.\npeak frame index: The frame index at which the respective peak was identified.\npeak bit value: The raw bit value of the signal trace at the indicated peak frame index. Remember: the computed signal trace represents the average intensity across all pixels within the given ROI.\npeak amplitude: The absolute difference between the peak bit value and the value of the estimated baseline at the indicated peak frame index, calculated as: peak bit value - bit value estimated baseline\npeak dF/F: The relative difference between peak bit value and the value of the estimated baseline at the indicated peak frame index, calculated as: peak amplitude / bit value estimated baseline\npeak AUC: The value of the area-under-curve associated with this peak. See the warning below, however.\npeak classification: A classification of whether the peak is singular, clustered, or isolated:\n\nisolated: A peak for which no area-under-curve could be calculated. This is usually the case, if the signal trace did not cross the estimated baseline on both sides of the peak, i.e.Â usually at the beginning or at the very end of the recording.\nsingular: A peak for which an area-under-curve could be calculated that is solely associated with this singular peak, i.e.Â the signal trace crosses the baseline between this and any preceeding or following peaks.\nclustered: A peak for which an area-under-curve could be calculated, but it is associated with multiple peaks, i.e.Â the signlar trace did not cross the baseline between this and a preceeding or following peak.\n\n\n\n\n\n\n\n\nAUCs of clustered peaks\n\n\n\nIn case of clustered peaks, NAÂ³ cannot clearly associate a given AUC value with a single peak, because the signal trace does not cross the estimated baseline between two (or more) peaks. Therefore, both peaks will be listed with the identical AUC value - which you may have to take into account when you perform statistical analysis of your data. We decided to include them, though, because these clustered peaks could also represent event trains, which might be interesting features to look at for some of you.\n\n\n\n\nArea-under-Curve (AUC) Results\nEssentially the same information about AUC values per peak, organized in a different format, though: wide instead of long.\n\n\nAmplitude and dF/F Results\nEssentially the same information about amplitude and dF/F values per peak, organized in a different format, though: wide instead of long.\n\n\nVariance Results (optional)\nVariance results will only be generated if you enabled the optional setting â€œInclude Varianceâ€. It comprises the computed variance area per ROI.",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#wed-love-to-hear-your-feedback",
    "href": "using_the_gui.html#wed-love-to-hear-your-feedback",
    "title": "Using NAÂ³ via the GUI",
    "section": "WeÂ´d love to hear your Feedback:",
    "text": "WeÂ´d love to hear your Feedback:\nYou are using NAÂ³ for your research - or would like to do so, but thereÂ´s that one cool feature missing for you? ThatÂ´s great! WeÂ´d love to hear your feedback, feature requests, or bug reports to keep improving NAÂ³ - please use this form on GitHub to submit it, we appreciate it a lot!\nPlease feel free to also explore all other Research Software that our not-for-profit organization has developed, or learn more about us and what we do on our website. If you are interested to partner with us for a similar collaboration on your Research Software, or to develop it with or for you from scratch, donÂ´t hesitate and drop us an email - weÂ´re always looking for new opportunities to catalyze science be delivering cutting-edge technology tailored to the specific research challenges of our collaborators!\n\n\n\nIndoc Research Europe gGmbH",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "using_the_gui.html#footnotes",
    "href": "using_the_gui.html#footnotes",
    "title": "Using NAÂ³ via the GUI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrada J, Sasi M, Martin C, Jablonka S, Dandekar T, Blum R (2018) An open source tool for automatic spatiotemporal assessment of calcium transients and local â€˜signal-close-to-noiseâ€™ activity in calcium imaging data. PLoS computational biology 14(3): e1006054. https://doi.org/10.1371/journal.pcbi.1006054â†©ï¸Ž",
    "crumbs": [
      "User Guides",
      "Using NAÂ³ via the GUI"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "results",
    "section": "",
    "text": "source\n\nplot_roi_boundaries\n\n plot_roi_boundaries (roi:Union[neuralactivitycubic.input.ROI,neuralactivi\n                      tycubic.analysis.AnalysisROI], line_color:str,\n                      line_style:str, line_width:Union[int,float])\n\n\nsource\n\n\nplot_window_size_preview\n\n plot_window_size_preview (preview_image:numpy.ndarray,\n                           grid_configs:Dict[str,Any], focus_area_roi:Opti\n                           onal[neuralactivitycubic.input.ROI]=None)\n\n\nsource\n\n\nplot_activity_overview\n\n plot_activity_overview (analysis_rois_with_sufficient_activity:Union[List\n                         [neuralactivitycubic.analysis.AnalysisROI],List],\n                         preview_image:numpy.ndarray,\n                         indicate_activity:bool=False, focus_area:Optional\n                         [neuralactivitycubic.input.ROI]=None,\n                         grid_configs:Optional[Dict[str,Any]]=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nanalysis_rois_with_sufficient_activity\nUnion\n\nWith python 3.11 - change back to: List[Never]\n\n\npreview_image\nndarray\n\n\n\n\nindicate_activity\nbool\nFalse\n\n\n\nfocus_area\nOptional\nNone\n\n\n\ngrid_configs\nOptional\nNone\n\n\n\nReturns\nTuple\n\n\n\n\n\n\nsource\n\n\nplot_rois_with_label_id_overview\n\n plot_rois_with_label_id_overview (analysis_rois_with_sufficient_activity:\n                                   Union[List[neuralactivitycubic.analysis\n                                   .AnalysisROI],List],\n                                   preview_image:numpy.ndarray, focus_area\n                                   :Optional[neuralactivitycubic.input.ROI\n                                   ]=None, grid_configs:Optional[Dict[str,\n                                   Any]]=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nanalysis_rois_with_sufficient_activity\nUnion\n\nWith python 3.11 - change back to: List[Never]\n\n\npreview_image\nndarray\n\n\n\n\nfocus_area\nOptional\nNone\n\n\n\ngrid_configs\nOptional\nNone\n\n\n\nReturns\nTuple\n\n\n\n\n\n\nsource\n\n\nplot_intensity_trace_with_identified_peaks_for_individual_roi\n\n plot_intensity_trace_with_identified_peaks_for_individual_roi\n                                                                (analysis_\n                                                                roi:neural\n                                                                activitycu\n                                                                bic.analys\n                                                                is.Analysi\n                                                                sROI)\n\n\nsource\n\n\nexport_peak_results_df_from_analysis_roi\n\n export_peak_results_df_from_analysis_roi\n                                           (analysis_roi:neuralactivitycub\n                                           ic.analysis.AnalysisROI)\n\n\nsource\n\n\ncreate_single_roi_amplitude_and_delta_f_over_f_results\n\n create_single_roi_amplitude_and_delta_f_over_f_results\n                                                         (df_all_results_s\n                                                         ingle_roi:pandas.\n                                                         core.frame.DataFr\n                                                         ame,\n                                                         zfill_factor:int)\n\n\nsource\n\n\ncreate_single_roi_auc_results\n\n create_single_roi_auc_results\n                                (df_all_results_single_roi:pandas.core.fra\n                                me.DataFrame, zfill_factor:int)",
    "crumbs": [
      "API",
      "results"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to NeuralActivityCubic",
    "section": "",
    "text": "NeuralActivityCubic (NAÂ³) is an open-source calcium image analysis tool published in 2018 by J. Prada and colleagues1, who describe it as following in their Author Summary:\nSince its publication in 2018, updates to several software packages on which the original implementation of NA3 depends have rendered this version of NAÂ³ virtually un-installable and, thus, effectively inaccessible for its target user audience - the Neuroscientific Community. Given the continued interest in NAÂ³, however, this was not acceptable. Thus, we formed a collaboration between the original developers of NAÂ³ and research software engineering experts from the not-for-profit organization Indoc Research Europe to revamp NAÂ³, with the goal of making it easily accessible to the Neuroscientific Community once again. While on it, we also enhanced NAÂ³Â´s performance, itÂ´s scope of features, and itÂ´s maintainability to ensure NAÂ³ remains accessible moving forward. Today, weÂ´re happy to present to you this revamped version of NAÂ³ - we hope youÂ´ll like it!\nNote: WeÂ´re still putting a few finishing touches on this new implementation of NAÂ³, so please be aware that this version remains under active development and should not yet be considered as a stable release. WeÂ´re currently also working on a paper describing our work in more details, so make sure you stay tuned and regularly check these docs for updates!",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Welcome to NeuralActivityCubic",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nIf youÂ´re comfortable working with virtual Python environments and installing packages via command line interfaces, please follow one of the subsequent options to install NAÂ³. If youÂ´d prefer a full step-by-step guide instead, we also got you covered: please find our detailed installation guide here.\nInstall latest from GitHub:\n$ pip install git+https://github.com/Indoc-Research/neuralactivitycubic.git\nor from pypi\n$ pip install neuralactivitycubic\n\n\nDocumentation\nDocumentation for NAÂ³ can be found here.",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#how-to-use---quick-start",
    "href": "index.html#how-to-use---quick-start",
    "title": "Welcome to NeuralActivityCubic",
    "section": "How to use - quick start:",
    "text": "How to use - quick start:\nAfter installing neuralactivitycubic, open a Jupyter Notebook and execute the following code to launch the GUI of NAÂ³:\n\nimport neuralactivitycubic as na3\n\nna3.open_gui()\n\n\n\n\nGUI of NAÂ³.",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "Welcome to NeuralActivityCubic",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall NeuralActivityCubic in Development mode\n# make sure NeuralActivityCubic package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to NeuralActivityCubic\n$ nbdev_prepare",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Welcome to NeuralActivityCubic",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPrada J, Sasi M, Martin C, Jablonka S, Dandekar T, Blum R (2018) An open source tool for automatic spatiotemporal assessment of calcium transients and local â€˜signal-close-to-noiseâ€™ activity in calcium imaging data. PLoS computational biology 14(3): e1006054. https://doi.org/10.1371/journal.pcbi.1006054â†©ï¸Ž",
    "crumbs": [
      "Welcome to NeuralActivityCubic"
    ]
  },
  {
    "objectID": "input.html",
    "href": "input.html",
    "title": "input",
    "section": "",
    "text": "source",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "input.html#source-data-handler",
    "href": "input.html#source-data-handler",
    "title": "input",
    "section": "Source Data Handler:",
    "text": "Source Data Handler:\n\nsource\n\nData\n\n Data (filepath:pathlib.Path, loaded_data:Any)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nRecording\n\n Recording (filepath:pathlib.Path, loaded_data:Any)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nROI\n\n ROI (filepath:pathlib.Path, loaded_data:Any)\n\nHelper class that provides a standard way to create an ABC using inheritance.",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "input.html#source-data-loader",
    "href": "input.html#source-data-loader",
    "title": "input",
    "section": "Source Data Loader:",
    "text": "Source Data Loader:\n\nsource\n\nDataLoader\n\n DataLoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nGridWrapperROILoader\n\n GridWrapperROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nRecordingLoader\n\n RecordingLoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nAVILoader\n\n AVILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nNWBRecordingLoader\n\n NWBRecordingLoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nROILoader\n\n ROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nImageJROILoader\n\n ImageJROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nNWBROILoader\n\n NWBROILoader (filepath:pathlib.Path)\n\nHelper class that provides a standard way to create an ABC using inheritance.",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "input.html#loader-factories",
    "href": "input.html#loader-factories",
    "title": "input",
    "section": "Loader Factories",
    "text": "Loader Factories\n\nsource\n\nDataLoaderFactory\n\n DataLoaderFactory ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nRecordingLoaderFactory\n\n RecordingLoaderFactory ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nROILoaderFactory\n\n ROILoaderFactory ()\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\nsource\n\n\nget_filepaths_with_supported_extension_in_dirpath\n\n get_filepaths_with_supported_extension_in_dirpath (dirpath:pathlib.Path,\n                                                    all_supported_extensio\n                                                    ns:List[str], max_resu\n                                                    lts:Optional[int]=None\n                                                    )\n\n\nsource\n\n\nRecLoaderROILoaderCombinator\n\n RecLoaderROILoaderCombinator (dir_path:pathlib.Path)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n# Specification of filepaths that can be used for testing:\n\n### ImageJ ROI filepaths:\nvalid_imagej_roi_filepath_1 = Path('../test_data/00/RoiSet_spiking.zip')\nvalid_imagej_roi_filepath_2 = Path('../test_data/00/focus_area/focus_spiking.roi')\n### NWB ROI filepath:\nvalid_nwb_roi_filepath = Path('../test_data/02/spiking_neuron.nwb')\n\n### AVI Recording filepath:\nvalid_avi_recording_filepath = Path('../test_data/00/spiking_neuron.avi')\n### NWB Recording filepath:\nvalid_nwb_recording_filepath = Path('../test_data/02/spiking_neuron.nwb')\n\n### A filepath of an unsupported file type:\nunsupported_filepath = Path('../test_data/00/example_test_results_for_spiking_neuron/activity_overview.png')\n\n\nroi_loader_factory = ROILoaderFactory()\n\n# tests that the ROILoaderFactory returns the correct Loader subclass for the respective file types:\nassert isinstance(roi_loader_factory.get_loader(valid_imagej_roi_filepath_1), ImageJROILoader)\nassert isinstance(roi_loader_factory.get_loader(valid_imagej_roi_filepath_2), ImageJROILoader)\nassert isinstance(roi_loader_factory.get_loader(valid_nwb_roi_filepath), NWBROILoader)\n\n# tests that the ROILoaderFactory raises a NotImplementedError for an unsupported file type:\nassert test_unsupported_file_extension(roi_loader_factory, unsupported_filepath)\n\n# tests that the individual Loader subclass correctly loads and parses the file content into ROI instances:\nassert test_successful_roi_loading(valid_imagej_roi_filepath_1)\nassert test_successful_roi_loading(valid_imagej_roi_filepath_2)\nassert test_successful_roi_loading(valid_nwb_roi_filepath)\n\n\nrecording_loader_factory = RecordingLoaderFactory()\n\n# tests that the RecordingLoaderFactory returns the correct Loader subclass for the respective file types:\nassert isinstance(recording_loader_factory.get_loader(valid_avi_recording_filepath), AVILoader)\nassert isinstance(recording_loader_factory.get_loader(valid_nwb_recording_filepath), NWBRecordingLoader)\n\n# tests that the RecordingLoaderFactory raises a NotImplementedError for an unsupported file type:\nassert test_unsupported_file_extension(recording_loader_factory, unsupported_filepath)\n\n# tests that the individual Loader subclass correctly loads and parses the file content into Recording instances:\nassert test_successful_recording_loading(valid_avi_recording_filepath)\nassert test_successful_recording_loading(valid_nwb_recording_filepath)",
    "crumbs": [
      "API",
      "input"
    ]
  },
  {
    "objectID": "controller.html",
    "href": "controller.html",
    "title": "controller",
    "section": "",
    "text": "source\n\nApp\n\n App ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nopen_gui\n\n open_gui ()\n\nStart the interactive widgets interface for NeuralActivityCubic\n\n\nGUI Tests Dependencies\n\nimport os\nimport uuid\nimport sys\nimport subprocess\nimport ipykernel\nimport requests\n\nfrom time import sleep\nfrom pathlib import Path\nfrom playwright.async_api import expect\nfrom playwright.async_api import ViewportSize\nfrom playwright.async_api import async_playwright\nfrom jupyter_server import serverapp\nfrom jupyter_server.services.contents.filemanager import FileContentsManager\nfrom contextlib import contextmanager\nfrom contextlib import asynccontextmanager\nfrom nbformat import v4 as nbformat\n\n\ndef get_kernel_id() -&gt; str | None:\n    try:\n        connection_file = ipykernel.get_connection_file()\n        return Path(connection_file).stem.split('-', 1).pop(-1)\n    except RuntimeError:\n        return None\n\n\ndef find_server_info_by_kernel_id(kernel_id: str):\n    servers = list(serverapp.list_running_servers())\n    for server in servers:\n        try:\n            response = requests.get(f\"{server['url']}api/sessions\", headers={'Authorization': f\"token {server['token']}\"})\n            for session in response.json():\n                if session['kernel']['id'] == kernel_id:\n                    return server\n        except Exception as e:\n            continue\n\n    return None\n\n\ndef find_server_info_by_process_id(process_id: int):\n    servers = list(serverapp.list_running_servers())\n    for server in servers:\n        if server['pid'] == process_id:\n            return server\n\n    return None\n\n\n@contextmanager\ndef run_jupyter_server_app(project_root: str):\n    jupyter_process = subprocess.Popen([sys.executable, '-m', 'jupyter', 'server', f'--ServerApp.root_dir={project_root}', '--ServerApp.open_browser=false'])\n\n    sleep(10)  # Wait for the server to start\n\n    yield find_server_info_by_process_id(jupyter_process.pid)\n\n    jupyter_process.terminate()\n\n\n@contextmanager\ndef get_jupyter_server():\n    kernel_id = get_kernel_id()\n\n    if kernel_id:\n        yield find_server_info_by_kernel_id(kernel_id)\n    else:\n        with run_jupyter_server_app(project_root=Path.cwd().parent) as server_info:\n            yield server_info\n\n\n@contextmanager\ndef create_temporary_notebook(cells: list[str], kernel_name: str = 'python3'):\n    fcm = FileContentsManager()\n    name = f\"test-{uuid.uuid4()}.ipynb\"\n    try:\n        notebook = nbformat.new_notebook()\n        notebook['metadata'] = {'kernelspec': {'name': kernel_name, 'display_name': kernel_name}}\n        for cell in cells:\n            notebook['cells'].append(nbformat.new_code_cell(source=cell))\n        fcm.save({'type': 'notebook', 'content': notebook}, name)\n        yield name\n    finally:\n        fcm.delete(name)\n\n\n@asynccontextmanager\nasync def provide_playwright_page(headless: bool):\n    browser = 'chromium'\n    if headless:\n        browser = 'chromium-headless-shell'\n    subprocess.run([sys.executable, '-m', 'playwright', 'install', '--with-deps', browser])\n\n    async with async_playwright() as playwright:\n        browser_type = playwright.chromium\n        browser = await browser_type.launch(headless=headless)\n        page = await browser.new_page(viewport=ViewportSize(width=1280, height=1024))\n\n        yield page\n\n        await browser.close()\n\n\n@asynccontextmanager\nasync def provide_na3_gui(visible: bool = True):\n    with get_jupyter_server() as server_info, create_temporary_notebook(cells=['import neuralactivitycubic as na3; na3.open_gui()']) as notebook_name:\n        token = server_info['token']\n        lab_url = server_info['url'] + 'lab/tree/nbs'\n\n        async with provide_playwright_page(headless=not visible) as page:\n            await page.goto(f'{lab_url}/{notebook_name}?token={token}')\n\n            await expect(page.get_by_role('main')).to_contain_text(notebook_name)\n\n            selected_tab_id = await page.get_by_role('main').get_by_role('tab', selected=True).first.get_attribute('data-id')\n            selected_tab = page.locator(f'#{selected_tab_id}').first\n            await expect(selected_tab).to_be_visible()\n\n            try:\n                await expect(page.locator('#filebrowser')).to_be_visible(timeout=1)\n                await page.locator('li[data-id=\"filebrowser\"]').click()\n            except AssertionError:\n                pass\n\n            kernel_status = selected_tab.locator('.jp-Notebook-ExecutionIndicator')\n            await expect(kernel_status).to_have_attribute('data-status', 'idle')\n            await page.wait_for_timeout(timeout=1000)\n\n            run_menu = page.get_by_role('menuitem').filter(has=page.get_by_text('Run')).first\n            await run_menu.click()\n\n            run_all_cells_option = page.get_by_role('menuitem').filter(has=page.get_by_text('Run All Cells')).first\n            await expect(run_all_cells_option).to_be_visible()\n            await run_all_cells_option.click()\n\n            na3_gui = selected_tab.locator('.box-na3-gui')\n            await expect(na3_gui).to_be_visible()\n\n            yield na3_gui\n\n\nvisible = os.environ.get('CI') != 'true'\n\nasync with provide_na3_gui(visible=visible) as gui:\n    general_settings = gui.locator('.box-general-settings')\n    assert not await general_settings.get_by_role(\"button\", name=\"Select\").is_disabled()\n    assert await general_settings.get_by_role(\"button\", name=\"Load Data\").is_disabled()\n\n    analysis_settings = gui.locator('.box-analysis-settings')\n\n    buttons = await analysis_settings.locator(\"button\").all()\n    assert buttons\n    for button in buttons:\n        assert await button.is_disabled()\n\n    inputs = await analysis_settings.locator(\"input\").all()\n    assert inputs\n    for input in inputs:\n        assert await input.is_disabled()",
    "crumbs": [
      "API",
      "controller"
    ]
  },
  {
    "objectID": "08_api.html",
    "href": "08_api.html",
    "title": "neuralactivitycubic",
    "section": "",
    "text": "source\n\nrun_analysis\n\n run_analysis (config:neuralactivitycubic.datamodels.Config|str)\n\n*Run analysis.\nArgs: config (Config | str): Configuration for analysis or data source path to run analysis with default settings.*"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "analysis",
    "section": "",
    "text": "source\n\nBaselineEstimatorFactory\n\n BaselineEstimatorFactory ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nPeak\n\n Peak (frame_idx:int, intensity:float, amplitude:Optional[float]=None,\n       delta_f_over_f:Optional[float]=None,\n       has_neighboring_intersections:Optional[bool]=None, frame_idxs_of_ne\n       ighboring_intersections:Optional[Tuple[int,int]]=None,\n       area_under_curve:Optional[float]=None,\n       peak_type:Optional[str]=None)\n\n\nsource\n\n\nAnalysisROI\n\n AnalysisROI (roi:neuralactivitycubic.input.ROI,\n              row_col_offset:Tuple[int,int], zstack:numpy.ndarray)\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API",
      "analysis"
    ]
  },
  {
    "objectID": "processing.html",
    "href": "processing.html",
    "title": "processing",
    "section": "",
    "text": "source\n\nprocess_analysis_rois\n\n process_analysis_rois\n                        (analysis_roi:neuralactivitycubic.analysis.Analysi\n                        sROI, configs:Dict[str,Any])\n\n\nsource\n\n\nAnalysisJob\n\n AnalysisJob (number_of_parallel_processes:int, data_loaders:Dict[str,Unio\n              n[neuralactivitycubic.input.DataLoader,List[neuralactivitycu\n              bic.input.DataLoader]]])\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API",
      "processing"
    ]
  },
  {
    "objectID": "view.html",
    "href": "view.html",
    "title": "view",
    "section": "",
    "text": "source\n\nchange_widget_state\n\n change_widget_state (widget:ipywidgets.widgets.widget.Widget,\n                      value:Optional[Any]=None,\n                      description:Optional[str]=None,\n                      disabled:Optional[bool]=None,\n                      visibility:Optional[str]=None,\n                      tooltip:Optional[str]=None,\n                      button_style:Optional[str]=None)\n\n\nsource\n\n\nUserInfoPanel\n\n UserInfoPanel ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSourceDataPanel\n\n SourceDataPanel (user_info_panel:__main__.UserInfoPanel)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nAnalysisSettingsPanel\n\n AnalysisSettingsPanel ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nMainScreen\n\n MainScreen ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nWidgetsInterface\n\n WidgetsInterface ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nSourceDataStructureWidget\n\n SourceDataStructureWidget ()\n\nInitialize self. See help(type(self)) for accurate signature.",
    "crumbs": [
      "API",
      "view"
    ]
  }
]