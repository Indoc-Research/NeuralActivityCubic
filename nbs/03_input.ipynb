{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input\n",
    "\n",
    "> Defines the import for different source filetypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import imageio.v3 as iio\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from shapely import Polygon\n",
    "import roifile\n",
    "from skimage.draw import polygon\n",
    "from skimage.measure import find_contours\n",
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FocusAreaPathRestrictions:\n",
    "    \n",
    "    @property\n",
    "    def supported_dir_names(self) -> List[str]:\n",
    "        supported_dir_names =  ['focus_area', 'focus_areas', 'focus-area', 'focus-areas', 'focus area', 'focus areas',\n",
    "                                'Focus_Area', 'Focus_Areas', 'Focus-Area', 'Focus-Areas', 'Focus Area', 'Focus Areas']\n",
    "        return supported_dir_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data Handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Data(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def _parse_loaded_data(self, loaded_data: Any) -> None:\n",
    "        pass\n",
    "\n",
    "    def __init__(self, filepath: Path, loaded_data: Any) -> None:\n",
    "        self.filepath = filepath\n",
    "        self._parse_loaded_data(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Recording(Data):\n",
    "\n",
    "    def _parse_loaded_data(self, loaded_data: np.ndarray) -> None:\n",
    "        self.zstack = loaded_data\n",
    "        self.estimated_bit_depth = self._estimate_bit_depth()\n",
    "        self.preview = self._create_brightness_and_contrast_enhanced_preview()\n",
    "\n",
    "\n",
    "    def _estimate_bit_depth(self) -> int:\n",
    "        max_bit_value = self.zstack.max()\n",
    "        if max_bit_value <= 255:\n",
    "            estimated_bit_depth = 255\n",
    "        elif max_bit_value <= 4095:\n",
    "            estimated_bit_depth = 4095\n",
    "        elif max_bit_value <= 65535:\n",
    "            estimated_bit_depth = 65535\n",
    "        else:\n",
    "            raise ValueError(f'Max bit value in recording found to be {max_bit_value}, but NA3 currently only handles up to 16-bit recordings!')\n",
    "        return estimated_bit_depth\n",
    "\n",
    "\n",
    "    def _create_brightness_and_contrast_enhanced_preview(self, percentile_for_adjustment: int=1) -> np.ndarray:\n",
    "        raw_image = self.zstack[0, :, :, :].copy() # ensure that dimensions are the same as for \".get_single_frame_as_preview()\"\n",
    "        lower_percentile_bit_value = np.percentile(raw_image, percentile_for_adjustment)\n",
    "        upper_percentile_bit_value = np.percentile(raw_image, 100-percentile_for_adjustment)\n",
    "        contrast_adjustment_factor = self.estimated_bit_depth / (upper_percentile_bit_value - lower_percentile_bit_value)\n",
    "        brightness_adjustment_factor = -(contrast_adjustment_factor * lower_percentile_bit_value)\n",
    "        raw_image_clipped_at_percentile_bit_values = self._clip_image_at_bit_values(raw_image, lower_percentile_bit_value, upper_percentile_bit_value)\n",
    "        brightness_contrast_adjusted_image = contrast_adjustment_factor * raw_image_clipped_at_percentile_bit_values + brightness_adjustment_factor\n",
    "        return brightness_contrast_adjusted_image\n",
    "        \n",
    "\n",
    "    def _compute_contrast_and_brightness_adjustment_factors(self, raw_image: np.ndarray, percentile_for_adjustment: int=1) -> Tuple[float, float]:\n",
    "        lower_percentile_bit_value = np.percentile(raw_image, percentile_for_adjustment)\n",
    "        upper_percentile_bit_value = np.percentile(raw_image, 100-percentile_for_adjustment)\n",
    "        contrast_adjustment = self.estimated_bit_depth / (upper_percentile_bit_value - lower_percentile_bit_value)\n",
    "        brightness_adjustment = -(contrast_adjustment * lower_percentile_bit_value)\n",
    "        return contrast_adjustment, brightness_adjustment\n",
    "\n",
    "\n",
    "    def _clip_image_at_bit_values(self, raw_image: np.ndarray, min_bit_value: float, max_bit_value: float) -> np.ndarray:\n",
    "        raw_image[raw_image <= min_bit_value] = min_bit_value\n",
    "        raw_image[raw_image >= max_bit_value] = max_bit_value\n",
    "        return raw_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROI(Data):\n",
    "\n",
    "    def _parse_loaded_data(self, loaded_data: List[Tuple[int, int]]) -> None:\n",
    "        self.boundary_row_col_coords = loaded_data\n",
    "        self.as_polygon = self._convert_to_valid_polygon()\n",
    "\n",
    "\n",
    "    def _convert_to_valid_polygon(self) -> Polygon:\n",
    "        roi_as_polygon = Polygon(self.boundary_row_col_coords)\n",
    "        assert roi_as_polygon.is_valid, f'Something went wrong when trying to create a Polygon out of your ROI: {self.filepath}.'\n",
    "        return roi_as_polygon\n",
    "\n",
    "\n",
    "    def add_label_id(self, label_id: str) -> None:\n",
    "        assert type(label_id) == str, f'\"label_id\" must be a string. However, you passed {label_id} which is of type {type(label_id)}.'\n",
    "        setattr(self, 'label_id', label_id)\n",
    "\n",
    "\n",
    "    def create_nwb_compliant_pixel_mask(self):\n",
    "        boundary_coords_with_subpixel_precision = np.asarray(self.boundary_row_col_coords)\n",
    "        row_idxs, col_idxs = polygon(boundary_coords_with_subpixel_precision[:, 0], boundary_coords_with_subpixel_precision[:, 1])\n",
    "        weights = np.ones(row_idxs.shape[0], dtype='int64')\n",
    "        self.pixel_mask = np.stack((row_idxs, col_idxs, weights), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data Loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoader(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_and_parse_file_content(self) -> Union[Data, List[Data]]:\n",
    "        # This method will be called when the data should be loaded for analysis\n",
    "        pass\n",
    "\n",
    "    def __init__(self, filepath: Path) -> None:\n",
    "        self.filepath = filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GridWrapperROILoader(DataLoader):\n",
    "\n",
    "    def set_configs_for_grid_creation(self, image_width: int, image_height: int, window_size: int) -> None:\n",
    "        self.configs = {}\n",
    "        self._add_to_configs_and_create_as_attribute('image_width', image_width)\n",
    "        self._add_to_configs_and_create_as_attribute('image_height', image_height)\n",
    "        self._add_to_configs_and_create_as_attribute('window_size', window_size)\n",
    "\n",
    "\n",
    "    def _add_to_configs_and_create_as_attribute(self, attribute_name: str, value: Any) -> None:\n",
    "        self.configs[attribute_name] = value\n",
    "        setattr(self, attribute_name, self.configs[attribute_name])\n",
    "\n",
    "    \n",
    "    def load_and_parse_file_content(self) -> List[ROI]:\n",
    "        row_cropping_idx, col_cropping_idx = self._get_cropping_indices_to_adjust_for_window_size()\n",
    "        self._add_to_configs_and_create_as_attribute('row_cropping_idx', row_cropping_idx)\n",
    "        self._add_to_configs_and_create_as_attribute('col_cropping_idx', col_cropping_idx)        \n",
    "        grid_row_idxs, grid_col_idxs = self._get_row_col_idxs_of_grid()\n",
    "        grid_row_labels, grid_col_labels = self._get_row_col_labels_for_rois_in_grid()\n",
    "        self._add_to_configs_and_create_as_attribute('max_len_row_label_id', len(str(grid_row_labels[-1])))\n",
    "        self._add_to_configs_and_create_as_attribute('max_len_col_label_id', len(str(grid_col_labels[-1])))\n",
    "        all_rois = []\n",
    "        for row_idx, row_label in zip(grid_row_idxs, grid_row_labels):\n",
    "            for col_idx, col_label in zip(grid_col_idxs, grid_col_labels):\n",
    "                square_corner_row_col_coords = self._get_boundary_row_col_coords_single_square(row_idx, col_idx)\n",
    "                label_id = f'{row_label}/{col_label}'\n",
    "                square_roi = ROI(self.filepath, square_corner_row_col_coords)\n",
    "                square_roi.add_label_id(label_id)\n",
    "                all_rois.append(square_roi)\n",
    "        return all_rois     \n",
    "\n",
    "    \n",
    "    def _get_cropping_indices_to_adjust_for_window_size(self) -> Tuple[int, int]:\n",
    "        row_cropping_index = (self.image_height // self.window_size) * self.window_size\n",
    "        col_cropping_index = (self.image_width // self.window_size) * self.window_size\n",
    "        return row_cropping_index, col_cropping_index\n",
    "        \n",
    "\n",
    "    def _get_row_col_idxs_of_grid(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        grid_row_idxs = np.arange(start = 0, stop = self.row_cropping_idx, step = self.window_size)\n",
    "        grid_col_idxs = np.arange(start = 0, stop = self.col_cropping_idx, step = self.window_size)\n",
    "        return grid_row_idxs, grid_col_idxs\n",
    "\n",
    "\n",
    "    def _get_row_col_labels_for_rois_in_grid(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        grid_row_labels = np.arange(start = 1, stop = self.row_cropping_idx / self.window_size + 1, step = 1, dtype = 'int')\n",
    "        grid_col_labels = np.arange(start = 1, stop = self.col_cropping_idx / self.window_size + 1, step = 1, dtype = 'int')\n",
    "        return grid_row_labels, grid_col_labels\n",
    "                \n",
    "    \n",
    "    def _get_boundary_row_col_coords_single_square(self, upper_left_corner_row_idx: int, upper_left_corner_col_idx: int) -> List[Tuple[(int, int)]]:\n",
    "        upper_left_corner = (upper_left_corner_row_idx, upper_left_corner_col_idx)\n",
    "        upper_right_corner = (upper_left_corner_row_idx, upper_left_corner_col_idx + self.window_size)\n",
    "        lower_right_corner = (upper_left_corner_row_idx + self.window_size, upper_left_corner_col_idx + self.window_size)\n",
    "        lower_left_corner = (upper_left_corner_row_idx + self.window_size, upper_left_corner_col_idx)\n",
    "        return [upper_left_corner, lower_left_corner, lower_right_corner, upper_right_corner, upper_left_corner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RecordingLoader(DataLoader):\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_all_frames(self) -> np.ndarray: \n",
    "        # To be implemented in individual subclasses\n",
    "        # Shape of returned numpy array: [frames, rows, cols, color_channels]\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _load_all_frames(self) -> np.ndarray: \n",
    "        all_frames = self._get_all_frames()\n",
    "        all_frames = self._validate_shape_and_convert_to_grayscale_if_possible(all_frames)\n",
    "        return all_frames\n",
    "\n",
    "\n",
    "    def load_and_parse_file_content(self) -> Recording:\n",
    "        all_frames = self._load_all_frames()\n",
    "        recording = Recording(self.filepath, all_frames)\n",
    "        return recording\n",
    "\n",
    "\n",
    "    def _validate_shape_and_convert_to_grayscale_if_possible(self, zstack: np.ndarray) -> np.ndarray:\n",
    "        self._validate_correct_array_shape(zstack)\n",
    "        if zstack.shape[3] > 1:\n",
    "            if self._check_if_color_channels_are_redunant(zstack) == True:\n",
    "                zstack = self._convert_to_grayscale(zstack)\n",
    "        return zstack\n",
    "\n",
    "\n",
    "    def _validate_correct_array_shape(self, zstack: np.ndarray) -> None:\n",
    "        assert len(zstack.shape) == 4, ('The shape of the zstack numpy array is not correct. It should be a 4 dimensional array, like '\n",
    "                                        f'[frames, rows, cols, color channels]. However, the current shape is: {zstack.shape}.')\n",
    "        assert zstack.shape[3] in [1, 3], ('The color channels of the recording you attempted to load are incorrect. Currently, only single '\n",
    "                                           f'channel or RGB (i.e. 1 or 3 color channels) are supported. However, your data has: {zstack.shape[3]}.')\n",
    "        \n",
    "    \n",
    "    def _check_if_color_channels_are_redunant(self, zstack: np.ndarray) -> bool:\n",
    "        reference_channel_idx = 0\n",
    "        color_channels_are_equal = []\n",
    "        for idx_of_channel_to_compare in range(1, zstack.shape[3]):\n",
    "            if np.array_equal(zstack[:, :, :, reference_channel_idx], zstack[: , :, :, idx_of_channel_to_compare]) == True:\n",
    "                color_channels_are_equal.append(True)\n",
    "            else:\n",
    "                color_channels_are_equal.append(False)\n",
    "                break\n",
    "        return all(color_channels_are_equal)\n",
    "\n",
    "\n",
    "    def _convert_to_grayscale(self, zstack: np.ndarray) -> np.ndarray:\n",
    "        return zstack[:, :, :, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AVILoader(RecordingLoader):\n",
    "\n",
    "    def _get_all_frames(self) -> np.ndarray: \n",
    "        return iio.imread(self.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NWBRecordingLoader(RecordingLoader):\n",
    "\n",
    "    def _get_all_frames(self) -> np.ndarray:\n",
    "        with NWBHDF5IO(self.filepath, \"r\") as io:\n",
    "            nwbfile = io.read()\n",
    "            if 'OnePhotonSeries' in nwbfile.acquisition.keys():\n",
    "                all_frames = nwbfile.acquisition['OnePhotonSeries'].data[:]\n",
    "            elif 'TwoPhotonSeries' in nwbfile.acquisition.keys():\n",
    "                all_frames = nwbfile.acquisition['TwoPhotonSeries'].data[:]\n",
    "            else:\n",
    "                raise ValueError('The NWB file you try to load does not have a recording stored as \"OnePhotonSeries\" or \"TwoPhotonSeries\" under Acquisition.')\n",
    "        if len(all_frames.shape) < 4:\n",
    "            all_frames = all_frames[..., np.newaxis]        \n",
    "        return all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROILoader(DataLoader):\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_boundary_row_col_coords_for_all_rois_in_source_data(self) -> List[List[Tuple[int, int]]]: \n",
    "        # To be implemented in individual subclasses\n",
    "        # Return a list of Tuples, where each tuple represents one boundary point: (row_coord, col_coord)\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def load_and_parse_file_content(self) -> List[ROI]:\n",
    "        boundary_row_col_coords_for_all_rois = self._get_boundary_row_col_coords_for_all_rois_in_source_data()\n",
    "        all_rois = []\n",
    "        for boundary_row_col_coords_single_roi in boundary_row_col_coords_for_all_rois:\n",
    "            boundary_row_col_coords_single_roi = self._add_first_boundary_point_also_add_end_to_close_roi(boundary_row_col_coords_single_roi)\n",
    "            roi = ROI(self.filepath, boundary_row_col_coords_single_roi)\n",
    "            all_rois.append(roi)\n",
    "        return all_rois\n",
    "\n",
    "\n",
    "    def _add_first_boundary_point_also_add_end_to_close_roi(self, boundary_row_col_coords: List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n",
    "        first_boundary_point_coords = boundary_row_col_coords[0]\n",
    "        boundary_row_col_coords.append(first_boundary_point_coords)\n",
    "        return boundary_row_col_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ImageJROILoader(ROILoader):\n",
    "\n",
    "    \n",
    "    def _get_boundary_row_col_coords_for_all_rois_in_source_data(self) -> List[List[Tuple[int, int]]]:\n",
    "        roi_file_content = roifile.roiread(self.filepath)\n",
    "        if type(roi_file_content) == list:\n",
    "            all_rois = self._extract_boundary_row_col_coords_from_roi_set(roi_file_content)\n",
    "        else:\n",
    "            all_rois = [self._extract_boundary_row_col_coords_from_single_roi(roi_file_content)]\n",
    "        return all_rois\n",
    "\n",
    "\n",
    "    def _extract_boundary_row_col_coords_from_roi_set(self, all_imagej_rois: List[roifile.roifile.ImagejRoi]) -> List[List[Tuple[int, int]]]:\n",
    "        boundary_coords_all_rois = []\n",
    "        for imagej_roi in all_imagej_rois:\n",
    "            boundary_row_col_coords_single_roi = self._extract_boundary_row_col_coords_from_single_roi(imagej_roi)\n",
    "            boundary_coords_all_rois.append(boundary_row_col_coords_single_roi)\n",
    "        return boundary_coords_all_rois\n",
    "    \n",
    "        \n",
    "    def _extract_boundary_row_col_coords_from_single_roi(self, imagej_roi: roifile.roifile.ImagejRoi) -> List[Tuple[int, int]]:\n",
    "        row_coords = imagej_roi.coordinates()[:, 1]\n",
    "        col_coords = imagej_roi.coordinates()[:, 0]\n",
    "        boundary_row_col_coords = list(zip(row_coords, col_coords))\n",
    "        return boundary_row_col_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NWBROILoader(ROILoader):\n",
    "\n",
    "    def _get_boundary_row_col_coords_for_all_rois_in_source_data(self) -> List[List[Tuple[int, int]]]:\n",
    "        with NWBHDF5IO(self.filepath, \"r\") as io:\n",
    "            nwbfile = io.read()\n",
    "            recording_frame_shape = nwbfile.processing['ophys']['ImageSegmentation']['PlaneSegmentation'].reference_images[0].data.shape[1:3]\n",
    "            rois_df = nwbfile.processing['ophys']['ImageSegmentation']['PlaneSegmentation'].to_dataframe()\n",
    "        all_rois = []\n",
    "        for roi_id in rois_df.index.values:\n",
    "            pixel_mask = rois_df.at[roi_id, 'pixel_mask']\n",
    "            binary_mask = np.zeros(recording_frame_shape, dtype='uint8')\n",
    "            for x, y, _ in pixel_mask:\n",
    "                binary_mask[int(x), int(y)] = 1\n",
    "            contour_results = find_contours(binary_mask)\n",
    "            if len(contour_results) != 1:\n",
    "                raise ValueError(f'Expected to find exactly one contour, but found {len(contour_results)} for ROI ID')\n",
    "            boundary_row_col_coords = [(float(row), float(col)) for row, col in contour_results[0]]\n",
    "            all_rois.append(boundary_row_col_coords)\n",
    "        return all_rois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader Factories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataLoaderFactory(ABC):\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def supported_extensions_per_data_loader(self) -> Dict[DataLoader, List[str]]:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def all_supported_extensions(self) -> List[str]:\n",
    "        all_supported_extensions = []\n",
    "        for value in self.supported_extensions_per_data_loader.values():\n",
    "            all_supported_extensions += value\n",
    "        return all_supported_extensions\n",
    "\n",
    "    \n",
    "    def get_loader(self, filepath: Path) -> DataLoader:\n",
    "        self._assert_validity_of_filepath(filepath)\n",
    "        data_loader = self._get_loader_for_file_extension(filepath)\n",
    "        return data_loader\n",
    "        \n",
    "\n",
    "    def _assert_validity_of_filepath(self, filepath: Path) -> None:\n",
    "        assert isinstance(filepath, Path), f'filepath must be an instance of a pathlib.Path. However, you passed {filepath}, which is of type {type(filepath)}'\n",
    "        assert filepath.exists(), f'The filepath you provided ({filepath}) does not seem to exist!'\n",
    "        \n",
    "\n",
    "    def _get_loader_for_file_extension(self, filepath: Path) -> DataLoader:\n",
    "        matching_loader = None\n",
    "        for loader_subclass, supported_extensions in self.supported_extensions_per_data_loader.items():\n",
    "            if filepath.suffix in supported_extensions:\n",
    "                matching_loader = loader_subclass(filepath)\n",
    "                break\n",
    "        if matching_loader == None:\n",
    "            raise NotImplementedError('It seems like there is no DataLoader implemented for the specific filetype youÂ´re trying to load - sorry!')\n",
    "        return matching_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RecordingLoaderFactory(DataLoaderFactory):\n",
    "\n",
    "    @property\n",
    "    def supported_extensions_per_data_loader(self) -> Dict[RecordingLoader, List[str]]:\n",
    "        supported_extensions_per_data_loader = {\n",
    "            AVILoader: ['.avi'],\n",
    "            NWBRecordingLoader: ['.nwb']\n",
    "        }\n",
    "        return supported_extensions_per_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROILoaderFactory(DataLoaderFactory):\n",
    "\n",
    "    @property\n",
    "    def supported_extensions_per_data_loader(self) -> Dict[ROILoader, List[str]]:\n",
    "        supported_extensions_per_data_loader = {\n",
    "            ImageJROILoader: ['.roi', '.zip'],\n",
    "            NWBROILoader: ['.nwb']\n",
    "        }\n",
    "        return supported_extensions_per_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_filepaths_with_supported_extension_in_dirpath(dirpath: Path, all_supported_extensions: List[str], max_results: Optional[int]=None) -> List[Path]:\n",
    "    all_filepaths_with_supported_extension = []\n",
    "    for elem in dirpath.iterdir():\n",
    "        if elem.is_file() == True:\n",
    "            if elem.suffix in all_supported_extensions:\n",
    "                all_filepaths_with_supported_extension.append(elem)\n",
    "    if type(max_results) == int:\n",
    "        assert len(all_filepaths_with_supported_extension) <= max_results, (\n",
    "            f'There are more than {max_results} file(s) of supported type in {dirpath}, '\n",
    "            f'but only a maximum of {max_results} are allowed. Please remove at least '\n",
    "            f'{len(all_filepaths_with_supported_extension) - max_results} of the following' \n",
    "            f'files and try again: {all_filepaths_with_supported_extension}'\n",
    "        )\n",
    "    return all_filepaths_with_supported_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RecLoaderROILoaderCombinator:\n",
    "\n",
    "        \n",
    "    def __init__(self, dir_path: Path) -> None:\n",
    "        self.dir_path = dir_path\n",
    "\n",
    "    \n",
    "    def get_all_recording_and_roi_loader_combos(self) -> List[Tuple[RecordingLoader, ROILoader]]:\n",
    "        recording_loader = self._get_the_recording_loader()\n",
    "        all_roi_loaders = self._get_all_roi_loaders()\n",
    "        if len(all_roi_loaders) > 0:\n",
    "            rec_roi_loader_combos = [(recording_loader, roi_loader) for roi_loader in all_roi_loaders]\n",
    "        else:\n",
    "            rec_roi_loader_combos = [(recording_loader, None)]\n",
    "        return rec_roi_loader_combos\n",
    "    \n",
    "\n",
    "    def _get_the_recording_loader(self) -> RecordingLoader:\n",
    "        recording_loader_factory = RecordingLoaderFactory()\n",
    "        recording_filepath = get_filepaths_with_supported_extension_in_dirpath(self.dir_path, recording_loader_factory.all_supported_extensions, 1)[0]\n",
    "        recording_loader = recording_loader_factory.get_loader(recording_filepath)\n",
    "        return recording_loader\n",
    "\n",
    "\n",
    "    def _get_all_roi_loaders(self) -> List[ROILoader]:\n",
    "        roi_loader_factory = ROILoaderFactory()\n",
    "        all_roi_filepaths = get_filepaths_with_supported_extension_in_dirpath(self.dir_path, roi_loader_factory.all_supported_extensions)\n",
    "        all_roi_loaders = [roi_loader_factory.get_loader(filepath) for filepath in all_roi_filepaths]\n",
    "        return all_roi_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "def test_unsupported_file_extension(loader_factory: DataLoaderFactory, filepath: Path) -> bool:\n",
    "    try:\n",
    "        loader_factory.get_loader(filepath)\n",
    "    except NotImplementedError:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "def test_successful_recording_loading(filepath: Path) -> bool:\n",
    "    recording_loader_factory = RecordingLoaderFactory()\n",
    "    recording_loader = recording_loader_factory.get_loader(filepath)\n",
    "    return isinstance(recording_loader.load_and_parse_file_content(), Recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| hide\n",
    "\n",
    "def test_successful_roi_loading(filepath: Path) -> bool:\n",
    "    roi_loader_factory = ROILoaderFactory()\n",
    "    roi_loader = roi_loader_factory.get_loader(filepath)\n",
    "    parsed_rois = roi_loader.load_and_parse_file_content()\n",
    "    if type(parsed_rois) == list:\n",
    "        are_rois = [isinstance(roi, ROI) for roi in parsed_rois]\n",
    "        if all(are_rois):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification of filepaths that can be used for testing:\n",
    "\n",
    "### ImageJ ROI filepaths:\n",
    "valid_imagej_roi_filepath_1 = Path('../test_data/00/RoiSet_spiking.zip')\n",
    "valid_imagej_roi_filepath_2 = Path('../test_data/00/focus_area/focus_spiking.roi')\n",
    "### NWB ROI filepath:\n",
    "valid_nwb_roi_filepath = Path('../test_data/02/spiking_neuron.nwb')\n",
    "\n",
    "### AVI Recording filepath:\n",
    "valid_avi_recording_filepath = Path('../test_data/00/spiking_neuron.avi')\n",
    "### NWB Recording filepath:\n",
    "valid_nwb_recording_filepath = Path('../test_data/02/spiking_neuron.nwb')\n",
    "\n",
    "### A filepath of an unsupported file type:\n",
    "unsupported_filepath = Path('../test_data/00/example_test_results_for_spiking_neuron/activity_overview.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_loader_factory = ROILoaderFactory()\n",
    "\n",
    "# tests that the ROILoaderFactory returns the correct Loader subclass for the respective file types:\n",
    "assert isinstance(roi_loader_factory.get_loader(valid_imagej_roi_filepath_1), ImageJROILoader)\n",
    "assert isinstance(roi_loader_factory.get_loader(valid_imagej_roi_filepath_2), ImageJROILoader)\n",
    "assert isinstance(roi_loader_factory.get_loader(valid_nwb_roi_filepath), NWBROILoader)\n",
    "\n",
    "# tests that the ROILoaderFactory raises a NotImplementedError for an unsupported file type:\n",
    "assert test_unsupported_file_extension(roi_loader_factory, unsupported_filepath)\n",
    "\n",
    "# tests that the individual Loader subclass correctly loads and parses the file content into ROI instances:\n",
    "assert test_successful_roi_loading(valid_imagej_roi_filepath_1)\n",
    "assert test_successful_roi_loading(valid_imagej_roi_filepath_2)\n",
    "assert test_successful_roi_loading(valid_nwb_roi_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_loader_factory = RecordingLoaderFactory()\n",
    "\n",
    "# tests that the RecordingLoaderFactory returns the correct Loader subclass for the respective file types:\n",
    "assert isinstance(recording_loader_factory.get_loader(valid_avi_recording_filepath), AVILoader)\n",
    "assert isinstance(recording_loader_factory.get_loader(valid_nwb_recording_filepath), NWBRecordingLoader)\n",
    "\n",
    "# tests that the RecordingLoaderFactory raises a NotImplementedError for an unsupported file type:\n",
    "assert test_unsupported_file_extension(recording_loader_factory, unsupported_filepath)\n",
    "\n",
    "# tests that the individual Loader subclass correctly loads and parses the file content into Recording instances:\n",
    "assert test_successful_recording_loading(valid_avi_recording_filepath)\n",
    "assert test_successful_recording_loading(valid_nwb_recording_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "na3_nbdev",
   "language": "python",
   "name": "na3_nbdev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
