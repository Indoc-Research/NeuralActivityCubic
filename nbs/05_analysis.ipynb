{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis\n",
    "\n",
    "> Defines the scientific analyses that are executed on ROI-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from pybaselines import Baseline\n",
    "from collections import Counter\n",
    "from shapely import Polygon, get_coordinates\n",
    "from skimage.measure import grid_points_in_poly\n",
    "\n",
    "from typing import Optional, Tuple, Dict, List, Callable\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from neuralactivitycubic.input import ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaselineEstimatorFactory:\n",
    "        \n",
    "    @property\n",
    "    def supported_baseline_estimation_methods(self) -> Dict[str, Callable]:\n",
    "        supported_baseline_estimation_methods = {'asls': Baseline().asls,\n",
    "                                                 'fabc': Baseline().fabc,\n",
    "                                                 'psalsa': Baseline().psalsa,\n",
    "                                                 'std_distribution': Baseline().std_distribution}\n",
    "        return supported_baseline_estimation_methods\n",
    "\n",
    "    def get_baseline_estimation_callable(self, algorithm_acronym: str) -> Callable:\n",
    "        baseline_estimation_method = self.supported_baseline_estimation_methods[algorithm_acronym]\n",
    "        return baseline_estimation_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Peak:\n",
    "\n",
    "    frame_idx: int\n",
    "    intensity: float\n",
    "    amplitude: Optional[float]=None\n",
    "    delta_f_over_f: Optional[float]=None\n",
    "    has_neighboring_intersections: Optional[bool]=None\n",
    "    frame_idxs_of_neighboring_intersections: Optional[Tuple[int, int]]=None\n",
    "    area_under_curve: Optional[float]=None\n",
    "    peak_type: Optional[str]=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AnalysisROI:\n",
    "    \n",
    "    def __init__(self, roi: ROI, row_col_offset: Tuple[int, int], zstack: np.ndarray):\n",
    "        self.label_id = roi.label_id\n",
    "        self.as_polygon = roi.as_polygon\n",
    "        self.boundary_row_col_coords = roi.boundary_row_col_coords\n",
    "        self.row_offset = row_col_offset[0]\n",
    "        self.col_offset = row_col_offset[1]\n",
    "        self.zstack = zstack\n",
    "        self.centroid_row_col_coords = get_coordinates(self.as_polygon.centroid).astype('int')[0]\n",
    "        self.as_mask = self._convert_polygon_to_mask()\n",
    "        self.peaks_count = 0\n",
    "\n",
    "\n",
    "    def _convert_polygon_to_mask(self) -> np.ndarray:\n",
    "        roi_row_col_coords = [(row_idx - self.row_offset, col_idx - self.col_offset) for (row_idx, col_idx) in get_coordinates(self.as_polygon).astype('int')]\n",
    "        grid_shape = (self.zstack.shape[1], self.zstack.shape[2])\n",
    "        mask = grid_points_in_poly(grid_shape, roi_row_col_coords)\n",
    "        dimension_adjusted_mask = np.expand_dims(mask, axis = (0, 3))\n",
    "        return dimension_adjusted_mask\n",
    "\n",
    "    \n",
    "    def compute_mean_intensity_timeseries(self, limit_analysis_to_frame_interval: bool, start_frame_idx: int, end_frame_idx: int) -> None:\n",
    "        if limit_analysis_to_frame_interval == True:\n",
    "            self.mean_intensity_over_time = np.mean(self.zstack[start_frame_idx:end_frame_idx], axis = (1,2,3), where = self.as_mask)\n",
    "        else:\n",
    "            self.mean_intensity_over_time = np.mean(self.zstack, axis = (1,2,3), where = self.as_mask)\n",
    "\n",
    "\n",
    "    def detect_peaks(self, signal_to_noise_ratio: float, octaves_ridge_needs_to_spann: float, noise_window_size: int) -> None:\n",
    "        widths = np.logspace(np.log10(1), np.log10(self.mean_intensity_over_time.shape[0]), 100)\n",
    "        min_length = octaves_ridge_needs_to_spann / np.log2(widths[1] / widths[0])\n",
    "        n_padded_frames = int(np.median(widths)) + 1\n",
    "        signal_padded_with_reflection = np.pad(self.mean_intensity_over_time, n_padded_frames, 'reflect')\n",
    "        frame_idxs_of_peaks_in_padded_signal = signal.find_peaks_cwt(vector = signal_padded_with_reflection, \n",
    "                                                         wavelet = signal.ricker, \n",
    "                                                         widths = widths, \n",
    "                                                         min_length = min_length,\n",
    "                                                         max_distances = widths / 4, # default\n",
    "                                                         gap_thresh = 0.0,\n",
    "                                                         noise_perc = 5, # default: 10\n",
    "                                                         min_snr = signal_to_noise_ratio,\n",
    "                                                         window_size = noise_window_size\n",
    "                                                        )\n",
    "        frame_idxs_of_peaks_in_padded_signal = frame_idxs_of_peaks_in_padded_signal[((frame_idxs_of_peaks_in_padded_signal >= n_padded_frames) & \n",
    "                                                                                     (frame_idxs_of_peaks_in_padded_signal < self.mean_intensity_over_time.shape[0] + n_padded_frames))]\n",
    "        self.frame_idxs_of_peaks = frame_idxs_of_peaks_in_padded_signal - n_padded_frames\n",
    "        self.peaks = {}\n",
    "        for peak_frame_idx in self.frame_idxs_of_peaks:\n",
    "            self.peaks[peak_frame_idx] = Peak(frame_idx = peak_frame_idx, intensity = self.mean_intensity_over_time[peak_frame_idx]) \n",
    "        self.peaks_count = self.frame_idxs_of_peaks.shape[0]\n",
    "\n",
    "\n",
    "    def estimate_baseline(self, algorithm_acronym: str) -> None:\n",
    "        baseline_estimation_method = BaselineEstimatorFactory().get_baseline_estimation_callable(algorithm_acronym)\n",
    "        self.baseline = baseline_estimation_method(data = self.mean_intensity_over_time)[0]\n",
    "\n",
    "\n",
    "    def compute_area_under_curve(self) -> None:\n",
    "        self._get_unique_frame_idxs_of_intersections_between_signal_and_baseline()\n",
    "        self._add_information_about_neighboring_intersections_to_peaks()\n",
    "        area_under_curve_classification = {'peaks_with_auc': [], 'all_intersection_frame_idxs_pairs': []}\n",
    "        for peak_frame_idx, peak in self.peaks.items():\n",
    "            if peak.has_neighboring_intersections == True:\n",
    "                idx_before_peak, idx_after_peak = peak.frame_idxs_of_neighboring_intersections\n",
    "                peak.area_under_curve = np.trapz(self.mean_intensity_over_time[idx_before_peak:idx_after_peak + 1] - self.baseline[idx_before_peak:idx_after_peak + 1])\n",
    "                area_under_curve_classification['peaks_with_auc'].append(peak)\n",
    "                area_under_curve_classification['all_intersection_frame_idxs_pairs'].append(peak.frame_idxs_of_neighboring_intersections)\n",
    "        self._classify_area_under_curve_types(area_under_curve_classification)\n",
    "\n",
    "    \n",
    "    def _get_unique_frame_idxs_of_intersections_between_signal_and_baseline(self) -> None:\n",
    "        quick_estimate_of_intersection_frame_idxs = np.argwhere(np.diff(np.sign(self.mean_intensity_over_time - self.baseline))).flatten()\n",
    "        intersection_frame_idxs = np.asarray([self._improve_intersection_frame_idx_estimation_by_interpolation(idx) for idx in quick_estimate_of_intersection_frame_idxs])\n",
    "        self.unique_intersection_frame_idxs = np.unique(intersection_frame_idxs)\n",
    "\n",
    "\n",
    "    def _add_information_about_neighboring_intersections_to_peaks(self) -> None:\n",
    "        for peak_frame_idx, peak in self.peaks.items():\n",
    "            if (peak_frame_idx > self.unique_intersection_frame_idxs[0]) & (peak_frame_idx < self.unique_intersection_frame_idxs[-1]):\n",
    "                peak.has_neighboring_intersections = True\n",
    "                idx_pre_peak = self.unique_intersection_frame_idxs[self.unique_intersection_frame_idxs < peak_frame_idx][-1]\n",
    "                idx_post_peak = self.unique_intersection_frame_idxs[self.unique_intersection_frame_idxs > peak_frame_idx][0]\n",
    "                peak.frame_idxs_of_neighboring_intersections = (idx_pre_peak, idx_post_peak)\n",
    "            else:\n",
    "                peak.has_neighboring_intersections = False\n",
    "\n",
    "\n",
    "    def _improve_intersection_frame_idx_estimation_by_interpolation(self, idx_frame_0: int) -> int:\n",
    "        \"\"\"\n",
    "        Designed to resolve the bias of the quick estimation of intersection points, which will always \n",
    "        return the first index of two frames between which an intersection was determined. This is done \n",
    "        by interpolating the data (for both signal & baseline) to a sub-frame resolution between the \n",
    "        previously identified intersection frame index, and the following frame index - as the intersection \n",
    "        might actually happen closer to this following frame. If interpolation estimates the intersection \n",
    "        precisely in the middle between the two frames, the later frame is returned (0.5 is rounded up).\n",
    "        \"\"\"\n",
    "        idx_frame_1 = idx_frame_0 + 1\n",
    "        num_interpolated_steps = 7\n",
    "        # interpolate signal & baseline to sub-frame resolution:\n",
    "        interpolated_signal_intensities = np.linspace(self.mean_intensity_over_time[idx_frame_0], self.mean_intensity_over_time[idx_frame_1], num = num_interpolated_steps)\n",
    "        interpolated_baseline = np.linspace(self.baseline[idx_frame_0], self.baseline[idx_frame_1], num = num_interpolated_steps)\n",
    "        # identify whether frame_idx_0 or frame_idx_1 is closer to interpolated intersection point\n",
    "        signed_differences = np.sign(interpolated_signal_intensities - interpolated_baseline)\n",
    "        if 0 in signed_differences: #intersection exactly at one or multiple interpolated index\n",
    "            intersection_idx_in_interpolation = np.argwhere(signed_differences == 0).flatten()[0]\n",
    "        else: \n",
    "            results_for_intersection_idxs = np.argwhere(np.diff(signed_differences)).flatten()\n",
    "            assert results_for_intersection_idxs.size != 0, ('get_improved_intersection_idx_estimation_by_interpolation() expected an intersection between frames '\n",
    "                                                             f'{idx_frame_0} and {idx_frame_1} in the provided arrays, but none were found!')\n",
    "            intersection_idx_in_interpolation = results_for_intersection_idxs[0]\n",
    "        # Based on interpolation, select whether intersection is closer to frame_idx_0 or frame_idx_1\n",
    "        if intersection_idx_in_interpolation < np.median(np.arange(0, num_interpolated_steps, 1)):\n",
    "            interpolation_evaluated_intersection_frame_idx = idx_frame_0\n",
    "        else:\n",
    "            interpolation_evaluated_intersection_frame_idx = idx_frame_1\n",
    "        return interpolation_evaluated_intersection_frame_idx\n",
    "    \n",
    "\n",
    "    def _classify_area_under_curve_types(self, data_for_auc_classification: Dict[str, List]) -> None:\n",
    "        if len(data_for_auc_classification['all_intersection_frame_idxs_pairs']) != len(set(data_for_auc_classification['all_intersection_frame_idxs_pairs'])):\n",
    "            counter = Counter(data_for_auc_classification['all_intersection_frame_idxs_pairs'])\n",
    "            reoccuring_intersection_frame_idxs = [pair_of_intersection_frame_idxs for pair_of_intersection_frame_idxs, count in counter.items() if count > 1]\n",
    "        else:\n",
    "            reoccuring_intersection_frame_idxs = []\n",
    "        for peak in self.peaks.values():\n",
    "            if peak in data_for_auc_classification['peaks_with_auc']:\n",
    "                if peak.frame_idxs_of_neighboring_intersections in reoccuring_intersection_frame_idxs:\n",
    "                    peak.peak_type = 'clustered'\n",
    "                else:\n",
    "                    peak.peak_type = 'singular'\n",
    "            else:\n",
    "                peak.peak_type = 'isolated'\n",
    "\n",
    "    \n",
    "    def compute_amplitude_and_delta_f_over_f(self):\n",
    "        for peak in self.peaks.values():\n",
    "            peak.amplitude = self.mean_intensity_over_time[peak.frame_idx] - self.baseline[peak.frame_idx]\n",
    "            peak.delta_f_over_f = peak.amplitude / self.baseline[peak.frame_idx]\n",
    "\n",
    "\n",
    "    def compute_variance_area(self, variance_window_size: int) -> None:\n",
    "        mean_intensity_over_time_as_series = pd.Series(self.mean_intensity_over_time)\n",
    "        rolling_means = mean_intensity_over_time_as_series.rolling(window=variance_window_size, min_periods=1).mean().values\n",
    "        rolling_variances = mean_intensity_over_time_as_series.rolling(window=variance_window_size, min_periods=1).var().values\n",
    "        variance_area_upper_border = rolling_means[1:] + rolling_variances[1:]\n",
    "        variance_area_lower_border = rolling_means[1:] - rolling_variances[1:]\n",
    "        self.variance_area = np.trapz(variance_area_upper_border - variance_area_lower_border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "na3_nbdev",
   "language": "python",
   "name": "na3_nbdev"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
