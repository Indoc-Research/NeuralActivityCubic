{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> Defines data and processing structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Welcome to the `Model` module of neuralactivitycubic!\n",
    "\n",
    "If you're here, chances are you're interested in understanding or extending the `Model` class that sits at the heart of neuralactivitycubic. Whether you're a researcher, a contributor, or just curious, this notebook is for you!\n",
    "\n",
    "Before we dive into the code, letâ€™s take a moment to talk about how this piece fits into the big picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§  What is the `Model`?\n",
    "\n",
    "Think of the `Model` as the brain of neuralactivitycubic. Itâ€™s the hub that:\n",
    "\n",
    "- Manages and validates configuration settings\n",
    "\n",
    "- Creates analysis jobs based on user input or directory structures\n",
    "\n",
    "- Executes the core analysis pipeline\n",
    "\n",
    "- Communicates with the graphical user interface (GUI), if used\n",
    "\n",
    "- Saves logs, plots, and results for each analysis\n",
    "\n",
    "It can run standalone or be accessed via the GUI. It handles batch jobs, logs everything, and orchestrates the creation of structured outputs, including NWB files. Crucially, though, as the *brain* of neuralactivitycubic, it essentially sends the commands to the other, downstream modules, which in turn implement the logic that will be executed. Like, for instance, the `Model` leverages the `RecordingLoader` that is implemented in the input module for loading of your recording data when creating the analysis jobs, and so on. Thus, the `Model` also represents the highest level of abstraction in neuralactivitycubic and enables you to run a wholistic analysis with only a few commands.\n",
    "\n",
    "Hereâ€™s what a minimal programmatic workflow might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "\n",
    "from neuralactivitycubic.model import Model\n",
    "\n",
    "model = Model('path/to/my/recording.avi')\n",
    "model.create_analysis_jobs()\n",
    "model.run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom, thatÂ´s it! Your data is analyzed, and results are saved. We made this even more accessible for you, by the way, in the api module. If youÂ´re planning to use neuralactivitycubic programmatically, we highly recommend you check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© Literate Programming in Action\n",
    "\n",
    "Also, before we start with the actual source code of this module, we also added some quick notes for those of you that are new to the concept of *Literate Programming*, which we are using throughout neuralactivitycubic. Why? Our goal is to make the logic transparent and easy to understand - because research software deserves to be as readable as the papers that use it! Therefore, all notebooks of neuralactivitycubic interleave documentation, explanations, usage examples, and tests with the actual source code. We hope you like it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip title = \"New to literate programming?\" appearance=\"simple\" collapse=\"true\"}\n",
    "\n",
    "WeÂ´re using a literate programming framework called [nbdev](https://nbdev.fast.ai/) for the development, testing, documentation, and dissemination of `neuralactivitycubic`. We believe the concept of rich annotations and usage examples directly intermixed with the source code (read more about the concept of literate programming for instance [here](https://blog.esciencecenter.nl/literate-programming-in-science-1669094541a7)) holds great value and potential, especially in the context of research software, as it makes it easier for others to understand, (re-)use, and ideally even to adapt or contribute to the code. That being said, some things in here might look a bit confusing to you - conversely especially if you are an experienced developer used to \"regular\" source code. \n",
    "\n",
    "For instance, youÂ´ll regularly find the implementation of a class, like the `Model` here, interrupted by markdown text and maybe even some addtional code cells that serve as usage examples or even represent the implementation of tests, only for the subsequent class methods to continue in code cells below, literally patched to the class using the `@patch` decorator. Just be aware that this may not meet your expectations of what conventional source code typically looks like and maybe you can discover some valuable takeaway for you along the way, too!\n",
    "\n",
    "One final note before you head on, though: we are ourselves still experimenting a lot with this concept and how it can be used to best effect and are more than happy to engage in discussion or to hear your feedback on this topic. Feel free to drop us a message via GitHub!\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ› ï¸ Import all dependencies\n",
    "\n",
    "Everybody needs some help - and weÂ´re standing on the shoulders of some giants here. LetÂ´s start by importing all dependencies we need to make neuralactivitycubic work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "# External functional dependencies:\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from matplotlib.pyplot import show\n",
    "import ipywidgets as w\n",
    "import multiprocessing\n",
    "from fastcore.basics import patch\n",
    "import gc\n",
    "\n",
    "# External dependencies for type hints:\n",
    "from typing import Callable\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes._axes import Axes\n",
    "\n",
    "# Internal dependencies\n",
    "from neuralactivitycubic.datamodels import Config\n",
    "from neuralactivitycubic.processing import AnalysisJob\n",
    "from neuralactivitycubic.input import RecordingLoaderFactory, ROILoaderFactory, RecordingLoader, ROILoader, get_filepaths_with_supported_extension_in_dirpath, FocusAreaPathRestrictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“˜ Start of the *source code*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready? Grab a coffee and enjoy the tour through the module! Letâ€™s jump into the first class, the `Logger`:\n",
    "\n",
    "Before any jobs are created or analysis is run, we need a reliable way to record what happens and when - and thatÂ´s what the `Logger` does. It logs messages with precise UTC timestamps and can:\n",
    "\n",
    "* Store messages in memory for programmatic access\n",
    "\n",
    "* Print logs to the console (or GUI)\n",
    "\n",
    "* Save logs to disk in a simple text file\n",
    "\n",
    "* Clear logs when switching contexts or rerunning jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    \"\"\"\n",
    "    A simple logging utility class that captures log messages with UTC timestamps,\n",
    "    allows retrieval and clearing of logs, and supports saving them to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "    def add_new_log(self, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a new log message with a UTC timestamp prefix. \n",
    "        The timestamp is formatted as 'dd-mm-yy HH:MM:SS.ffffff (UTC)'.\n",
    "        \"\"\"\n",
    "        time_prefix_in_utc = datetime.now(timezone.utc).strftime('%d-%m-%y %H:%M:%S.%f')\n",
    "        self.logs.append(f'{time_prefix_in_utc} (UTC): {message}')\n",
    "        print(f'{time_prefix_in_utc} (UTC): {message}')\n",
    "\n",
    "    def get_logs(self) -> list[str]:\n",
    "        return self.logs\n",
    "\n",
    "    def clear_logs(self) -> None:\n",
    "        self.logs = []\n",
    "\n",
    "    def save_current_logs(self, save_dir: Path) -> None:\n",
    "        filepath = save_dir.joinpath('logs.txt')\n",
    "        with open(filepath , 'w+') as logs_file:\n",
    "            for log_message in self.logs:\n",
    "                logs_file.write(f'{log_message}\\n')\n",
    "        print(f'Logs saved to {filepath}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its usage is simple, but invaluable for debugging and traceability. HereÂ´s a short example of how to interact with the `Logger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "# Create a logger instance\n",
    "logger = Logger()\n",
    "\n",
    "# Simulate logging workflow\n",
    "logger.add_new_log('Starting simulated analysis...')\n",
    "logger.add_new_log('Loading data...')\n",
    "logger.add_new_log('Data successfully loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "# Access what has been logged so far:\n",
    "logger.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if youÂ´d like to save the logs, you can do so pretty easily as well - just be aware that it expects a `pathlib.Path` object that defines the **directory** in which the logs.txt file should be saved! Once all logs are saved, you might want to clean it up before starting your next analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "\n",
    "# Create a Path:\n",
    "destination_directory = Path('path/to/my/directory/of/choice')\n",
    "\n",
    "# Pass the Path to the Logger to save the logs.txt\n",
    "logger.save_current_logs(destination_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "# Clean up all previously logged messages and your off to a fresh start:\n",
    "logger.clear_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have some trust issues? You can actually check that the list of logged messages is actually empty by calling the `get_logs()` method again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exec_doc\n",
    "\n",
    "# Confirm the logs were actually cleaned:\n",
    "logger.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome â€” thatâ€™s exactly the behavior we expected! ðŸŽ¯\n",
    "\n",
    "Sure, this might be a simple example, but itâ€™s a perfect introduction to the idea of testing your code. If you know what your code *should* do â€” and maybe just as importantly, what it *shouldnâ€™t* do â€” you can write a test to check for that.\n",
    "\n",
    "Even better: those tests are just regular Python code. That means they can be executed automatically, for example, every time you make changes â€” helping you catch bugs early and ensure nothing breaks by accident.\n",
    "\n",
    "And since weâ€™re using the literate programming framework `nbdev`, we can write these tests right here, directly next to the source code they validate. Thatâ€™s not just tidy â€” itâ€™s powerful.\n",
    "\n",
    "So letâ€™s put that into practice and write a test for the `Logger` class that automates the check we just did manually: confirming that all log messages are properly cleared when calling `clear_logs()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip title = \"How does testing in nbdev work?\" appearance=\"simple\" collapse=\"true\"}\n",
    "\n",
    "The literate programming framework we are using here, called nbdev, essentially uses any `assert` statements you define in the notebook for testing (see for instance [here](https://nbdev.fast.ai/tutorials/best_practices.html#consider-turning-code-examples-into-tests-by-adding-assertions) in their official documentation on how they envision *literate* test implementation.\n",
    "\n",
    "To add some more background, an `assert` statement only passes if itÂ´s `True`, otherwise it raises an `AssertionError`. For instance: \n",
    "\n",
    "> assert 1 == 1\n",
    "\n",
    "would pass, while:\n",
    "\n",
    "> assert 1 == 2\n",
    "\n",
    "would raise an `AssertionError` and nbdevÂ´s automated testing pipelines would catch and raise that error, notifying you that something is wrong with your code.\n",
    "\n",
    "For this reason, if weÂ´re defining custom functions for the testing, we always need to make sure they return `True` when the check passes, such that the corresponding `assert` that calls the function also passes.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_log_clean_up():\n",
    "    logger = Logger()\n",
    "    logger.add_new_log('test')\n",
    "    logger.clear_logs()\n",
    "    logs_count = len(logger.get_logs())\n",
    "    return logs_count == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_log_clean_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that â€” weâ€™ve got automated tests running! ðŸŽ‰\n",
    "\n",
    "Every time we push a new version of the code to GitHub, this test (along with all the others weâ€™ve written) runs automatically. If something breaks or behaves unexpectedly, we find out right away and can fix it before it becomes a problem.\n",
    "\n",
    "::: {.callout-important}\n",
    "Think about how powerful that is: we donâ€™t have to wait for a sharp-eyed user to notice somethingâ€™s off, track us down, and explain the issue. We catch bugs early â€” long before they can silently affect someoneâ€™s work.\n",
    "\n",
    "In the context of research software, thatâ€™s not just convenient â€” itâ€™s critical. A bug that slips through unnoticed could end up influencing someoneâ€™s results, and possibly even get baked into a publication. Thatâ€™s a scenario none of us want.\n",
    "\n",
    "Testing is always important â€” but in science and research, the stakes are higher. Thatâ€™s why we take it seriously.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the `Model`, which sits right at the core of `neuralactivitycubic`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, \n",
    "                 config: Config | str\n",
    "                ) -> None:\n",
    "        self.num_processes = multiprocessing.cpu_count()\n",
    "        self.analysis_job_queue = []\n",
    "        self.result_directories = []\n",
    "        self.logs = Logger()\n",
    "        if isinstance(config, str):\n",
    "            config = Config(data_source_path=config)\n",
    "        self.config = config\n",
    "        self.nwb_metadata = None\n",
    "        self.gui_enabled = False\n",
    "        self.callback_view_update_infos = None\n",
    "        self.callback_view_show_output_screen = None\n",
    "        self.view_output = None\n",
    "        self.pixel_conversion = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This constructor sets up everything needed to use na3 in either programmatic, batch, or GUI mode.\n",
    "\n",
    "- The config parameter can either be a `Config` object or just a path string to the data source. If itâ€™s a string, the constructor wraps it in a `Config`.\n",
    "\n",
    "- A `Logger` is created to track what's happening and report status.\n",
    "\n",
    "- GUI-related hooks are initialized but left inactive until explicitly connected.\n",
    "\n",
    "This object can now create and run jobsâ€”but wait, whatâ€™s a job in this context? Thatâ€™s what weâ€™ll get into next ðŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `Model` is initialized, the first thing you do is call `create_analysis_jobs()`. This method determines what data should be analyzed and sets up jobs accordingly.\n",
    "\n",
    "It supports both single-file and batch-directory analysis and uses a set of private helpers to figure out how to match recordings, ROI masks, and focus area files.\n",
    "\n",
    "Letâ€™s take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "\n",
    "@patch\n",
    "def create_analysis_jobs(self: Model\n",
    "                        ) -> None:\n",
    "    self._ensure_data_from_previous_jobs_was_removed()\n",
    "    self.add_info_to_logs('Basic configurations for data import validated. Starting creation of analysis job(s)...', True)\n",
    "    if self.config.batch_mode:\n",
    "        all_subdir_paths_with_rec_file = self._get_all_subdir_paths_with_rec_file(self.config.data_source_path)\n",
    "        all_subdir_paths_with_rec_file.sort()\n",
    "        for idx, subdir_path in enumerate(all_subdir_paths_with_rec_file):\n",
    "            if self.config.results_filepath:\n",
    "                result_path = self.config.results_filepath / subdir_path.name\n",
    "            else:\n",
    "                result_path = subdir_path\n",
    "            self._create_analysis_jobs_for_single_rec(subdir_path, result_path)\n",
    "    else:\n",
    "        self._create_analysis_jobs_for_single_rec()\n",
    "    self.add_info_to_logs('All job creation(s) completed.', True, 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method acts like a scout: it ventures into the provided folder(s), searches for files that NAÂ³ knows how to handle, and sets up one or more analysis jobs based on what it finds.\n",
    "\n",
    "Whether it creates one job or many depends on the configuration youâ€™ve selected. The only setup that guarantees a single job is the following:\n",
    "\n",
    "```\n",
    "batch mode: OFF\n",
    "focus area: OFF\n",
    "ROI mode: Grid\n",
    "```\n",
    "\n",
    "For any other combination, the number of analysis jobs depends entirely on how your source data is structuredâ€”e.g. how many recordings are detected, how many ROI masks are found, or how many focus area regions exist.\n",
    "\n",
    "The actual logic for locating those files and assembling jobs doesnâ€™t live directly in `create_analysis_jobs()` â€” instead, itâ€™s delegated to a series of helper methods. These methods do the real legwork and are marked as private by convention, meaning theyâ€™re not intended to be called from outside the Model class. Each isolates a specific aspect of the analysis job creation, like locating recordings or mapping ROI files, making it easier to extend or debug individual steps.\n",
    "\n",
    "For your convenience, weâ€™ve bundled all of them together in the next code block so you can collapse them as a group, skim through them if you like, and then jump back into the higher-level functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def _ensure_data_from_previous_jobs_was_removed(self: Model\n",
    "                                               ) -> None:\n",
    "    self.add_info_to_logs('Loading of new source data. All previously created jobs & logs will be deleted.', True)\n",
    "    self.analysis_job_queue = []\n",
    "    self.logs.clear_logs()\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_all_subdir_paths_with_rec_file(self: Model, \n",
    "                                        top_level_dir_path: Path\n",
    "                                       ) -> list[Path]:\n",
    "    rec_loader_factory = RecordingLoaderFactory()\n",
    "    supported_extensions_for_recordings = rec_loader_factory.all_supported_extensions\n",
    "    all_subdir_paths_that_contain_a_supported_recording_file = []\n",
    "    for elem in top_level_dir_path.iterdir():\n",
    "        if not elem.name.startswith('.'):\n",
    "            if elem.is_dir():\n",
    "                supported_recording_filepaths = [elem_2 for elem_2 in elem.iterdir() if elem_2.suffix in supported_extensions_for_recordings]\n",
    "                if len(supported_recording_filepaths) > 0:\n",
    "                    all_subdir_paths_that_contain_a_supported_recording_file.append(elem)\n",
    "    return all_subdir_paths_that_contain_a_supported_recording_file\n",
    "\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_analysis_jobs_for_single_rec(self: Model, \n",
    "                                         recording_path: Path = None,\n",
    "                                         result_path: Path = None\n",
    "                                        ) -> None:\n",
    "    if recording_path is None:\n",
    "        if self.config.recording_filepath:\n",
    "            recording_path = self.config.recording_filepath\n",
    "        else:\n",
    "            recording_path = self.config.data_source_path\n",
    "    self.add_info_to_logs(f'Starting with Job creation(s) for {str(recording_path)}', True)\n",
    "    recording_loader = self._get_recording_loader(recording_path)\n",
    "\n",
    "\n",
    "    if self.config.roi_filepath:\n",
    "        roi_filepath = self.config.roi_filepath\n",
    "    else:\n",
    "        roi_filepath = recording_loader.filepath.parent\n",
    "\n",
    "    if self.config.roi_mode == 'file':\n",
    "        roi_loaders = self._get_all_roi_loaders(roi_filepath)\n",
    "    else:\n",
    "        roi_loaders = None\n",
    "\n",
    "    if self.config.focus_area_filepath:\n",
    "        focus_area_filepath = self.config.focus_area_filepath\n",
    "    else:\n",
    "        focus_area_filepath = recording_loader.filepath.parent\n",
    "\n",
    "    if result_path is None:\n",
    "        if self.config.results_filepath:\n",
    "            result_path = self.config.results_filepath\n",
    "        else:\n",
    "            result_path = recording_loader.filepath.parent\n",
    "\n",
    "    if self.config.focus_area_enabled:\n",
    "        focus_area_dir_path = self._get_focus_area_dir_path(focus_area_filepath)\n",
    "        if focus_area_dir_path is None:\n",
    "            analysis_job = self._create_single_analysis_job(recording_loader, roi_loaders, result_filepath=result_path)\n",
    "            self.analysis_job_queue.append(analysis_job)\n",
    "            self.add_info_to_logs(f'Successfully created a single job for {focus_area_filepath} at queue position: #{len(self.analysis_job_queue)}.', True)\n",
    "        else:\n",
    "            all_focus_area_loaders = self._get_all_roi_loaders(focus_area_dir_path)\n",
    "            assert len(all_focus_area_loaders) > 0, f'Focus Area analysis enabled, but no focus area ROIs could be found. Please revisit your source data and retry!'\n",
    "            for idx, focus_area_loader in enumerate(all_focus_area_loaders):\n",
    "                if result_path is None:\n",
    "                    if self.config.results_filepath:\n",
    "                        result_path = self.config.results_filepath / focus_area_loader.filepath.stem\n",
    "                analysis_job_with_focus_area = self._create_single_analysis_job(recording_loader, roi_loaders, focus_area_loader, result_filepath=result_path)\n",
    "                self.analysis_job_queue.append(analysis_job_with_focus_area)\n",
    "                job_creation_message = (f'Successfully created {idx + 1} out of {len(all_focus_area_loaders)} job(s) for {recording_path} '\n",
    "                                        f'at queue position: #{len(self.analysis_job_queue)}.')\n",
    "                self.add_info_to_logs(job_creation_message, True)\n",
    "    else:\n",
    "        analysis_job = self._create_single_analysis_job(recording_loader, roi_loaders, result_filepath=result_path)\n",
    "        self.analysis_job_queue.append(analysis_job)\n",
    "        self.add_info_to_logs(f'Successfully created a single job for {recording_path} at queue position: #{len(self.analysis_job_queue)}.', True)\n",
    "    self.add_info_to_logs(f'Finished Job creation(s) for {recording_path}!', True)\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_recording_loader(self: Model, \n",
    "                          source_path: Path\n",
    "                         ) -> RecordingLoader:\n",
    "    rec_loader_factory = RecordingLoaderFactory()\n",
    "    if source_path.is_dir():\n",
    "        self.add_info_to_logs(f'Looking for a valid recording file in {source_path}...', True)\n",
    "        valid_filepaths = get_filepaths_with_supported_extension_in_dirpath(source_path, rec_loader_factory.all_supported_extensions, 1)\n",
    "        if len(valid_filepaths) == 0:\n",
    "            self.add_info_to_logs(f'Could not find any recording files of supported type at {source_path}!', True)\n",
    "        elif len(valid_filepaths) >  1:\n",
    "            filepath = valid_filepaths[0]\n",
    "            too_many_files_message = (f'Found more than a single recording file of supported type at {source_path}, i.e.: {valid_filepaths}. '\n",
    "                                      f'However, only a single file was expected. NA3 continues with {filepath} and will ignore the other files.')\n",
    "            self.add_info_to_logs(too_many_files_message, True)\n",
    "        else:\n",
    "            filepath = valid_filepaths[0]\n",
    "            self.add_info_to_logs(f'Found recording file of supported type at: {filepath}.', True)\n",
    "    else:\n",
    "        filepath = source_path\n",
    "        self.add_info_to_logs(f'Found recording file of supported type at: {filepath}.', True)\n",
    "    recording_loader = rec_loader_factory.get_loader(filepath)\n",
    "    return recording_loader\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_all_roi_loaders(self: Model, \n",
    "                         data_source_path: Path\n",
    "                        ) -> list[ROILoader]:\n",
    "    assert data_source_path.is_dir(), f'You must provide a directory as source data when using ROI mode or enabling Focus Areas. Please revisit your input data and retry.'\n",
    "    roi_loader_factory = ROILoaderFactory()\n",
    "    all_filepaths_with_supported_filetype_extensions = get_filepaths_with_supported_extension_in_dirpath(data_source_path, roi_loader_factory.all_supported_extensions)\n",
    "    all_roi_loaders = [roi_loader_factory.get_loader(roi_filepath) for roi_filepath in all_filepaths_with_supported_filetype_extensions]\n",
    "    return all_roi_loaders\n",
    "\n",
    "\n",
    "@patch\n",
    "def _get_focus_area_dir_path(self: Model, \n",
    "                             source_path: Path\n",
    "                            ) -> Path:\n",
    "    focus_area_path_restrictions = FocusAreaPathRestrictions()\n",
    "    supported_dir_names = focus_area_path_restrictions.supported_dir_names\n",
    "    if source_path.is_dir():\n",
    "        source_dir_path = source_path\n",
    "    else:\n",
    "        source_dir_path = source_path.parent\n",
    "    dirs_with_valid_name = [elem for elem in source_dir_path.iterdir() if (elem.name in supported_dir_names) & (elem.is_dir() == True)]\n",
    "    if len(dirs_with_valid_name) == 0:\n",
    "        no_dir_found_message = (f'You enabled Focus Area but a correspondingly named directory could not be found in {source_dir_path}. '\n",
    "                                f'Please use one of the following for the name of the directory that contains the Focus Area ROIs: {supported_dir_names}. '\n",
    "                                'In absence of such a directory, analysis is continued without using the Focus Area mode for this data.')\n",
    "        self.add_info_to_logs(no_dir_found_message, True)\n",
    "        focus_area_dir_path = None\n",
    "    elif len(dirs_with_valid_name) > 1:\n",
    "        too_many_dirs = (f'More than a single Focus Area directory was found in the following parent directory: {source_dir_path}, i.e.: '\n",
    "                         f'{dirs_with_valid_name}. However, only the use of a single one that contains all your Focus Area ROIS is '\n",
    "                         f'currently supported. {dirs_with_valid_name[0]} will be used for this analysis, while the other(s): {dirs_with_valid_name[1:]} '\n",
    "                         'will be ignored to continue processing.')\n",
    "        self.add_info_to_logs(too_many_dirs, True)\n",
    "        focus_area_dir_path = dirs_with_valid_name[0]\n",
    "    else:\n",
    "        focus_area_dir_path = dirs_with_valid_name[0]\n",
    "    return focus_area_dir_path\n",
    "\n",
    "\n",
    "@patch\n",
    "def _create_single_analysis_job(self: Model,\n",
    "                                recording_loader: RecordingLoader,\n",
    "                                roi_loaders: list[ROILoader] | None,\n",
    "                                focus_area_loader: ROILoader = None,\n",
    "                                result_filepath: Path = None\n",
    "                               ) -> AnalysisJob:\n",
    "    data_loaders = {'recording': recording_loader}\n",
    "    if roi_loaders is not None:\n",
    "        data_loaders['rois'] = roi_loaders\n",
    "    if focus_area_loader is not None:\n",
    "        data_loaders['focus_area'] = focus_area_loader\n",
    "    return AnalysisJob(self.num_processes, data_loaders, result_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the analysis jobs have been created, itâ€™s time to actually process them! âœ¨\n",
    "\n",
    "To kick off the entire analysis pipeline, you simply call the other main method of the `Model`:\n",
    "\n",
    "> run_analysis()\n",
    "\n",
    "This method takes care of everything:\n",
    "\n",
    "* executing each analysis job in turn,\n",
    "\n",
    "* logging the progress and any issues along the way,\n",
    "\n",
    "* generating results and saving them to disk,\n",
    "\n",
    "* and even updating the GUI if you're running in interactive mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "\n",
    "@patch\n",
    "def run_analysis(self: Model\n",
    "                ) -> None:\n",
    "    self._display_configs()\n",
    "    self.add_info_to_logs('Starting analysis...', True)\n",
    "    for job_idx in range(len(self.analysis_job_queue)):\n",
    "        analysis_job = self.analysis_job_queue.pop(0)\n",
    "        self.add_info_to_logs(f'Starting to process analysis job with index #{job_idx}.')\n",
    "        analysis_job.run_analysis(self.config)\n",
    "        self.add_info_to_logs(f'Analysis successfully completed. Continue with creation of results.. ')\n",
    "        analysis_job.create_results(self.config, self.nwb_metadata)\n",
    "        self.add_info_to_logs(f'Results successfully created at: {analysis_job.results_dir_path}')\n",
    "        if self.gui_enabled:\n",
    "            self.callback_view_show_output_screen()\n",
    "            with self.view_output:\n",
    "                activity_overview_fig = analysis_job.activity_overview_plot[0]\n",
    "                activity_overview_fig.set_figheight(400 * self.pixel_conversion)\n",
    "                activity_overview_fig.tight_layout()\n",
    "                show(activity_overview_fig)\n",
    "        self._save_user_settings_as_json(analysis_job)\n",
    "        self.result_directories.append(analysis_job.results_dir_path)\n",
    "        self.add_info_to_logs('Updating all log files to contain all logs as final step. All valid logs files will end with this message.')\n",
    "        self.logs.save_current_logs(analysis_job.results_dir_path)\n",
    "    else:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "Analysis jobs are currently processed sequentiallyâ€”one after another. However, each job itself is internally parallelized: the ROI traces are processed using all available CPU cores.\n",
    "\n",
    "That means thereâ€™s definitely room for future optimization at the job level. So if youâ€™re feeling adventurous and want to dive into parallel job execution... weâ€™d love to see your pull request! ðŸ”§ðŸš€\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The private helper methods that `run_analysis()` uses are significantly less complex. As before, theyâ€™re grouped below for convenience to enable optional collapsing. ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def _display_configs(self: Model\n",
    "                    ) -> None:\n",
    "    self.add_info_to_logs('Configurations for Analysis Settings and Result Creation validated successfully.', True)\n",
    "    self.add_info_to_logs(f'Analysis Settings are:')\n",
    "    for line in self.config.display_all_attributes():\n",
    "        self.add_info_to_logs(line)\n",
    "\n",
    "\n",
    "@patch\n",
    "def _save_user_settings_as_json(self: Model, \n",
    "                                analysis_job: AnalysisJob\n",
    "                               ) -> None:\n",
    "    filepath = analysis_job.results_dir_path.joinpath('user_settings.json')\n",
    "    self.config.recording_filepath = analysis_job.recording.filepath\n",
    "    if analysis_job.focus_area_enabled:\n",
    "        self.config.focus_area_filepath = analysis_job.focus_area.filepath\n",
    "    else:\n",
    "        self.config.focus_area_filepath = None\n",
    "    if analysis_job.rois_source == 'file':\n",
    "        self.config.roi_filepath = [roi.filepath for roi in analysis_job.all_rois]\n",
    "    with open(filepath, 'w+') as user_settings_json:\n",
    "        user_settings_json.write(self.config.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model` is also designed to cooperate closely with na3's graphical user interface (GUI) â€” without ever depending on it directly. This is thanks to a clear separation of concerns, following the Model-View-Controller (MVC) design pattern.\n",
    "\n",
    "The idea is simple:\n",
    "\n",
    "* The `Model` knows how to run analyses\n",
    "\n",
    "* The `View` handles the display and user interaction\n",
    "\n",
    "* The controller (in this case, the `App` class) wires the two together\n",
    "\n",
    "To support this structure, the Model exposes two public methods that let the controller hook up GUI elements like progress messages and result previews. Letâ€™s look at them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def setup_connection_to_update_infos_in_view(self: Model, \n",
    "                                             update_infos: Callable\n",
    "                                            ) -> None:\n",
    "    \"\"\" Allows to configure the widget in the GUI that is used to display the logs. \"\"\"\n",
    "    self.callback_view_update_infos = update_infos\n",
    "    self.gui_enabled = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method registers a callback that will be used to send updates (logs or progress percentages) from the model to the GUI's log display widget. Once this is connected, log messages generated during analysis can be sent live to the GUIâ€™s log panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def setup_connection_to_display_results(self: Model, \n",
    "                                        show_output_screen: Callable, \n",
    "                                        output: w.Output, \n",
    "                                        pixel_conversion: float\n",
    "                                       ) -> None:\n",
    "    \"\"\" Allows to configure the widget in the GUI that is used to display images, plots, and figures. \"\"\"\n",
    "    self.callback_view_show_output_screen = show_output_screen\n",
    "    self.view_output = output\n",
    "    self.pixel_conversion = pixel_conversion\n",
    "    self.gui_enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second method connects the model to GUI elements responsible for showing figures and images â€” specifically the results of an analysis job. Once wired up, na3 can render the activity overview plots or other visualizations directly into the notebook interface. This makes for a much smoother and more intuitive analysis experience â€” especially for exploratory workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next method was also designed to smoothen your experience with na3Â´s GUI while trying to figure out which grid size you should use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def preview_window_size(self: Model, \n",
    "                        grid_size\n",
    "                       ) -> tuple[Figure, Axes]:\n",
    "    job_for_preview = self.analysis_job_queue[0]\n",
    "    preview_fig, preview_ax = job_for_preview.preview_window_size(grid_size)\n",
    "    return preview_fig, preview_ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method shows you how the selected grid_size will divide your image into analysis windows. It uses the first job in the queue (since you haven't run any yet) and overlays the grid onto the corresponding image. It integrates directly with the GUI output, so users can click a button and immediately see how their current settings affect the analysis layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the very beginning of this notebook, we introduced the humble `Logger` â€” a simple but powerful tool for capturing status updates throughout the analysis pipeline.\n",
    "\n",
    "Well, guess what: weâ€™re coming full circle now. The `Model` uses its own method, `add_info_to_logs()`, to centralize how messages are handled â€” whether you're running headless, in a notebook, or in the GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "@patch\n",
    "def add_info_to_logs(self: Model, \n",
    "                     message: str, \n",
    "                     display_in_gui: bool = False, \n",
    "                     progress_in_percent: float | None = None\n",
    "                    ) -> None:\n",
    "    self.logs.add_new_log(message)\n",
    "    if (display_in_gui == True) and (self.gui_enabled == True): \n",
    "        self.callback_view_update_infos(message, progress_in_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method serves as a bridge between the raw logging system and the user interface. It logs the message internally, prints it to the console, and (if GUI mode is active) pushes the update to the GUI via the connected callback.\n",
    "\n",
    "Itâ€™s small, but it plays a central role in making sure everything you do with NAÂ³ is traceable, debuggable, and visible â€” no matter how you interact with the tool.\n",
    "\n",
    "So if you're seeing beautifully time-stamped updates in the GUI while a job runs, now you know who's responsible! ðŸ™Œ\n",
    "\n",
    "And that concludes our tour through the source code of the `Model` â€” the central brain of na3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Testing, testing, testing .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We emphasized above already how important testing of your research software code is. Therefore, we also have some more tests implemented in here that test some of the `Model`s behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "import os\n",
    "from re import compile\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "from neuralactivitycubic.view import WidgetsInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath = Path('../test_data/00')\n",
    "\n",
    "test00_filepath = Path('../test_data/00')\n",
    "test01_filepath = Path('../test_data/01')\n",
    "parent_test_filepath = Path('../test_data')\n",
    "example_results_dir = Path('../test_data/00/example_test_results_for_spiking_neuron')\n",
    "results_filepath = Path('../test_data/00/results_directory')\n",
    "# we have to split results by own directories due to concurrency issues\n",
    "results_case01_filepath = Path('../test_data/results/case_01/')\n",
    "results_case02_filepath = Path('../test_data/results/case_02/')\n",
    "results_case03_filepath = Path('../test_data/results/case_03/')\n",
    "results_case04_filepath = Path('../test_data/results/case_04/')\n",
    "results_case05_filepath = Path('../test_data/results/case_05/')\n",
    "results_case06_filepath = Path('../test_data/results/case_06/')\n",
    "\n",
    "def test_correct_model_run():\n",
    "    correct_config = WidgetsInterface().export_user_settings()\n",
    "    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n",
    "    correct_config.save_single_trace_results = True\n",
    "    model = Model(correct_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "    return model.result_directories\n",
    "\n",
    "def test_correct_model_run_with_custom_results_dir():\n",
    "    correct_config = WidgetsInterface().export_user_settings()\n",
    "    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n",
    "    correct_config.results_filepath = results_filepath\n",
    "    model = Model(correct_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "    return correct_config.results_filepath\n",
    "\n",
    "# tests with different modes of running analysis\n",
    "\n",
    "def test_run_grid_focus_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus area enabled.\n",
    "    \"\"\"\n",
    "    grid_focus_mode_config = WidgetsInterface().export_user_settings()\n",
    "    grid_focus_mode_config.data_source_path = test01_filepath\n",
    "    grid_focus_mode_config.focus_area_enabled = True\n",
    "    grid_focus_mode_config.results_filepath = results_case01_filepath\n",
    "    model = Model(grid_focus_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return grid_focus_mode_config.results_filepath\n",
    "\n",
    "def test_run_file_focus_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus area enabled.\n",
    "    \"\"\"\n",
    "    file_focus_mode_config = WidgetsInterface().export_user_settings()\n",
    "    file_focus_mode_config.data_source_path = test01_filepath\n",
    "    file_focus_mode_config.focus_area_enabled = True\n",
    "    file_focus_mode_config.roi_mode = 'file'\n",
    "    file_focus_mode_config.results_filepath = results_case01_filepath\n",
    "    model = Model(file_focus_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return file_focus_mode_config.results_filepath\n",
    "\n",
    "def test_run_grid_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with batch mode enabled.\n",
    "    \"\"\"\n",
    "    grid_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    grid_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    grid_batch_mode_config.batch_mode = True\n",
    "    grid_batch_mode_config.results_filepath = results_case02_filepath\n",
    "    model = Model(grid_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return grid_batch_mode_config.results_filepath\n",
    "\n",
    "def test_run_file_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with batch mode enabled.\n",
    "    \"\"\"\n",
    "    file_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    file_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    file_batch_mode_config.batch_mode = True\n",
    "    file_batch_mode_config.roi_mode = 'file'\n",
    "    file_batch_mode_config.results_filepath = results_case02_filepath\n",
    "    model = Model(file_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return file_batch_mode_config.results_filepath\n",
    "\n",
    "def test_run_grid_focus_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus and batch mode enabled.\n",
    "    \"\"\"\n",
    "    grid_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    grid_focus_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    grid_focus_batch_mode_config.batch_mode = True\n",
    "    grid_focus_batch_mode_config.focus_area_enabled = True\n",
    "    grid_focus_batch_mode_config.results_filepath = results_case03_filepath\n",
    "    model = Model(grid_focus_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return grid_focus_batch_mode_config.results_filepath\n",
    "\n",
    "def test_run_file_focus_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus and batch mode enabled.\n",
    "    \"\"\"\n",
    "    file_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    file_focus_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    file_focus_batch_mode_config.batch_mode = True\n",
    "    file_focus_batch_mode_config.focus_area_enabled = True\n",
    "    file_focus_batch_mode_config.roi_mode = 'file'\n",
    "    file_focus_batch_mode_config.results_filepath = results_case03_filepath\n",
    "    model = Model(file_focus_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return file_focus_batch_mode_config.results_filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_csv_files(relative_filepath_to_csv: str, results_dir: Path) -> bool:\n",
    "    filepath = results_dir / relative_filepath_to_csv\n",
    "    # confirm results have been created:\n",
    "    if not filepath.is_file():\n",
    "        return False\n",
    "    # confirm computational consistency of results, while allowing minor numerical tolerance\n",
    "    df_test = pd.read_csv(filepath)\n",
    "    df_validation = pd.read_csv(example_results_dir / relative_filepath_to_csv)\n",
    "    if assert_frame_equal(df_test, df_validation) is not None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def test_all_peak_results(results_dir):\n",
    "    return _test_csv_files('all_peak_results.csv', results_dir)\n",
    "\n",
    "def test_amplitude_and_df_over_f_results(results_dir):\n",
    "    return _test_csv_files('Amplitude_and_dF_over_F_results.csv', results_dir)\n",
    "\n",
    "def test_auc_results(results_dir):\n",
    "    return _test_csv_files('AUC_results.csv', results_dir)\n",
    "\n",
    "def test_variance_area_results(results_dir):\n",
    "    return _test_csv_files('Variance_area_results.csv', results_dir)\n",
    "\n",
    "def test_representative_single_trace_results(results_dir):\n",
    "    return _test_csv_files('single_traces/data_of_ROI_7-10.csv', results_dir)\n",
    "\n",
    "def test_activity_overview_png(results_dir):\n",
    "    filepath = results_dir / 'activity_overview.png'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_roi_label_ids_overview_png(results_dir):\n",
    "    filepath = results_dir / 'ROI_label_IDs_overview.png'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_individual_traces_with_identified_events_pdf(results_dir):\n",
    "    filepath = results_dir / 'Individual_traces_with_identified_events.pdf'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_logs_txt(results_dir):\n",
    "    filepath = results_dir / 'logs.txt'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_user_settings_json(results_dir):\n",
    "    filepath = results_dir / 'user_settings.json'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_nwb_export(results_dir):\n",
    "    filepath = results_dir / 'autogenerated_nwb_file.nwb'\n",
    "\n",
    "def test_all_correct_files_created(results_dir: Path, test_csv: bool = True, single_trace: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Check if all expected files have been created in the results directory.\n",
    "    \"\"\"\n",
    "    # confirm all csv files have been created and are correct:\n",
    "    if test_csv:\n",
    "        assert test_all_peak_results(results_dir), 'There is an issue with the \"all_peak_results.csv\" file!'\n",
    "        assert test_amplitude_and_df_over_f_results(results_dir)\n",
    "        assert test_auc_results(results_dir)\n",
    "        assert test_variance_area_results(results_dir)\n",
    "        if single_trace:\n",
    "            assert test_representative_single_trace_results(results_dir)\n",
    "\n",
    "    # confirm all other result files have been created:\n",
    "    assert test_activity_overview_png(results_dir)\n",
    "    assert test_roi_label_ids_overview_png(results_dir)\n",
    "    assert test_logs_txt(results_dir)\n",
    "    assert test_user_settings_json(results_dir)\n",
    "    if single_trace:\n",
    "        assert test_individual_traces_with_identified_events_pdf(results_dir)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def find_directories_after_test(base_path):\n",
    "    \"\"\"\n",
    "    Find directories after test with timestamps.\n",
    "    \"\"\"\n",
    "    pattern = compile(r'\\d{4}_\\d{2}_\\d{2}_\\d{2}-\\d{2}-\\d{2}_.+')\n",
    "\n",
    "    matching_dirs = [\n",
    "        str(base_path) + d for d in os.listdir(base_path)\n",
    "        if os.path.isdir(os.path.join(base_path, d)) and pattern.fullmatch(d)\n",
    "    ]\n",
    "\n",
    "    return matching_dirs\n",
    "\n",
    "def delete_directories_after_test(paths_list):\n",
    "    \"\"\"\n",
    "    Delete directories after test.\n",
    "    \"\"\"\n",
    "    for res_dir in find_directories_after_test(paths_list):\n",
    "        rmtree(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed:\n",
    "result_directories = test_correct_model_run()\n",
    "\n",
    "for directory in result_directories:\n",
    "    assert directory.exists()\n",
    "    assert test_all_correct_files_created(directory)\n",
    "    # cleanup\n",
    "    rmtree(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with custom result directory:\n",
    "results_directory = test_correct_model_run_with_custom_results_dir()\n",
    "\n",
    "assert results_directory.exists()\n",
    "# only one directory with analysis files should be created, as there are only one focus area:\n",
    "assert len(list(results_directory.iterdir())) == 1\n",
    "\n",
    "for directory in results_directory.iterdir():\n",
    "    assert test_all_correct_files_created(directory)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with focus mode:\n",
    "results_directory = test_run_grid_focus_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "# only one directory should be created, as there are only one focus area:\n",
    "assert len(list(results_directory.iterdir())) == 2\n",
    "for directory in results_directory.iterdir():\n",
    "    assert test_all_correct_files_created(directory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch mode:\n",
    "results_directory = test_run_grid_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch and focus mode:\n",
    "results_directory = test_run_grid_focus_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    # test case with multiple focus areas\n",
    "    if '01' in directory.name:\n",
    "        assert len(list(directory.iterdir())) == 2\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with focus mode:\n",
    "results_directory = test_run_file_focus_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "# only one directory should be created, as there are only one focus area:\n",
    "assert len(list(results_directory.iterdir())) == 2\n",
    "for directory in results_directory.iterdir():\n",
    "    assert test_all_correct_files_created(directory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch mode:\n",
    "results_directory = test_run_file_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch and focus mode:\n",
    "results_directory = test_run_file_focus_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    # test case with multiple focus areas\n",
    "    if '01' in directory.name:\n",
    "        assert len(list(directory.iterdir())) == 2\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
