{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "\n",
    "> Defines data and processing structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from matplotlib.pyplot import show\n",
    "import ipywidgets as w\n",
    "import multiprocessing\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Callable\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes._axes import Axes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from neuralactivitycubic.datamodels import Config\n",
    "from neuralactivitycubic.processing import AnalysisJob\n",
    "from neuralactivitycubic.input import RecordingLoaderFactory, ROILoaderFactory, RecordingLoader, ROILoader, get_filepaths_with_supported_extension_in_dirpath, FocusAreaPathRestrictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.logs = []\n",
    "\n",
    "    def add_new_log(self, message: str) -> None:\n",
    "        time_prefix_in_utc = datetime.now(timezone.utc).strftime('%d-%m-%y %H:%M:%S.%f')\n",
    "        self.logs.append(f'{time_prefix_in_utc} (UTC): {message}')\n",
    "        print(f'{time_prefix_in_utc} (UTC): {message}')\n",
    "\n",
    "    def get_logs(self) -> list[str]:\n",
    "        return self.logs\n",
    "\n",
    "    def clear_logs(self) -> None:\n",
    "        self.logs = []\n",
    "\n",
    "    def save_current_logs(self, save_dir: Path) -> None:\n",
    "        filepath = save_dir.joinpath('logs.txt')\n",
    "        with open(filepath , 'w+') as logs_file:\n",
    "            for log_message in self.logs:\n",
    "                logs_file.write(f'{log_message}\\n')\n",
    "        print(f'Logs saved to {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, config: Config | str) -> None:\n",
    "        self.num_processes = multiprocessing.cpu_count()\n",
    "        self.analysis_job_queue = []\n",
    "        self.result_directories = []\n",
    "        self.logs = Logger()\n",
    "        if isinstance(config, str):\n",
    "            config = Config(data_source_path=config)\n",
    "        self.config = config\n",
    "        self.nwb_metadata = None\n",
    "        self.gui_enabled = False\n",
    "        self.callback_view_update_infos = None\n",
    "        self.callback_view_show_output_screen = None\n",
    "        self.view_output = None\n",
    "        self.pixel_conversion = None\n",
    "\n",
    "\n",
    "    def setup_connection_to_update_infos_in_view(self, update_infos: Callable) -> None:\n",
    "        self.callback_view_update_infos = update_infos\n",
    "        self.gui_enabled = True\n",
    "\n",
    "\n",
    "    def setup_connection_to_display_results(self, show_output_screen: Callable, output: w.Output, pixel_conversion: float) -> None:\n",
    "        self.callback_view_show_output_screen = show_output_screen\n",
    "        self.view_output = output\n",
    "        self.pixel_conversion = pixel_conversion\n",
    "        self.gui_enabled = True\n",
    "\n",
    "\n",
    "    def add_info_to_logs(self, message: str, display_in_gui: bool = False, progress_in_percent: float | None = None) -> None:\n",
    "        self.logs.add_new_log(message)\n",
    "        if (display_in_gui == True) and (self.gui_enabled == True): \n",
    "            self.callback_view_update_infos(message, progress_in_percent)\n",
    "\n",
    "\n",
    "    def _ensure_data_from_previous_jobs_was_removed(self) -> None:\n",
    "        self.add_info_to_logs('Loading of new source data. All previously created jobs & logs will be deleted.', True)\n",
    "        self.analysis_job_queue = []\n",
    "        self.logs.clear_logs()\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_all_subdir_paths_with_rec_file(top_level_dir_path: Path) -> list[Path]:\n",
    "        rec_loader_factory = RecordingLoaderFactory()\n",
    "        supported_extensions_for_recordings = rec_loader_factory.all_supported_extensions\n",
    "        all_subdir_paths_that_contain_a_supported_recording_file = []\n",
    "        for elem in top_level_dir_path.iterdir():\n",
    "            if not elem.name.startswith('.'):\n",
    "                if elem.is_dir():\n",
    "                    supported_recording_filepaths = [elem_2 for elem_2 in elem.iterdir() if elem_2.suffix in supported_extensions_for_recordings]\n",
    "                    if len(supported_recording_filepaths) > 0:\n",
    "                        all_subdir_paths_that_contain_a_supported_recording_file.append(elem)\n",
    "        return all_subdir_paths_that_contain_a_supported_recording_file\n",
    "\n",
    "\n",
    "    def _create_analysis_jobs_for_single_rec(self, recording_path: Path = None, result_path: Path = None) -> None:\n",
    "        if recording_path is None:\n",
    "            if self.config.recording_filepath:\n",
    "                recording_path = self.config.recording_filepath\n",
    "            else:\n",
    "                recording_path = self.config.data_source_path\n",
    "        self.add_info_to_logs(f'Starting with Job creation(s) for {str(recording_path)}', True)\n",
    "        recording_loader = self._get_recording_loader(recording_path)\n",
    "\n",
    "\n",
    "        if self.config.roi_filepath:\n",
    "            roi_filepath = self.config.roi_filepath\n",
    "        else:\n",
    "            roi_filepath = recording_loader.filepath.parent\n",
    "\n",
    "        if self.config.roi_mode == 'file':\n",
    "            roi_loaders = self._get_all_roi_loaders(roi_filepath)\n",
    "        else:\n",
    "            roi_loaders = None\n",
    "\n",
    "        if self.config.focus_area_filepath:\n",
    "            focus_area_filepath = self.config.focus_area_filepath\n",
    "        else:\n",
    "            focus_area_filepath = recording_loader.filepath.parent\n",
    "\n",
    "        if result_path is None:\n",
    "            if self.config.results_filepath:\n",
    "                result_path = self.config.results_filepath\n",
    "            else:\n",
    "                result_path = recording_loader.filepath.parent\n",
    "\n",
    "        if self.config.focus_area_enabled:\n",
    "            focus_area_dir_path = self._get_focus_area_dir_path(focus_area_filepath)\n",
    "            if focus_area_dir_path is None:\n",
    "                analysis_job = self._create_single_analysis_job(recording_loader, roi_loaders, result_filepath=result_path)\n",
    "                self.analysis_job_queue.append(analysis_job)\n",
    "                self.add_info_to_logs(f'Successfully created a single job for {focus_area_filepath} at queue position: #{len(self.analysis_job_queue)}.', True)\n",
    "            else:\n",
    "                all_focus_area_loaders = self._get_all_roi_loaders(focus_area_dir_path)\n",
    "                assert len(all_focus_area_loaders) > 0, f'Focus Area analysis enabled, but no focus area ROIs could be found. Please revisit your source data and retry!'\n",
    "                for idx, focus_area_loader in enumerate(all_focus_area_loaders):\n",
    "                    if result_path is None:\n",
    "                        if self.config.results_filepath:\n",
    "                            result_path = self.config.results_filepath / focus_area_loader.filepath.stem\n",
    "                    analysis_job_with_focus_area = self._create_single_analysis_job(recording_loader, roi_loaders, focus_area_loader, result_filepath=result_path)\n",
    "                    self.analysis_job_queue.append(analysis_job_with_focus_area)\n",
    "                    job_creation_message = (f'Successfully created {idx + 1} out of {len(all_focus_area_loaders)} job(s) for {recording_path} '\n",
    "                                            f'at queue position: #{len(self.analysis_job_queue)}.')\n",
    "                    self.add_info_to_logs(job_creation_message, True)\n",
    "        else:\n",
    "            analysis_job = self._create_single_analysis_job(recording_loader, roi_loaders, result_filepath=result_path)\n",
    "            self.analysis_job_queue.append(analysis_job)\n",
    "            self.add_info_to_logs(f'Successfully created a single job for {recording_path} at queue position: #{len(self.analysis_job_queue)}.', True)\n",
    "        self.add_info_to_logs(f'Finished Job creation(s) for {recording_path}!', True)\n",
    "\n",
    "\n",
    "    def _get_recording_loader(self, source_path: Path) -> RecordingLoader:\n",
    "        rec_loader_factory = RecordingLoaderFactory()\n",
    "        if source_path.is_dir():\n",
    "            self.add_info_to_logs(f'Looking for a valid recording file in {source_path}...', True)\n",
    "            valid_filepaths = get_filepaths_with_supported_extension_in_dirpath(source_path, rec_loader_factory.all_supported_extensions, 1)\n",
    "            if len(valid_filepaths) == 0:\n",
    "                self.add_info_to_logs(f'Could not find any recording files of supported type at {source_path}!', True)\n",
    "            elif len(valid_filepaths) >  1:\n",
    "                filepath = valid_filepaths[0]\n",
    "                too_many_files_message = (f'Found more than a single recording file of supported type at {source_path}, i.e.: {valid_filepaths}. '\n",
    "                                          f'However, only a single file was expected. NA3 continues with {filepath} and will ignore the other files.')\n",
    "                self.add_info_to_logs(too_many_files_message, True)\n",
    "            else:\n",
    "                filepath = valid_filepaths[0]\n",
    "                self.add_info_to_logs(f'Found recording file of supported type at: {filepath}.', True)\n",
    "        else:\n",
    "            filepath = source_path\n",
    "            self.add_info_to_logs(f'Found recording file of supported type at: {filepath}.', True)\n",
    "        recording_loader = rec_loader_factory.get_loader(filepath)\n",
    "        return recording_loader\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_all_roi_loaders(data_source_path: Path) -> list[ROILoader]:\n",
    "        assert data_source_path.is_dir(), f'You must provide a directory as source data when using ROI mode or enabling Focus Areas. Please revisit your input data and retry.'\n",
    "        roi_loader_factory = ROILoaderFactory()\n",
    "        all_filepaths_with_supported_filetype_extensions = get_filepaths_with_supported_extension_in_dirpath(data_source_path, roi_loader_factory.all_supported_extensions)\n",
    "        all_roi_loaders = [roi_loader_factory.get_loader(roi_filepath) for roi_filepath in all_filepaths_with_supported_filetype_extensions]\n",
    "        return all_roi_loaders\n",
    "\n",
    "\n",
    "    def _get_focus_area_dir_path(self, source_path: Path) -> Path:\n",
    "        focus_area_path_restrictions = FocusAreaPathRestrictions()\n",
    "        supported_dir_names = focus_area_path_restrictions.supported_dir_names\n",
    "        if source_path.is_dir():\n",
    "            source_dir_path = source_path\n",
    "        else:\n",
    "            source_dir_path = source_path.parent\n",
    "        dirs_with_valid_name = [elem for elem in source_dir_path.iterdir() if (elem.name in supported_dir_names) & (elem.is_dir() == True)]\n",
    "        if len(dirs_with_valid_name) == 0:\n",
    "            no_dir_found_message = (f'You enabled Focus Area but a correspondingly named directory could not be found in {source_dir_path}. '\n",
    "                                    f'Please use one of the following for the name of the directory that contains the Focus Area ROIs: {supported_dir_names}. '\n",
    "                                    'In absence of such a directory, analysis is continued without using the Focus Area mode for this data.')\n",
    "            self.add_info_to_logs(no_dir_found_message, True)\n",
    "            focus_area_dir_path = None\n",
    "        elif len(dirs_with_valid_name) > 1:\n",
    "            too_many_dirs = (f'More than a single Focus Area directory was found in the following parent directory: {source_dir_path}, i.e.: '\n",
    "                             f'{dirs_with_valid_name}. However, only the use of a single one that contains all your Focus Area ROIS is '\n",
    "                             f'currently supported. {dirs_with_valid_name[0]} will be used for this analysis, while the other(s): {dirs_with_valid_name[1:]} '\n",
    "                             'will be ignored to continue processing.')\n",
    "            self.add_info_to_logs(too_many_dirs, True)\n",
    "            focus_area_dir_path = dirs_with_valid_name[0]\n",
    "        else:\n",
    "            focus_area_dir_path = dirs_with_valid_name[0]\n",
    "        return focus_area_dir_path\n",
    "\n",
    "\n",
    "    def _create_single_analysis_job(self, recording_loader: RecordingLoader, roi_loaders: list[ROILoader] | None, focus_area_loader: ROILoader = None, result_filepath: Path = None) -> AnalysisJob:\n",
    "        data_loaders = {'recording': recording_loader}\n",
    "        if roi_loaders is not None:\n",
    "            data_loaders['rois'] = roi_loaders\n",
    "        if focus_area_loader is not None:\n",
    "            data_loaders['focus_area'] = focus_area_loader\n",
    "        return AnalysisJob(self.num_processes, data_loaders, result_filepath)\n",
    "\n",
    "\n",
    "    def _display_configs(self) -> None:\n",
    "        self.add_info_to_logs('Configurations for Analysis Settings and Result Creation validated successfully.', True)\n",
    "        self.add_info_to_logs(f'Analysis Settings are:')\n",
    "        for line in self.config.display_all_attributes():\n",
    "            self.add_info_to_logs(line)\n",
    "\n",
    "    def _save_user_settings_as_json(self, analysis_job: AnalysisJob) -> None:\n",
    "        filepath = analysis_job.results_dir_path.joinpath('user_settings.json')\n",
    "        self.config.recording_filepath = analysis_job.recording.filepath\n",
    "        if analysis_job.focus_area_enabled:\n",
    "            self.config.focus_area_filepath = analysis_job.focus_area.filepath\n",
    "        else:\n",
    "            self.config.focus_area_filepath = None\n",
    "        if analysis_job.rois_source == 'file':\n",
    "            self.config.roi_filepath = [roi.filepath for roi in analysis_job.all_rois]\n",
    "        with open(filepath, 'w+') as user_settings_json:\n",
    "            user_settings_json.write(self.config.to_json())\n",
    "\n",
    "\n",
    "    def create_analysis_jobs(self) -> None:\n",
    "        self._ensure_data_from_previous_jobs_was_removed()\n",
    "        self.add_info_to_logs('Basic configurations for data import validated. Starting creation of analysis job(s)...', True)\n",
    "        if self.config.batch_mode:\n",
    "            all_subdir_paths_with_rec_file = self._get_all_subdir_paths_with_rec_file(self.config.data_source_path)\n",
    "            all_subdir_paths_with_rec_file.sort()\n",
    "            for idx, subdir_path in enumerate(all_subdir_paths_with_rec_file):\n",
    "                if self.config.results_filepath:\n",
    "                    result_path = self.config.results_filepath / subdir_path.name\n",
    "                else:\n",
    "                    result_path = subdir_path\n",
    "                self._create_analysis_jobs_for_single_rec(subdir_path, result_path)\n",
    "        else:\n",
    "            self._create_analysis_jobs_for_single_rec()\n",
    "        self.add_info_to_logs('All job creation(s) completed.', True, 100.0)\n",
    "\n",
    "    def run_analysis(self) -> None:\n",
    "        self._display_configs()\n",
    "        self.add_info_to_logs('Starting analysis...', True)\n",
    "        for job_idx in range(len(self.analysis_job_queue)):\n",
    "            analysis_job = self.analysis_job_queue.pop(0)\n",
    "            self.add_info_to_logs(f'Starting to process analysis job with index #{job_idx}.')\n",
    "            analysis_job.run_analysis(self.config)\n",
    "            self.add_info_to_logs(f'Analysis successfully completed. Continue with creation of results.. ')\n",
    "            analysis_job.create_results(self.config, self.nwb_metadata)\n",
    "            self.add_info_to_logs(f'Results successfully created at: {analysis_job.results_dir_path}')\n",
    "            if self.gui_enabled:\n",
    "                self.callback_view_show_output_screen()\n",
    "                with self.view_output:\n",
    "                    activity_overview_fig = analysis_job.activity_overview_plot[0]\n",
    "                    activity_overview_fig.set_figheight(400 * self.pixel_conversion)\n",
    "                    activity_overview_fig.tight_layout()\n",
    "                    show(activity_overview_fig)\n",
    "            self._save_user_settings_as_json(analysis_job)\n",
    "            self.result_directories.append(analysis_job.results_dir_path)\n",
    "            self.add_info_to_logs('Updating all log files to contain all logs as final step. All valid logs files will end with this message.')\n",
    "            self.logs.save_current_logs(analysis_job.results_dir_path)\n",
    "        else:\n",
    "            gc.collect()\n",
    "\n",
    "\n",
    "    def preview_window_size(self, grid_size) -> tuple[Figure, Axes]:\n",
    "        job_for_preview = self.analysis_job_queue[0]\n",
    "        preview_fig, preview_ax = job_for_preview.preview_window_size(grid_size)\n",
    "        return preview_fig, preview_ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree\n",
    "import os\n",
    "from re import compile\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "from neuralactivitycubic.view import WidgetsInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_filepath = Path('../test_data/00')\n",
    "\n",
    "test00_filepath = Path('../test_data/00')\n",
    "test01_filepath = Path('../test_data/01')\n",
    "parent_test_filepath = Path('../test_data')\n",
    "example_results_dir = Path('../test_data/00/example_test_results_for_spiking_neuron')\n",
    "results_filepath = Path('../test_data/00/results_directory')\n",
    "# we have to split results by own directories due to concurrency issues\n",
    "results_case01_filepath = Path('../test_data/results/case_01/')\n",
    "results_case02_filepath = Path('../test_data/results/case_02/')\n",
    "results_case03_filepath = Path('../test_data/results/case_03/')\n",
    "results_case04_filepath = Path('../test_data/results/case_04/')\n",
    "results_case05_filepath = Path('../test_data/results/case_05/')\n",
    "results_case06_filepath = Path('../test_data/results/case_06/')\n",
    "\n",
    "def test_correct_model_run():\n",
    "    correct_config = WidgetsInterface().export_user_settings()\n",
    "    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n",
    "    correct_config.save_single_trace_results = True\n",
    "    model = Model(correct_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "    return model.result_directories\n",
    "\n",
    "def test_correct_model_run_with_custom_results_dir():\n",
    "    correct_config = WidgetsInterface().export_user_settings()\n",
    "    correct_config.data_source_path = test00_filepath / 'spiking_neuron.avi'\n",
    "    correct_config.results_filepath = results_filepath\n",
    "    model = Model(correct_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "    return correct_config.results_filepath\n",
    "\n",
    "# tests with different modes of running analysis\n",
    "\n",
    "def test_run_grid_focus_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus area enabled.\n",
    "    \"\"\"\n",
    "    grid_focus_mode_config = WidgetsInterface().export_user_settings()\n",
    "    grid_focus_mode_config.data_source_path = test01_filepath\n",
    "    grid_focus_mode_config.focus_area_enabled = True\n",
    "    grid_focus_mode_config.results_filepath = results_case01_filepath\n",
    "    model = Model(grid_focus_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return grid_focus_mode_config.results_filepath\n",
    "\n",
    "def test_run_file_focus_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus area enabled.\n",
    "    \"\"\"\n",
    "    file_focus_mode_config = WidgetsInterface().export_user_settings()\n",
    "    file_focus_mode_config.data_source_path = test01_filepath\n",
    "    file_focus_mode_config.focus_area_enabled = True\n",
    "    file_focus_mode_config.roi_mode = 'file'\n",
    "    file_focus_mode_config.results_filepath = results_case01_filepath\n",
    "    model = Model(file_focus_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return file_focus_mode_config.results_filepath\n",
    "\n",
    "def test_run_grid_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with batch mode enabled.\n",
    "    \"\"\"\n",
    "    grid_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    grid_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    grid_batch_mode_config.batch_mode = True\n",
    "    grid_batch_mode_config.results_filepath = results_case02_filepath\n",
    "    model = Model(grid_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return grid_batch_mode_config.results_filepath\n",
    "\n",
    "def test_run_file_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with batch mode enabled.\n",
    "    \"\"\"\n",
    "    file_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    file_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    file_batch_mode_config.batch_mode = True\n",
    "    file_batch_mode_config.roi_mode = 'file'\n",
    "    file_batch_mode_config.results_filepath = results_case02_filepath\n",
    "    model = Model(file_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return file_batch_mode_config.results_filepath\n",
    "\n",
    "def test_run_grid_focus_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus and batch mode enabled.\n",
    "    \"\"\"\n",
    "    grid_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    grid_focus_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    grid_focus_batch_mode_config.batch_mode = True\n",
    "    grid_focus_batch_mode_config.focus_area_enabled = True\n",
    "    grid_focus_batch_mode_config.results_filepath = results_case03_filepath\n",
    "    model = Model(grid_focus_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return grid_focus_batch_mode_config.results_filepath\n",
    "\n",
    "def test_run_file_focus_batch_mode():\n",
    "    \"\"\"\n",
    "    Test run_analysis function with focus and batch mode enabled.\n",
    "    \"\"\"\n",
    "    file_focus_batch_mode_config = WidgetsInterface().export_user_settings()\n",
    "    file_focus_batch_mode_config.data_source_path = parent_test_filepath\n",
    "    file_focus_batch_mode_config.batch_mode = True\n",
    "    file_focus_batch_mode_config.focus_area_enabled = True\n",
    "    file_focus_batch_mode_config.roi_mode = 'file'\n",
    "    file_focus_batch_mode_config.results_filepath = results_case03_filepath\n",
    "    model = Model(file_focus_batch_mode_config)\n",
    "    model.create_analysis_jobs()\n",
    "    model.run_analysis()\n",
    "\n",
    "    return file_focus_batch_mode_config.results_filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_csv_files(relative_filepath_to_csv: str, results_dir: Path) -> bool:\n",
    "    filepath = results_dir / relative_filepath_to_csv\n",
    "    # confirm results have been created:\n",
    "    if not filepath.is_file():\n",
    "        return False\n",
    "    # confirm computational consistency of results, while allowing minor numerical tolerance\n",
    "    df_test = pd.read_csv(filepath)\n",
    "    df_validation = pd.read_csv(example_results_dir / relative_filepath_to_csv)\n",
    "    if assert_frame_equal(df_test, df_validation) is not None:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def test_all_peak_results(results_dir):\n",
    "    return _test_csv_files('all_peak_results.csv', results_dir)\n",
    "\n",
    "def test_amplitude_and_df_over_f_results(results_dir):\n",
    "    return _test_csv_files('Amplitude_and_dF_over_F_results.csv', results_dir)\n",
    "\n",
    "def test_auc_results(results_dir):\n",
    "    return _test_csv_files('AUC_results.csv', results_dir)\n",
    "\n",
    "def test_variance_area_results(results_dir):\n",
    "    return _test_csv_files('Variance_area_results.csv', results_dir)\n",
    "\n",
    "def test_representative_single_trace_results(results_dir):\n",
    "    return _test_csv_files('single_traces/data_of_ROI_7-10.csv', results_dir)\n",
    "\n",
    "def test_activity_overview_png(results_dir):\n",
    "    filepath = results_dir / 'activity_overview.png'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_roi_label_ids_overview_png(results_dir):\n",
    "    filepath = results_dir / 'ROI_label_IDs_overview.png'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_individual_traces_with_identified_events_pdf(results_dir):\n",
    "    filepath = results_dir / 'Individual_traces_with_identified_events.pdf'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_logs_txt(results_dir):\n",
    "    filepath = results_dir / 'logs.txt'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_user_settings_json(results_dir):\n",
    "    filepath = results_dir / 'user_settings.json'\n",
    "    return filepath.is_file()\n",
    "\n",
    "def test_nwb_export(results_dir):\n",
    "    filepath = results_dir / 'autogenerated_nwb_file.nwb'\n",
    "\n",
    "def test_all_correct_files_created(results_dir: Path, test_csv: bool = True, single_trace: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Check if all expected files have been created in the results directory.\n",
    "    \"\"\"\n",
    "    # confirm all csv files have been created and are correct:\n",
    "    if test_csv:\n",
    "        assert test_all_peak_results(results_dir), 'There is an issue with the \"all_peak_results.csv\" file!'\n",
    "        assert test_amplitude_and_df_over_f_results(results_dir)\n",
    "        assert test_auc_results(results_dir)\n",
    "        assert test_variance_area_results(results_dir)\n",
    "        if single_trace:\n",
    "            assert test_representative_single_trace_results(results_dir)\n",
    "\n",
    "    # confirm all other result files have been created:\n",
    "    assert test_activity_overview_png(results_dir)\n",
    "    assert test_roi_label_ids_overview_png(results_dir)\n",
    "    assert test_logs_txt(results_dir)\n",
    "    assert test_user_settings_json(results_dir)\n",
    "    if single_trace:\n",
    "        assert test_individual_traces_with_identified_events_pdf(results_dir)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def find_directories_after_test(base_path):\n",
    "    \"\"\"\n",
    "    Find directories after test with timestamps.\n",
    "    \"\"\"\n",
    "    pattern = compile(r'\\d{4}_\\d{2}_\\d{2}_\\d{2}-\\d{2}-\\d{2}_.+')\n",
    "\n",
    "    matching_dirs = [\n",
    "        str(base_path) + d for d in os.listdir(base_path)\n",
    "        if os.path.isdir(os.path.join(base_path, d)) and pattern.fullmatch(d)\n",
    "    ]\n",
    "\n",
    "    return matching_dirs\n",
    "\n",
    "def delete_directories_after_test(paths_list):\n",
    "    \"\"\"\n",
    "    Delete directories after test.\n",
    "    \"\"\"\n",
    "    for res_dir in find_directories_after_test(paths_list):\n",
    "        rmtree(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed:\n",
    "result_directories = test_correct_model_run()\n",
    "\n",
    "for directory in result_directories:\n",
    "    assert directory.exists()\n",
    "    assert test_all_correct_files_created(directory)\n",
    "    # cleanup\n",
    "    rmtree(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with custom result directory:\n",
    "results_directory = test_correct_model_run_with_custom_results_dir()\n",
    "\n",
    "assert results_directory.exists()\n",
    "# only one directory with analysis files should be created, as there are only one focus area:\n",
    "assert len(list(results_directory.iterdir())) == 1\n",
    "\n",
    "for directory in results_directory.iterdir():\n",
    "    assert test_all_correct_files_created(directory)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with focus mode:\n",
    "results_directory = test_run_grid_focus_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "# only one directory should be created, as there are only one focus area:\n",
    "assert len(list(results_directory.iterdir())) == 2\n",
    "for directory in results_directory.iterdir():\n",
    "    assert test_all_correct_files_created(directory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch mode:\n",
    "results_directory = test_run_grid_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch and focus mode:\n",
    "results_directory = test_run_grid_focus_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    # test case with multiple focus areas\n",
    "    if '01' in directory.name:\n",
    "        assert len(list(directory.iterdir())) == 2\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with focus mode:\n",
    "results_directory = test_run_file_focus_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "# only one directory should be created, as there are only one focus area:\n",
    "assert len(list(results_directory.iterdir())) == 2\n",
    "for directory in results_directory.iterdir():\n",
    "    assert test_all_correct_files_created(directory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch mode:\n",
    "results_directory = test_run_file_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that model can be executed with batch and focus mode:\n",
    "results_directory = test_run_file_focus_batch_mode()\n",
    "\n",
    "assert results_directory.exists()\n",
    "\n",
    "assert len(list(results_directory.iterdir())) == 3\n",
    "for directory in results_directory.iterdir():\n",
    "    # test case with multiple focus areas\n",
    "    if '01' in directory.name:\n",
    "        assert len(list(directory.iterdir())) == 2\n",
    "    for subdirectory in directory.iterdir():\n",
    "        assert test_all_correct_files_created(subdirectory, test_csv=False)\n",
    "\n",
    "# cleanup\n",
    "rmtree(results_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
